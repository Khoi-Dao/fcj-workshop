[
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "Introduction This workshop guides you through deploying a full-stack DNA Analysis application on AWS. The application allows users to analyze DNA sequences, manage results, and visualize biological data.\nApplication Architecture Frontend (React + Vite) Framework: React 18 with TypeScript UI Libraries: Material-UI, TailwindCSS, Recharts State Management: React Context API Routing: React Router v6 Form Handling: React Hook Form with Zod validation HTTP Client: Axios Hosting: S3 + CloudFront CDN Backend (Spring Boot) Framework: Spring Boot 3.x Language: Java 17 Database: MySQL 8.0 with Spring Data JPA Security: Spring Security with JWT authentication API: RESTful API with proper error handling Hosting: EC2 instances with Auto Scaling Database (RDS MySQL) Engine: MySQL 8.0.40 Instance: db.t3.micro (scalable) Storage: 20GB gp3 with encryption Backup: Automated backups with 3-7 days retention High Availability: Multi-AZ deployment (optional) AWS Architecture Network Layer VPC (10.0.0.0/16)\r├── Public Subnets (10.0.1.0/24, 10.0.3.0/24)\r│ ├── Internet Gateway\r│ ├── NAT Gateway\r│ └── Application Load Balancer\r│\r└── Private Subnets (10.0.2.0/24, 10.0.4.0/24)\r├── EC2 Instances (Auto Scaling Group)\r├── RDS MySQL (Multi-AZ)\r└── VPC Endpoints (S3, CloudWatch, SSM, Cognito) Application Flow User Browser\r│\r├─── HTTPS ──\u0026gt; CloudFront ──\u0026gt; S3 (Static Frontend)\r│\r└─── HTTPS ──\u0026gt; API Gateway ──\u0026gt; ALB ──\u0026gt; EC2 (Backend API)\r│\r└──\u0026gt; RDS MySQL Security Architecture Internet\r│\r├─── CloudFront (HTTPS only)\r│ └─── S3 Bucket Policy (CloudFront OAI)\r│\r└─── API Gateway (Resource Policy)\r└─── ALB Security Group (Port 80/443)\r└─── EC2 Security Group (Port 8080 from ALB only)\r└─── RDS Security Group (Port 3306 from EC2 only) Key Features 1. User Authentication User registration and login JWT token-based authentication AWS Cognito integration (optional) Session management 2. DNA Analysis Upload and analyze DNA sequences Support multiple file formats Batch processing capability Store analysis results 3. Data Visualization DNA analysis charts Dashboard with metrics Export results in multiple formats 4. User Management User profile management Analysis history Role-based access control Infrastructure as Code CloudFormation Template The infrastructure.yaml template includes:\nNetworking (Lines 1-400)\nVPC with DNS support 2 Public Subnets (Multi-AZ) 2 Private Subnets (Multi-AZ) Internet Gateway NAT Gateway (can be disabled for cost savings) Route Tables VPC Endpoints (S3, CloudWatch, SSM, Cognito) Compute (Lines 400-700)\nLaunch Template with User Data script Auto Scaling Group (1-4 instances) Application Load Balancer Target Group with health checks Scaling Policies (CPU-based) Storage \u0026amp; CDN (Lines 700-900)\nS3 Bucket for Frontend S3 Bucket Policy CloudFront Distribution CloudFront Origin Access Identity Database (Lines 900-1000)\nRDS MySQL Instance DB Subnet Group Automated Backups Encryption at rest Security (Lines 1000-1200)\nSecurity Groups (ALB, EC2, RDS, VPC Endpoints) IAM Roles (EC2, CloudWatch, S3) IAM Instance Profile Cognito User Pool (optional) Secrets Manager (optional) Monitoring (Lines 1200-1393)\nCloudWatch Log Groups CloudWatch Alarms (CPU, Memory) SNS Topic for alerts API Gateway with CORS Cost Optimization 1. VPC Endpoints instead of NAT Gateway Savings: ~$20-25/month\nS3 Gateway Endpoint: FREE Interface Endpoints: $7.20/endpoint/month Total: ~$28/month vs NAT Gateway $32/month + data transfer 2. Instance Sizing Development: t3.micro ($7-10/month) Production: t3.small or t3.medium\n3. RDS Optimization Single-AZ for development Multi-AZ for production Automated backups with appropriate retention 4. CloudFront Caching Reduce requests to S3 Lower latency for users Free tier: 1TB data transfer/month Best Practices Applied 1. Security ✅ Private subnets for EC2 and RDS ✅ Security Groups with least privilege ✅ IAM Roles instead of hardcoded credentials ✅ Encryption at rest and in transit ✅ VPC Endpoints for private connectivity ✅ CloudTrail for audit logging (optional)\n2. High Availability ✅ Multi-AZ deployment ✅ Auto Scaling Group ✅ Application Load Balancer ✅ RDS automated backups ✅ CloudFront global CDN\n3. Monitoring \u0026amp; Logging ✅ CloudWatch Logs for application logs ✅ CloudWatch Alarms for metrics ✅ SNS notifications ✅ Health checks on ALB and ASG\n4. Automation ✅ Infrastructure as Code with CloudFormation ✅ User Data scripts for EC2 initialization ✅ Systemd service for application management ✅ Automated deployments with scripts\nDeployment Steps Preparation (10 minutes)\nInstall AWS CLI Create EC2 Key Pair Configure parameters Deploy Infrastructure (15-20 minutes)\nValidate CloudFormation template Create stack Wait for resources to be created Deploy Backend (20-30 minutes)\nBuild JAR file Upload to S3 Deploy to EC2 Configure database connection Deploy Frontend (10-15 minutes)\nBuild React application Upload to S3 Invalidate CloudFront cache Testing (15-30 minutes)\nTest authentication Test DNA analysis features Verify monitoring Cleanup (5-10 minutes)\nDelete CloudFormation stack Verify all resources deleted Expected Outcomes After completing this workshop, you will have:\n✅ A working full-stack application on AWS ✅ Deep understanding of AWS networking and security ✅ Experience with Infrastructure as Code ✅ Knowledge of cost optimization ✅ Best practices for production deployment\nReference Resources AWS CloudFormation Documentation AWS VPC Best Practices AWS Well-Architected Framework Spring Boot on AWS React Deployment Best Practices "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI by Sapna Kumar and Nickee DeLeon on 23 SEP 2025 in Amazon Bedrock, Artificial Intelligence, Customer Solutions, Generative AI, Healthcare, Nonprofit, Public Sector\nFounded in 1990, the International Myeloma Foundation (IMF) is the first and largest global foundation focused specifically on multiple myeloma. The IMF’s mission is to improve the quality of life of myeloma patients while working toward prevention and a cure by focusing on four key areas: research, education, support, and advocacy. In September 2024, the IMF launched Myelo—an artificial intelligence-powered virtual assistant built using Amazon Web Services (AWS) and Max.AI, developed by ZS Associates.\nThis blog post highlights Myelo’s capabilities to provide myeloma patients, caregivers, and healthcare professionals a trustworthy, round-the-clock source of information that is accessible from anywhere in the world, grounded in empathy, and can communicate in multiple languages.\nGenerative AI: Closing the gap by disseminating timely information For 35 years, the IMF has been supporting patients’ informational needs through its extensive library of over 100 multilingual publications, a robust website, and a highly trained InfoLine team that answers questions by phone and email.\nHowever, the IMF needed to keep up with the rapid influx of new information (about research, treatments, drug approvals, etc.), manage its ever-growing content, and deliver quick, personalized information to the myeloma community it serves.\nThe onset of generative AI technology effectively and efficiently filled this need, and Myelo has become the IMF’s dependable AI-powered chatbot—addressing questions and providing information specific to multiple myeloma in a swift, accurate, and empathetic manner.\nMyelo uses generative AI to search and synthesize information across the IMF’s deep knowledge base, including web pages, PDFs, videos, and podcasts. It delivers professional, accurate, and compassionate responses—from treatment options and clinical trials to symptom management and diagnosis—within seconds.\nMyelo is multilingual and available 24/7, 365 days a year—making it a lifeline for patients and caregivers who need to make complex decisions at any time. As myeloma patient and support group leader Oya Gilbert said: “Having that ability to go onto and talk to Myelo and know that you’re getting a trusted resource, that’s the key—a trusted resource from an organization that is trusted throughout the world, not just in the United States.”\nWhile Myelo can break down complex medical terms into more comprehensible information, it is important to note that it does not offer medical advice or diagnoses. Users are still encouraged to consult healthcare professionals for personalized care.\nAWS Bedrock: The strong foundation that sustains Myelo Myelo is built on Amazon Bedrock, a service that allows organizations to build and scale generative AI applications using foundational models (FMs)—large language models pre-trained on vast datasets. Bedrock eliminates the need for infrastructure management or model training from scratch, enabling the IMF to move quickly and securely.\nAt the heart of Myelo’s language processing capabilities is Anthropic’s Claude, a state-of-the-art large language model accessed through AWS Bedrock. Claude’s advanced reasoning abilities and focus on safety make it particularly well-suited for healthcare applications, providing nuanced understanding of medical terminology while maintaining appropriate boundaries around medical advice.\nWith Bedrock and Claude, the IMF can deploy tools for real-time summarization, question-answering, text classification, and information retrieval—all fine-tuned with the foundation’s own data.\nTMax.AI: Delivering smart, empathetic responses At the core of Myelo’s intelligence is Max.AI—an enterprise-grade AI platform created by ZS Associates and integrated with AWS services. Max.AI enables Myelo to process and organize large volumes of unstructured content—from transcripts and PDFs to web articles—so users get rich, accurate responses with citations.\nThe combination of Max.AI’s enterprise capabilities with Claude’s sophisticated language understanding creates a powerful synergy. Max.AI handles the complex data orchestration and content management, while Claude processes natural language queries and generates contextually appropriate responses that maintain the empathetic tone essential for cancer patient support.\nMax.AI also allows Myelo to maintain conversation history, delivering contextual, personalized responses over the course of a chat. The platform supports a seamless user experience across all devices, and users can even download their chat transcripts for reference.\nEquipped with built-in ethical safeguards and responsible AI protocols, Max.AI ensures that every response Myelo gives is both secure and trustworthy. Myelo: Breaking barriers and expanding the IMF’s global reach Since its launch, Myelo has become a staple of the IMF’s website, where it appears as a conspicuous pop-up. Users have asked it everything from “What are the infection prevention guidelines for myeloma patients?” to “What does it mean when my FISH chromosome analysis says, ‘Quantity Not Sufficient’?” In 2024 alone, Myelo fielded 16,883 inquiries. In the first half of 2025, it had already responded to more than 55,000.\nBy addressing many urgent and frequently asked questions, Myelo has enabled the IMF’s InfoLine team to handle more complex, one-on-one needs. Its multilingual capacity has significantly expanded the IMF’s global reach.\nOne US-based physician praised Myelo after testing it in Spanish: “There was not one question in the session that got an incorrect answer. AMAZING!”\nWhat’s next for Myelo? While Myelo has been successfully delivered, it is merely one milestone on IMF’s digital transformation journey. The chatbot continues to improve based on user feedback, including “thumbs down” ratings that trigger internal review and optimization.\nSoon, Myelo will be able to:\nPersonalize responses based on user profiles and prior chats Tailor information using geofencing (location-based customization) Take digital actions on behalf of users via AI agents Understand and respond to more nuanced or complex medical queries Offer a voice interface, adding speech recognition and synthesis capabilities Lay the foundation for the Myeloma Knowledge Platform Additionally, a version of Myelo to serve solely the healthcare professional community is being developed. Furthermore, Myelo is one part of a larger digital ecosystem the IMF is building: the Myeloma Knowledge Platform (MKP). Built on AWS Data Lake for Nonprofits, the MKP connects Myelo with other tools—such as a clinical trial finder—and unlocks insights by analyzing previously siloed data.\nBy identifying patterns across these systems, the MKP will offer personalized recommendations to patients, helping them navigate their unique myeloma journeys with greater clarity and confidence. Front Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate A more connected, personalized, and patient-centered future As the IMF continues to innovate at the intersection of healthcare and technology, Myelo stands as a compelling example of how generative AI can be used responsibly to empower patients, support caregivers, and ease the burden of a complex disease.\nBy delivering timely, trusted information with compassion, Myelo is more than just a chatbot: It’s a digital companion helping people make sense of life with myeloma. As part of the broader Myeloma Knowledge Platform, Myelo marks the beginning of a more connected, personalized, and patient-centered future in cancer care.\nThe IMF believes that by arming patients with knowledge about myeloma, they can make informed decisions about their care, gain access to myeloma specialists in their areas, find the best treatments available to them, and learn practical tools to live well with the disease.\nThis very much aligns with the IMF’s vision: “A world where every myeloma patient can live life to the fullest, unburdened by the disease.”\nAbout Author -Sapna Kumar\nFor six years, Sapna has served as the IMF\u0026rsquo;s manager of marketing and communications. She brings editorial expertise from the publishing industry, from roles at companies including McGraw-Hill, Pearson Education, and Encyclopedia Britannica. In 2021, she served as a digital learning manager with the American Academy of Physical Medicine \u0026amp; Rehabilitation, where she worked with medical professionals to develop continuing medical education courses. She holds a B.A. in writing from Purdue University and certificates in digital marketing, Google Analytics, and web-based communications. In the fall of 2022, she rejoined the IMF team to return to focusing on mission-driven work, always with the patients, caregivers, and the myeloma community at top of mind. Nickee DeLeon\nNickee is an editorial consultant. She is a versatile creative professional with a proven track record across multiple disciplines in communications and design. With extensive experience spanning publishing, editorial work, writing, graphic design, and art direction, Nickee brings a comprehensive understanding of how visual and written content work together to create compelling narratives. Original blog site: IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Transforming SAP Technical Documentation with GenAI: Accelerating knowledge generation for SAP Notes with Amazon Bedrock This blog was made by Zhang Wenju, Abhi Shivaditya, Beth Sharp, and Zhenyu Yin on 23 SEP 2025\nThis post was jointly authored by AWS and SAP\nWhat are SAP Notes? SAP Notes and Knowledge Base Articles (KBA) are fundamental components of SAP’s support ecosystem. These documents provide detailed instructions and solutions for known issues within SAP software products. While SAP Notes primarily focus on coding corrections and technical solutions, KBAs complement them by describing issues in business language, often including screenshots and visual aids. Together, they form a comprehensive knowledge base that helps users and consultants effectively troubleshoot and resolve SAP-related challenges.\nChallenges faced by customers in navigating complex SAP systems and documentation As SAP professionals increasingly adopt mobile-first workflows for on-the-go troubleshooting, accessing critical documentation like SAP Notes poses a growing challenge. While users now expect to search and retrieve SAP Notes instantly via smartphones or tablets, the content remains optimized for desktop browsers—forcing mobile viewers to endure excessive zooming, scrolling, and text-heavy navigation. Key implementation steps or urgent configuration fixes are buried in dense technical jargon and multi-page layouts unsuited to compact screens. This disconnect delays time-sensitive resolutions, as support teams waste minutes deciphering desktop-formatted content on mobile devices, increasing the risk of oversight and cognitive fatigue during critical tasks.\nWhat is the summarization feature that the SAP for Me team introduced? To address these challenges, the SAP for Me team partnered with AWS. Using Amazon Bedrock, they developed a new version of their mobile app featuring AI-generated summaries of SAP Notes and KBAs. This innovation allows users to quickly navigate and understand technical documentation through concise, automatically generated summaries.\nWhy is this summarization feature useful for users of the SAP for Me Mobile app? With the summarization feature, users can quickly assess note relevance for specific business scenarios and access prerequisites and implementation requirements instantly. The feature transforms lengthy technical documentation into mobile-friendly content, highlighting key implementation steps without extensive scrolling thereby significantly reducing time spent during urgent troubleshooting scenarios. End users, for example Basis and functional support teams and consultants, benefit from improved efficiency while on the go, while expert knowledge becomes more accessible in mobile-first contexts, enabling faster decision-making and providing the ability to quickly determine note relevance for investigation. |\nLaying the foundation: from vision to first deployment Think big and start small After the Amazon Bedrock integration into SAP’s Generative AI Hub, featuring Claude 3.5 Sonnet and Amazon Titan, was announced at SAP Sapphire in 2024, AWS initiated Generative AI workshops with SAP China Labs. The AWS and SAP China labs fostered a collaborative environment through regular workshops, identifying SAP notes and KBAs, comprising millions of technical documents, as prime candidates for Generative AI enhancement. The teams established weekly meetings and on-demand support to implement SAP Generative AI Hub and invoking Claude models via Amazon Bedrock APIs. Despite initial technical challenges, AWS technical assistance helped SAP meet their pre-Christmas 2024 deployment deadline.\nWhat were the initial blockers the team faced? The team faced time pressure when attempting to summarize 6 million SAP Notes within tight timeframes. Ensuring technical accuracy of AI-generated summaries proved crucial, as did developing efficient systems for managing summarization updates for both new and existing notes.\nArchitecture Deep Dive: Knowledge bases as a backbone This project uses retrieval-augmented generation (RAG) to provide customers with precise answers to their queries directly from SAP Notes and KBAs, eliminating the need to manually search through multiple documents. Customers benefit from immediate access to accurate and relevant information, significantly accelerating issue resolution.\nThe RAG system begins with data ingestion and processing pipeline tailored to SAP Notes and KBAs. In this phase, unstructured and semi-structured documents like SAP notes in html format were systematically collected, cleansed, and transformed into a retrievable format. Key steps include extracting text, metadata (e.g., note numbers, applicability, release versions), and resolving cross-references between documents. Preprocessing techniques, such as semantic chunking and entity recognition, are applied to segment content into contextually meaningful units, while preserving technical nuances. These chunks were then encoded into high-dimensional vector embeddings using Amazon Titan Embedding model. The processed data was indexed in HANA vector database, optimized for fast similarity searches, ensuring the system can efficiently map user queries to the most relevant SAP content during retrieval.\nDuring the query phase, the RAG service combines retrieval and generation to deliver precise answers. When a user submits a query, the system first leverages the HANA vector database to retrieve the top-k SAP Notes or KBA snippets semantically aligned with the query intent. A reranking step prioritizes results based on relevance scores, publication dates, or applicability criteria to ensure up-to-date and actionable insights. The retrieved context is then fed into Bedrock Claude 3.5 Sonnet model via SAP Generative AI Hub into Amazon Bedrock, which synthesizes a concise, natural language response. Crucially, the response is grounded strictly in the retrieved sources, with built-in citations to original notes/KBAs for transparency. This hybrid approach balances speed and accuracy, allowing customers to resolve issues in real time while minimizing hallucinations, thereby reducing reliance on traditional support channels.\nCustomer value \u0026amp; benefit SAP’s initiatives to enhance the customer experience through the “Summary for SAP Notes/KBA” and “RAG for SAP Notes/KBA” projects provide significant business value. The two projects streamline access to critical information, enabling customers to efficiently resolve issues and maximize their use of SAP software products. By improving clarity and accessibility, customers can quickly identify relevant solutions, thus reducing downtime and optimizing productivity. This enhanced access to precise information not only improves operational efficiency but also strengthens the overall customer support experience.\nNext step: online recommendations with context The SAP for Me team is moving from basic summarization to using agentic RAG and knowledge graphs. This will help provide smarter, context-aware technical guidance and make it easier to visualize system dependencies and knowledge map. The team is also working on adding predictive support and personalized recommendations to improve the user experience even more. These steps will build a more powerful and intuitive support system for SAP users.\nConclusion SAP for Me’s Generative AI journey demonstrates the transformative impact of thoughtfully applying AI to enterprise software challenges. Leveraging the integration of Amazon Bedrock’s Claude 3.5 Sonnet with SAP’s Generative AI Hub, the team tackled the fundamental problem of knowledge accessibility by implementing summarization capabilities for SAP’s vast repository of technical notes. This initial feature addressed the critical need for efficient mobile consumption of complex documentation.\nDespite early implementation challenges, including Generative AI upskilling and ensuring technical accuracy, the collaborative effort between SAP and AWS teams successfully delivered the summarization feature before the targeted 2024 year-end holiday deadline. This achievement represents more than just a technical milestone; it marks a fundamental shift in how SAP customers can access and utilize critical technical knowledge on mobile devices.\nThe roadmap ahead, progressing through agentic RAG, knowledge graph, and proactive recommendations, shows a strategic vision for evolving from static documentation to dynamic, interconnected, and anticipatory knowledge systems. This evolution promises to transform SAP’s customer support model from reactive problem-solving to proactive guidance tailored to each customer’s unique environment.\nWe encourage you to explore these new capabilities of SAP for Me mobile application and see firsthand how Generative AI can transform your technical support and knowledge management. Reach out to us to learn how you can leverage Amazon Bedrock and unlock greater efficiency and insight for your organization.\nOriginal blog site: Transforming SAP Technical Documentation with GenAI\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "MOSIP on AWS: Transforming digital identity for modern governments by Andrew Johnston and Mohamed Heiba on 23 SEP 2025 in AWS Outposts, Government, Public Sector, Security, Identity, \u0026amp; Compliance\nAccording to the World Bank’s Identification for Development (ID4D) initiative, approximately 850 million people globally don’t have official identification. This prevents citizens from access to essential services including healthcare, education, and social benefits. To address these challenges, Atos and Amazon Web Services (AWS) have collaborated on an innovative cloud-based digital identity system using the Modular Open-Source Identity Platform (MOSIP), making these systems more accessible, secure, and scalable than ever before.\nThe need for digital identity systems The COVID-19 pandemic has fundamentally transformed how we approach identification systems, accelerating the need for contactless solutions across various sectors. According to industry data, the integration of biometric technology plays a vital role in this digital transformation. The global biometric technology market is projected to grow from USD $34.27 billion in 2022 to USD $150.58 billion by 2030. Government agencies and citizens recognize the importance of preventing fraud and enhancing data security while ensuring reliable identification methods that promote trust and accessibility in our increasingly digital world.\nThe MOSIP solution on AWS MOSIP—established in 2018 at the International Institute of Information Technology Bangalore—is a not-for-profit initiative helping governments implement digital ID systems. Governments can rely on the platform’s robust, scalable architecture to build effective civil registries and service delivery systems, forming the foundation for digital economies. Successful implementations in multiple governments across Africa demonstrate MOSIP’s ability to facilitate essential services through secure and accessible digital identity systems.\nChallenges with on-premises digital identity systems As various levels of governments throughout the world strive to implement reliable and secure identification systems in service of their mission to provide constituent services, they’re faced with several challenges from maintaining on-premises digital identity systems. There are operational challenges, implementation and social challenges, and technical challenges they must address before finding success.\nHigh infrastructure costs create significant barriers to entry, and limited scalability struggles to handle growing population needs. These systems often face security vulnerabilities, complex maintenance procedures, and integration difficulties with existing government systems.\nOrganizations must maintain specialized IT staff, manage complex backup procedures, and handle high maintenance costs. Manual processes can introduce errors in identity verification, while ensuring 24/7 system availability remains challenging.\nNew system deployments face resistance from traditional system users and require extensive training. Privacy concerns, regulatory compliance, and digital literacy barriers affect system adoption. Additionally, ensuring inclusion for remote and marginalized populations presents ongoing challenges.\nBenefits of a cloud-based digital identity system A cloud-based digital identity system offers benefits to governments and their agencies on multiple fronts. They’ll discover operational and financial benefits, improvements to security and accessibility, and integration and business continuity advantages.\nCloud deployment of MOSIP on AWS transforms service delivery through auto scaling capabilities and high availability. The pay-as-you-go model eliminates large upfront investments, and automated scaling handles population growth efficiently. AWS managed services reduce infrastructure costs and eliminate routine maintenance tasks.\nThe platform enables government services across remote regions through standard internet connectivity, with built-in disaster recovery. AWS provides comprehensive security features, including encryption and identity management, to help meet regulatory requirements and minimize risk management costs.\nStrong integration capabilities allow seamless connection with existing government systems and future services. Automated backup enables business continuity without additional investment. This cloud-based approach accelerates implementation timeframes, reduces operational complexity, and provides enterprise-grade security and performance for modern digital identity systems.\nHybrid deployment solutions To address government requirements, four deployment models are available:\nHybrid deployment – Separates production and nonproduction environments Split security model – Maintains key management and backups on premises Sensitive data protection – Keeps citizen data on premises and uses cloud for other services AWS Outposts – Provides full AWS capabilities within government data centers Cost optimization The implementation of MOSIP on AWS offers significant cost advantages compared to traditional on-premises deployments. Based on our analysis using AWS Pricing Calculator, we’ve identified three deployment tiers plus a foundational layer that accommodates different scale requirements.\nThe foundational infrastructure layer costs approximately $4,000 per month and provides essential shared services, including networking, security, and DevOps tools. This includes Amazon Virtual Private Cloud (Amazon VPC) configurations, AWS Transit Gateway gateways, VPN connectivity, AWS Network Firewall firewall, and development tools such as AWS CodeBuild and Amazon Elastic Container Registry (Amazon ECR), forming the backbone of any MOSIP deployment, regardless of scale.\nThe following table compares three different scalable deployment tiers. All costs are approximate and can vary based on specific requirements, Region, and actual usage patterns.\nSmall scale Medium scale Large scale Estimate cost per month USD $4,168 per month USD $9,896.10 per month USD $14,395.85 per month Enrollments per day Suitable for hundreds of enrollments or authentications per day Handles tens of thousands of enrollments daily Processes hundreds of thousands of enrollments daily Estimate cost per enrollment USD $1.39 per enrollment USD $0.033 per enrollment USD $0.0048 per enrollment Availability -Single Availability Zone deployment High availability across two zones Enterprise-grade high availability Use case Ideal for pilot programs or smaller implementations Production-grade infrastructure Full production capability Cost comparison with on-premises -Traditional on-premises MOSIP implementations typically require the following budget considerations:\nInitial infrastructure investment: USD $2–3 million (estimate) Data center setup and maintenance: USD $500,000–750,000 annually (estimate) Specialized IT staff: USD $250,000–400,000 annually (estimate) Hardware refresh cycles every 3–5 years: USD $1–1.5 million (estimate) The cloud-based approach eliminates approximately 60–70 percent of upfront costs and reduces ongoing operational expenses by 40–50 percent. Implementation time is reduced from 12–18 months to 3–6 months, accelerating time to value. The pay-as-you-grow model enables organizations to only pay for resources they use, with the ability to scale up or down based on demand. Government impact The MOSIP on AWS solution enables:\nImproved public service delivery Enhanced citizen inclusion Stronger data security Better resource utilization Scalable population coverage This cloud-based approach transforms digital identity from a resource-intensive requirement into an efficient public service enabler, which means governments can focus on citizen services rather than infrastructure management.\nFor governments seeking to modernize their identification systems, MOSIP on AWS provides a compelling solution that combines cost-effectiveness, scalability, and security. The platform’s successful implementations in multiple countries demonstrate its reliability and effectiveness in serving diverse population needs, making it an ideal choice for governments committed to digital transformation and improved citizen services.\nConclusion In part two of this blog post series, we will dive deep into the technical architecture of MOSIP on AWS, exploring the intricate details of its deployment models, security frameworks, and integration patterns. We’ll examine the specific AWS services that power this solution, detailed infrastructure configurations, and best practices for implementation. Stay tuned for an in-depth technical exploration of how governments can use this innovative platform to build robust digital identity systems.\nReference: Future of Government Awards 2023 celebrate use of technology to transform people’s lives Original blog site: MOSIP on AWS: Transforming digital identity for modern governments\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event 1 “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” Summary Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Provide comprehensive overview of AWS AI/ML services and capabilities Introduce Amazon SageMaker as an end-to-end ML platform Explore Generative AI with Amazon Bedrock Demonstrate practical applications through live demos Share best practices for AI/ML implementation in Vietnam Event Details Date: Saturday, November 15, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office Duration: 3.5 hours (excluding lunch break) Agenda 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Highlights Amazon SageMaker Platform Comprehensive ML Platform: Complete solution for building, training, and deploying machine learning models Data Preparation: Tools for data labeling, feature engineering, and data validation Model Training: Support for various ML frameworks and algorithms with distributed training capabilities Model Deployment: Flexible deployment options including real-time inference, batch processing, and serverless inference MLOps Integration: Built-in capabilities for model monitoring, versioning, and automated workflows Generative AI with Amazon Bedrock Foundation Model Selection: Understanding differences between Claude, Llama, and Titan models Claude: Strong reasoning and conversation capabilities Llama: Open-source models with good performance Titan: AWS-developed models optimized for specific use cases Prompt Engineering Techniques: Chain-of-Thought reasoning for complex problem solving Few-shot learning with examples Context management and prompt optimization RAG Architecture: Combining retrieval with generation for accurate, context-aware responses Knowledge base integration Vector embeddings and similarity search Document chunking strategies Bedrock Agents: Autonomous agents that can perform multi-step tasks Tool integrations and API calling Workflow orchestration Decision-making capabilities Guardrails for AI Safety: Content filtering and safety controls Harmful content detection Custom policy configurations Compliance and governance AI/ML Landscape in Vietnam Current adoption trends and opportunities Use cases specific to Vietnamese market Challenges and solutions for local businesses Success stories and case studies Key Takeaways Machine Learning Best Practices End-to-end Platform Approach: Use SageMaker for complete ML lifecycle management Data Quality First: Invest in data preparation and labeling for better model performance MLOps Integration: Implement monitoring and automated workflows from the start Model Selection Strategy: Choose the right model based on use case, not just performance metrics Generative AI Implementation Foundation Model Selection: Understand strengths of each model (Claude, Llama, Titan) for different scenarios Prompt Engineering Mastery: Chain-of-Thought and Few-shot learning significantly improve results RAG for Accuracy: Use RAG architecture when factual accuracy is critical Agent Design: Build agents that can handle multi-step workflows with proper tool integration Safety First: Always implement guardrails for content filtering and compliance Production Readiness Start Small, Scale Gradually: Begin with pilot projects before full deployment Cost Optimization: Monitor and optimize inference costs with serverless options Security \u0026amp; Compliance: Implement proper access controls and data privacy measures Continuous Improvement: Monitor model performance and iterate based on real-world feedback Applying to Work Explore SageMaker: Start with SageMaker Studio for ML experimentation and model development Implement RAG Solutions: Build knowledge bases for domain-specific applications using RAG architecture Develop Bedrock Agents: Create autonomous agents for customer service or workflow automation Prompt Engineering Practice: Apply Chain-of-Thought and Few-shot techniques to improve AI responses Deploy Guardrails: Implement content filtering and safety controls for production GenAI applications MLOps Setup: Establish model monitoring and automated deployment pipelines using SageMaker capabilities Event Experience Attending the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an exceptional learning experience that provided comprehensive insights into AWS\u0026rsquo;s AI and machine learning capabilities. The event combined theoretical knowledge with practical demonstrations, giving me a clear understanding of how to implement AI/ML solutions on AWS.\nLearning from comprehensive agenda The structured agenda covered everything from foundational ML concepts to advanced Generative AI implementations. Starting with SageMaker platform overview helped me understand the complete ML lifecycle before diving into GenAI specifics. The progression from traditional ML to Generative AI showed the evolution and complementary nature of these technologies. Hands-on technical exposure The SageMaker Studio walkthrough demonstrated the practical workflow of building ML models, from data preparation to deployment. I learned about data labeling tools and how they can significantly improve model accuracy with proper data quality. The MLOps capabilities showed me how to implement continuous integration and monitoring for ML models in production. Generative AI deep dive The Amazon Bedrock session was eye-opening, showing me how to leverage foundation models without training them from scratch. Prompt Engineering techniques like Chain-of-Thought reasoning and Few-shot learning were demonstrated with practical examples. Learning about RAG architecture helped me understand how to build accurate AI applications that combine retrieval with generation. The Bedrock Agents demo showed how to build autonomous AI systems that can perform complex multi-step tasks. Practical demonstrations The live demo of building a Generative AI chatbot using Bedrock gave me a complete picture of implementation from start to finish. Seeing Guardrails in action demonstrated the importance of safety and content filtering in production GenAI applications. The comparison between Claude, Llama, and Titan models helped me understand when to use each model. Networking and discussions The workshop provided excellent networking opportunities with other AI/ML enthusiasts and practitioners in Vietnam. Discussing AI/ML landscape in Vietnam gave me context-specific insights into local market opportunities and challenges. Sharing experiences with peers helped me understand real-world implementation challenges and solutions. Lessons learned SageMaker provides a complete platform that simplifies the entire ML lifecycle, from data preparation to deployment.\nFoundation models in Bedrock eliminate the need to train large models from scratch, significantly reducing time and cost.\nRAG architecture is crucial for building accurate GenAI applications that need to reference specific knowledge bases.\nPrompt engineering is a skill that requires practice and understanding of different techniques for optimal results.\nGuardrails are essential for production GenAI applications to ensure safety and compliance.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Introduce DevOps culture, principles, and key metrics Demonstrate AWS DevOps services for CI/CD pipeline automation Explore Infrastructure as Code (IaC) with CloudFormation and CDK Cover container services and microservices deployment strategies Provide monitoring and observability best practices Share real-world DevOps case studies and best practices Event Details Date: Monday, November 17, 2025 Time: 8:30 AM – 5:00 PM Location: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: Full day (8.5 hours with breaks) Agenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 – 1:00 PM) Afternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps career pathways AWS certification roadmap Key Highlights DevOps Culture and Principles DevOps Mindset: Collaboration between development and operations teams Cultural Transformation: Breaking down silos and fostering shared responsibility Key Metrics (DORA): Measuring DevOps performance Deployment Frequency: How often deployments occur Lead Time: Time from code commit to production MTTR (Mean Time To Recovery): Time to recover from failures Change Failure Rate: Percentage of deployments causing failures Benefits: Faster delivery, improved reliability, better collaboration AWS CI/CD Pipeline Services AWS CodeCommit:\nFully managed source control service Git-based version control Integration with other AWS services Git strategies: GitFlow, Trunk-based development, feature branches AWS CodeBuild:\nFully managed build service Scalable build environments Supports multiple programming languages and build tools Build artifacts and test reports Integration with testing frameworks AWS CodeDeploy:\nAutomated deployment service Deployment strategies: Blue/Green: Zero-downtime deployments with instant rollback Canary: Gradual rollout with automatic rollback on errors Rolling: Rolling updates with configurable batch sizes Application deployment across EC2, Lambda, and on-premises AWS CodePipeline:\nFully managed continuous delivery service Visual workflow builder Integration with third-party tools Automated pipeline orchestration Approval gates and manual intervention points Infrastructure as Code (IaC) AWS CloudFormation:\nDeclarative IaC service JSON/YAML template syntax Stack management and resource provisioning Drift detection and stack updates Change sets for previewing changes Nested stacks for modular infrastructure AWS CDK (Cloud Development Kit):\nProgrammatic IaC using familiar programming languages TypeScript, Python, Java, C#, and Go support Constructs for reusable infrastructure patterns Higher-level abstractions and best practices Integration with CloudFormation CLI tools for deployment and management Choosing Between IaC Tools:\nCloudFormation: Declarative, template-based, AWS-native CDK: Programmatic, type-safe, developer-friendly Use cases and when to choose each approach Container Services on AWS Docker Fundamentals:\nContainerization benefits and use cases Microservices architecture with containers Docker image creation and optimization Multi-stage builds and best practices Amazon ECR (Elastic Container Registry):\nFully managed Docker container registry Image storage and versioning Image scanning for vulnerabilities Lifecycle policies for automated cleanup Integration with ECS and EKS Amazon ECS (Elastic Container Service):\nFully managed container orchestration Fargate (serverless) and EC2 launch types Task definitions and service configurations Auto-scaling and load balancing Service discovery and networking Amazon EKS (Elastic Kubernetes Service):\nManaged Kubernetes service Kubernetes-native orchestration Worker nodes management Add-ons and ecosystem integration Multi-tenant and namespace isolation AWS App Runner:\nSimplified container deployment Automatic scaling and load balancing Source code or container image deployment Built-in CI/CD integration Pay-per-use pricing model Monitoring \u0026amp; Observability Amazon CloudWatch:\nMetrics: Application and infrastructure metrics Logs: Centralized log management and analysis Alarms: Automated alerting and notifications Dashboards: Custom visualization of metrics and logs Insights: Automated anomaly detection Composite Alarms: Complex alerting logic AWS X-Ray:\nDistributed tracing for microservices Request flow visualization Performance bottleneck identification Service map generation Integration with Lambda, ECS, and API Gateway Trace analysis and filtering Best Practices:\nSetting up effective alerting strategies Creating meaningful dashboards On-call processes and incident response Log aggregation and analysis Metric collection and retention policies DevOps Best Practices Deployment Strategies:\nFeature Flags: Gradual feature rollouts A/B Testing: Comparing different versions Canary Deployments: Risk mitigation through gradual rollout Blue/Green Deployments: Zero-downtime updates Automated Testing:\nUnit, integration, and end-to-end testing Test automation in CI/CD pipelines Quality gates and test coverage Performance and load testing Incident Management:\nRunbook creation and maintenance Incident response procedures Postmortem analysis and learning Continuous improvement processes Key Takeaways DevOps Culture Transformation Cultural Change is Fundamental: Tools alone don\u0026rsquo;t make DevOps—culture and collaboration are key Measure What Matters: Use DORA metrics to track DevOps maturity Continuous Improvement: DevOps is a journey, not a destination Automation First: Automate repetitive tasks to focus on high-value work CI/CD Best Practices Start Simple, Scale Gradually: Begin with basic pipelines and add complexity over time Git Strategy Matters: Choose GitFlow or Trunk-based based on team size and release cadence Testing is Critical: Integrate automated testing at every stage Deployment Strategies: Use appropriate deployment strategy based on risk tolerance Infrastructure as Code: Always use IaC for reproducible and version-controlled infrastructure Container Orchestration Choose Wisely: ECS for simplicity, EKS for Kubernetes ecosystem Start with Serverless: Fargate eliminates node management overhead Optimize Images: Smaller images mean faster deployments and lower costs Security First: Scan images and use least-privilege IAM policies Observability Strategy Implement Full-Stack Observability: Metrics, logs, and traces together Proactive Monitoring: Set up alarms before incidents occur Meaningful Dashboards: Create dashboards that provide actionable insights Distributed Tracing: Essential for debugging microservices architectures Applying to Work Implement CI/CD Pipelines: Set up CodePipeline for automated deployments Adopt Infrastructure as Code: Use CloudFormation or CDK for all infrastructure Containerize Applications: Start containerizing applications for better portability Set Up Monitoring: Implement CloudWatch and X-Ray for observability Establish DevOps Practices: Create runbooks, incident response procedures, and postmortem templates Measure DevOps Metrics: Track DORA metrics to measure improvement Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop was an intensive and comprehensive learning experience that covered the entire DevOps spectrum from culture to implementation. The event provided both theoretical knowledge and practical demonstrations, giving me a complete understanding of implementing DevOps practices on AWS.\nLearning DevOps fundamentals The session started with DevOps mindset and culture, emphasizing that DevOps is more than just tools—it\u0026rsquo;s about collaboration and shared responsibility. I learned about DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) and how to measure DevOps maturity. Understanding the benefits of DevOps helped me see the bigger picture beyond technical implementation. AWS CI/CD pipeline deep dive The CodeCommit, CodeBuild, CodeDeploy, and CodePipeline walkthrough showed me how to build a complete CI/CD pipeline. Learning about different Git strategies (GitFlow vs Trunk-based) helped me understand when to use each approach. The deployment strategies (Blue/Green, Canary, Rolling) demo was eye-opening, showing how to minimize risk and downtime. The live CI/CD pipeline demo demonstrated the entire workflow from code commit to production deployment. Infrastructure as Code mastery CloudFormation demonstrated how to manage infrastructure declaratively with templates. AWS CDK showed me how to write infrastructure code in familiar programming languages, making it more maintainable. The comparison between CloudFormation and CDK helped me understand when to use each tool. Learning about drift detection and change sets gave me confidence in managing infrastructure safely. Container services exploration Docker fundamentals refreshed my understanding of containerization and its benefits. Amazon ECR showed how to manage container images securely with scanning and lifecycle policies. Comparing ECS and EKS helped me understand the trade-offs between managed services and Kubernetes flexibility. AWS App Runner introduced a simpler way to deploy containers without managing infrastructure. The microservices deployment case study provided real-world insights into choosing the right container service. Monitoring and observability setup CloudWatch comprehensive coverage showed me how to collect metrics, logs, and set up alarms. AWS X-Ray distributed tracing demonstrated how to debug complex microservices architectures. The full-stack observability demo showed how to connect all monitoring pieces together. Learning about alerting best practices and on-call processes provided practical operational knowledge. Best practices and case studies Deployment strategies like feature flags and A/B testing showed advanced techniques for safe deployments. Automated testing integration demonstrated how to build quality gates into CI/CD pipelines. Incident management practices and postmortem templates provided structure for handling production issues. Case studies from startups and enterprises showed real-world DevOps transformations and lessons learned. Career and certification guidance The DevOps career pathways discussion helped me understand different roles and skill requirements. The AWS certification roadmap provided clear guidance on certifications relevant to DevOps. Understanding the career progression gave me a roadmap for professional development. Practical demonstrations Every session included live demos that showed real implementations, not just slides. The full CI/CD pipeline walkthrough demonstrated end-to-end automation. CloudFormation and CDK demos showed both approaches to infrastructure management. Container deployment comparison helped me visualize different approaches side by side. Networking and discussions The full-day format allowed for extended networking with other DevOps practitioners. Q\u0026amp;A sessions provided opportunities to get answers to specific questions. Discussing real-world challenges with peers helped me understand common pitfalls and solutions. Lessons learned DevOps is a cultural transformation that requires buy-in from both development and operations teams.\nAutomation is essential but must be implemented thoughtfully to avoid creating technical debt.\nInfrastructure as Code is non-negotiable for modern DevOps practices.\nMonitoring and observability are crucial for maintaining production systems.\nStart simple and iterate rather than trying to implement everything at once.\nMeasure everything using DORA metrics to track improvement over time.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Introduce AWS Well-Architected Framework Security Pillar Demonstrate 5 main Security pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response Share best practices and core principles about cloud security Provide practical knowledge about threats and prevention in Vietnam Guide building security architecture according to AWS Well-Architected standards Connect with security experts and cloud practitioners Event Details Date: Saturday, November 29, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3.5 hours (including coffee break) Agenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation (20 minutes) Role of Security Pillar in Well-Architected Framework Core Principles: Least Privilege: Grant minimum necessary permissions Zero Trust: Never trust, always verify Defense in Depth: Multiple layers of protection Shared Responsibility Model: AWS and customer responsibilities Top threats in cloud environment in Vietnam Q\u0026amp;A ⭐ Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture (40 minutes) IAM Fundamentals:\nUsers, Roles, Policies – avoid long-term credentials Best practices for IAM setup Temporary credentials and session management IAM Identity Center:\nSingle Sign-On (SSO) configuration Permission sets and assignment Multi-account management Advanced IAM:\nService Control Policies (SCP) for multi-account Permission boundaries to limit permissions MFA (Multi-Factor Authentication) requirements Credential rotation strategies Access Analyzer to detect external access Mini Demo: Validate IAM Policy + simulate access\nCheck policy syntax and permissions Simulate access scenarios Troubleshoot common IAM issues ⭐ Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring (25 minutes) AWS Security Services:\nCloudTrail: Organization-level logging and audit GuardDuty: Threat detection and intelligent security Security Hub: Centralized security findings Comprehensive Logging:\nVPC Flow Logs: Network traffic monitoring ALB Access Logs: Application layer monitoring S3 Access Logs: Object access tracking Logging at every layer of infrastructure Alerting \u0026amp; Automation:\nEventBridge rules for security events Automated response workflows Integration with notification systems Detection-as-Code:\nInfrastructure as Code for security rules Version control for detection rules Automated deployment and testing 9:55 – 10:10 AM | Coffee Break (15 minutes) Break time Networking with participants Informal Q\u0026amp;A ⭐ Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security (30 minutes) Network Security:\nVPC Segmentation: Separate network segments Private vs Public placement strategies Network isolation and security zones Security Groups vs NACLs:\nWhen to use Security Groups When to use NACLs Practical application models Best practices and common mistakes Advanced Network Protection:\nAWS WAF: Web Application Firewall AWS Shield: DDoS protection Network Firewall: Managed network firewall service Workload Protection:\nEC2 Security: Instance hardening, patch management ECS Security: Container security best practices EKS Security: Kubernetes security fundamentals Security baselines and compliance ⭐ Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets (30 minutes) AWS KMS (Key Management Service):\nKey policies and access control Grants and delegation Key rotation strategies Multi-region key management Encryption at Rest:\nS3: Server-side encryption (SSE-S3, SSE-KMS, SSE-C) EBS: Volume encryption and snapshots RDS: Database encryption DynamoDB: Table encryption Encryption in Transit:\nTLS/SSL best practices Certificate management End-to-end encryption Secrets Management:\nSecrets Manager: Automated rotation patterns Parameter Store: Secure parameter storage Rotation patterns and best practices Integration with applications Data Classification \u0026amp; Access Guardrails:\nData classification frameworks Access controls based on classification Compliance and regulatory requirements ⭐ Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation (30 minutes) IR Lifecycle according to AWS:\nPrepare: Preparation and planning Detect: Incident detection Respond: Response and containment Recover: Recovery and lessons learned IR Playbooks for Common Scenarios:\n1. Compromised IAM Key:\nDetect compromised credentials Immediate response steps Key rotation and access revocation Investigation and forensics 2. S3 Public Exposure:\nDetect public buckets Immediate remediation Access review and audit Prevention strategies 3. EC2 Malware Detection:\nDetect malware and suspicious activity Isolation procedures Evidence collection Cleanup and recovery Automated Response:\nLambda functions for automated response Step Functions for complex workflows Integration with security services Playbook automation patterns Evidence Collection:\nSnapshot creation for forensics Log preservation Chain of custody Compliance with legal requirements 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A (20 minutes) Summary of 5 Security pillars Common pitfalls and frequent mistakes Vietnamese enterprise reality: Security challenges in Vietnam Compliance requirements Best practices for local context Security learning roadmap: AWS Certified Security – Specialty AWS Certified Solutions Architect – Professional Security training paths Q\u0026amp;A session Commemorative photos Key Highlights Security Foundation Principles Least Privilege:\nGrant only minimum necessary permissions to perform work Regular review and audit permissions Use temporary credentials instead of long-term keys Principle of least privilege at every layer Zero Trust:\nNever trust by default, always verify Verify identity and authorization for every request Network segmentation and micro-segmentation Continuous verification and monitoring Defense in Depth:\nMultiple layers of protection: Network, Application, Data, Identity Don\u0026rsquo;t rely on a single layer of protection Layered security controls Fail-safe defaults Shared Responsibility Model:\nAWS: Security OF the cloud (infrastructure) Customer: Security IN the cloud (data, applications, configurations) Understand responsibilities of each party Best practices for customer responsibilities Pillar 1: Identity \u0026amp; Access Management Modern IAM Architecture:\nUse IAM Roles instead of Users when possible Temporary credentials with STS IAM Identity Center for SSO Permission boundaries and SCPs Best Practices:\nEnable MFA for all users Regular credential rotation Use Access Analyzer to detect external access Least privilege policies Regular access reviews Pillar 2: Detection Comprehensive Monitoring:\nCloudTrail for audit trail GuardDuty for threat detection Security Hub for centralized view VPC Flow Logs for network monitoring Detection-as-Code:\nVersion control for detection rules Automated testing CI/CD for security rules Infrastructure as Code approach Pillar 3: Infrastructure Protection Network Security:\nVPC segmentation Security Groups and NACLs WAF, Shield, Network Firewall Private subnets and NAT gateways Workload Security:\nEC2 hardening Container security Kubernetes security Patch management Pillar 4: Data Protection Encryption:\nEncryption at rest with KMS Encryption in transit with TLS Key management best practices Secrets management Data Classification:\nClassify data by sensitivity Apply appropriate controls Access guardrails Compliance requirements Pillar 5: Incident Response IR Lifecycle:\nPrepare: Planning and tools Detect: Monitoring and alerting Respond: Containment and investigation Recover: Restoration and lessons learned Automation:\nAutomated response with Lambda Step Functions for workflows Integration with security services Playbook automation Key Takeaways Security Foundation Well-Architected Framework: Understanding Security Pillar and its role Core Principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility: AWS and customer responsibilities Threat Landscape: Top threats in Vietnam and prevention methods IAM Best Practices Modern IAM: Use roles, temporary credentials IAM Identity Center: SSO and permission management Advanced Features: SCPs, permission boundaries, Access Analyzer Security: MFA, credential rotation, least privilege Detection \u0026amp; Monitoring Security Services: CloudTrail, GuardDuty, Security Hub Comprehensive Logging: VPC Flow Logs, ALB logs, S3 logs Alerting: EventBridge and automation Detection-as-Code: Infrastructure as Code for security Infrastructure Protection Network Security: VPC segmentation, Security Groups, NACLs Advanced Protection: WAF, Shield, Network Firewall Workload Security: EC2, ECS, EKS security Best Practices: Hardening and patch management Data Protection Encryption: At rest and in transit KMS: Key management and rotation Secrets Management: Secrets Manager and Parameter Store Data Classification: Access guardrails and compliance Incident Response IR Lifecycle: Prepare, Detect, Respond, Recover Playbooks: Common scenarios and response procedures Automation: Lambda and Step Functions Forensics: Evidence collection and preservation Applying to Work Design Security Architecture: Apply 5 pillars to architecture design Implement IAM Best Practices: Use modern IAM patterns Setup Detection: Deploy comprehensive monitoring Protect Infrastructure: Apply network and workload security Protect Data: Implement encryption and secrets management Prepare IR: Build incident response playbooks and automation Security Reviews: Regular security assessments and improvements Event Experience Attending the \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive learning experience about cloud security. The event provided comprehensive knowledge about the 5 Security pillars and practical best practices, helping me understand how to build secure cloud architecture.\nSecurity Foundation Opening session introduced Security Pillar in Well-Architected Framework. I learned about core principles: Least Privilege, Zero Trust, Defense in Depth. Shared Responsibility Model helped me understand AWS and customer responsibilities. Top threats in Vietnam provided real-world context for security challenges. Modern IAM Architecture IAM session went deep into modern IAM patterns and best practices. Learning about IAM Identity Center for SSO and multi-account management. Advanced features like SCPs and permission boundaries were very useful. Demo validate IAM policy showed practical approach to test policies. Detection \u0026amp; Continuous Monitoring Detection session covered comprehensive monitoring strategy. CloudTrail, GuardDuty, and Security Hub form a powerful security monitoring stack. Logging at every layer helped me understand defense in depth. Detection-as-Code approach was very innovative and practical. Network \u0026amp; Workload Security Infrastructure Protection session went deep into network security. Clear understanding of when to use Security Groups vs NACLs. Advanced protection with WAF, Shield, Network Firewall. Workload security for EC2, ECS, EKS provided practical guidance. Data Protection Data Protection session covered encryption and secrets management. KMS key management and rotation strategies are very important. Encryption at rest and in transit for all services. Secrets Manager patterns for automated rotation. Incident Response IR session provided practical playbooks for common scenarios. Learning about IR lifecycle and best practices. Automated response with Lambda and Step Functions is very powerful. Evidence collection procedures for forensics and compliance. Wrap-up and Q\u0026amp;A Wrap-up session summarized 5 pillars comprehensively. Common pitfalls helped me avoid frequent mistakes. Vietnamese enterprise reality provided local context. Learning roadmap for security certifications was very useful. Lessons Learned Security is foundation: Must design security from the start, not add-on later.\nDefense in Depth: Don\u0026rsquo;t rely on a single layer of protection.\nAutomation is key: Automated detection and response reduce response time.\nContinuous improvement: Security is an ongoing process, not a one-time setup.\nCompliance matters: Understand regulatory requirements and best practices.\nPractice makes perfect: Need regular practice and review.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Integration with Amazon Bedrock:\nSeamless integration with Bedrock models Custom model selection Prompt engineering and optimization Response formatting and validation Context optimization APIs Key Takeaways Understanding Building Agentic AI What Agentic AI is: Autonomous AI agents capable of performing tasks independently Context Optimization: Techniques to optimize context and reduce costs Use cases: Real-world use cases for agentic AI Architecture: Architecture and design patterns AWS Bedrock Agent Bedrock Agent Core: Understanding how agents work and interact Context Management: Managing and optimizing context Integration: How to integrate with other AWS services Best Practices: Best practices from AWS experts Agentic Workflow Design Workflow patterns: Common patterns for agentic workflows Orchestration: How to manage and coordinate multiple agents Context Optimization: Strategies for context optimization Error handling: Strategies for error handling and recovery CloudThinker Platform Agentic orchestration: How CloudThinker addresses orchestration challenges Context optimization: Advanced techniques to optimize context Platform capabilities: Features and capabilities of CloudThinker Integration patterns: How to integrate CloudThinker into existing systems Hands-on Experience Practical skills: Practical skills in building Bedrock Agents Context optimization: Practicing context optimization techniques Troubleshooting: How to debug and troubleshoot common issues Real-world scenarios: Working with real-world scenarios Applying to Work Build Agentic AI: Use AWS Bedrock Agent to build autonomous AI agents Context Optimization: Apply context optimization techniques to reduce costs and improve performance Design Workflows: Apply agentic workflow patterns to projects Integrate CloudThinker: Evaluate and integrate CloudThinker into existing solutions Best Practices: Apply best practices from the workshop to production systems Event Experience Attending the \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; workshop was an intensive learning experience about agentic AI and context optimization. The event provided both theoretical knowledge and hands-on practice, giving me a clear understanding of how to build and optimize autonomous AI agents.\nOpening and Introduction Opening session by Nguyen Gia Hung created a professional and inspiring atmosphere. I understood the importance of Building Agentic AI and Context Optimization. Overview of event agenda helped me visualize the learning journey. AWS Bedrock Agent Core Session by Kien Nguyen provided a solid foundation about Bedrock Agent. I learned about architecture, components, and how Bedrock Agent works. Context management was a key highlight in this session. Demo of creating Bedrock Agent showed the actual process from start to finish. Real-World Use Case Use case presentation by Viet Pham illustrated how agentic workflows are used in production. Learning about real-world challenges and how to solve them. Context optimization in production was very practical and insightful. Agentic workflow demo showed performance and capabilities in practice. CloudThinker Platform CloudThinker introduction by Thang Ton introduced the platform and solution. L300 session by Henry Bui went deep into technical details and advanced patterns. Learning about advanced context optimization techniques to improve performance and reduce costs. CloudThinker platform demo showed capabilities and ease of use. Hands-on Workshop Hands-on workshop by Kha Van provided direct practice opportunities. Building Bedrock Agent from scratch with guidance from an expert. Practicing context optimization and agentic orchestration. Troubleshooting session helped me understand how to solve common issues. Direct Q\u0026amp;A provided answers to specific questions. Networking and Connections Networking sessions allowed connections with AWS experts and AI practitioners. Sharing experiences and learnings with other participants. Lunch buffet created opportunities for informal discussions and connections. Meeting experts and receiving advice about career development. Lessons Learned Agentic AI is the future: Autonomous AI agents will change how we build AI applications.\nContext Optimization is key: Optimizing context can significantly reduce costs and improve performance.\nHands-on practice is essential: Practical experience is crucial to truly understand and apply concepts.\nPlatform solutions matter: CloudThinker and similar platforms simplify building agentic systems.\nCommunity is valuable: Networking with experts and practitioners provides valuable insights and opportunities.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Dao Nguyen Khoi\nPhone Number: 0904096480\nEmail: khoi.dao2411@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Giao lưu và tìm hiểu thêm về team First Cloud Journey. Tìm hiểu về các dịch vụ cơ bản của AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know about FCJ members\n- Read and try to remember about internship rules and regulations 09/02/2025 09/02/2025 3 - Study AWS and its main service groups:\n+ Compute + Storage + Networking + Database 09/03/2025 09/03/2025 https://000001.awsstudygroup.com/en/ 4 - Register AWS Free Tier account\n- Explore and Test about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + Basic CLI tasks 09/04/2025 09/04/2025 https://000001.awsstudygroup.com/en/ 5 - Configure basic security:\n+ Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support + Create Budget 09/05/2025 09/05/2025 https://000007.awsstudygroup.com/en/ 6 -Practice cost management: + Create Cost Budget + Create Usage Budget + Reservation Instance (RI) + Savings Plans Budget 09/06/2025 09/06/2025 https://000007.awsstudygroup.com/en/ 7 -Submit AWS support request and manage responses. -Write worklog \u0026amp; self-assess AWS fundamentals. - Prepare for Week 2 goals. volume 09/07/2025 09/07/2025 https://000009.awsstudygroup.com/en/ Week 1 Result: About internship:\nCompleted onboarding/introduction with Mr. Hung and the FCJ team. Familiarized myself with the internship policies and regulations. Understood what AWS stands for , learned and understood the basic service groups:\nCompute Storage Networking Database Set up account and Finished configuration:\nRegistered account with AWS Free Tier Configured basic security: enabled MFA, created admin group \u0026amp; admin user (IAM user). Created budgets to monitor costs: Cost Budget, Usage Budget, RI, Savings Plans. Management tools:\nTested and used AWS Management Console: navigating and using services via GUI. Installed and configured AWS CLI with: Access Key, Secret Key, default Region. Hands-on labs with AWS CLI:\nChecked account and configuration info. Listed regions. Viewed EC2 information. Created and managed key pairs. Monitored running services. Console \u0026amp; CLI integration:\nManaged AWS resources in parallel using Console and CLI. Compared approaches and gained insights on when to use each tool. Personal reflection:\nFinished Week 1 worklog reflected the lack of current understand about work and AWS fully Will try to understand fully in upcoming weeks "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept and structure of VPC (CIDR, Subnet, Route Table, ENI). Learn to configure firewalls in VPC (NACL, Security Group). Understand the concept of networking services: VPN, Direct Connect. Experience on Load Balancer Experience on creating and configuring core components: VPC, Subnet, Route Table, IGW, EBS, Elastic IP. Understand about connection to EC2 using remote via SSH Experience Hybrid DNS using Route 53 Resolver. Experience multiple VPCs using VPC Peering. Testing AWS Transit Gateway to manage inter-VPC connections Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory - What is VPC and how to optimize cloud service usage - Learn about VPC: + Subnet, CIDR + Route table + ENI (Elastic Network Interface)s 09/15/2025 09/15/2025 1. AWS VPC Documentation 2. YouTube - AWS VPC 3 - Configure VPC firewalls: NACL, Security Group - VPN, Direct Connect - Load Balancer - Extra Resources 09/14/2025 09/14/2025 YouTube - AWS Security 4 - Labs: + VPC + Subnet + Route Table + IGW(Internet GateWay) + EBS + \u0026hellip; - Remote SSH into EC2 - Learn Elastic IP 09/15/2025 09/15/2025 AWS Study Group - 000003 : Amazon VPC and AWS Site-to-Site VPN Workshop 5 - Labs: + Set up Hybrid DNS with Route 53 Resolver + Set up VPC Peering 09/16/2025 09/16/2025 AWS Study Group - 000010 : Set up Hybrid DNS with Route 53 Resolver AWS Study Group - 000019 : Setting up VPC Peering 6 - Labs: + Continue on experiencing VPC Peering + Set up AWS Transit Gateway 09/17/2025 09/17/2025 AWS Study Group - 000020 : Set up AWS Transit Gateway Week 2 Result: Understood the fundamental of Amazon VPC, and its key components: CIDR blocks, subnets, route tables, and ENIs. Experienced the activity of securing VPCs using both Security Groups and Network ACLs, and understand their differences in scope and use cases. Explored AWS networking services : VPN and Direct Connect Learnt Elastic Load Balancing and its role in distributing traffic for high availability. Did labs about VPC essentials: creating subnets, configuring route tables, attaching internet gateways, working with EBS, and managing Elastic IPs. Reinforced skills in accessing and managing EC2 instances securely via SSH. Tested and practiced connecting multiple VPCs through VPC Peering. Deployed AWS Transit Gateway to design and manage scalable multi-VPC architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Build a Strong foundation of Amazon EC2 and its ecosystem (AMI, Backup, Key Pair, EBS, Instance Store, User Data, Metadata) knowledge. Learn about EC2 Auto Scaling and its role in elasticity and cost optimization. Search and experience related compute services including EFS, FSx, Lightsail, and AWS MGN. Reinforce AWS storage knowledge with practicing labs covering S3, AWS Backup, and Storage Gateway. Reinforce practical skills in configuring, managing, and scaling EC2 workloads. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory: + EC2 overview: AMI, Backup, Key Pair + EBS (Elastic Block Store) + Instance Store 09/22/2025 09/22/2025 1. AWS EC2 Documentation 2. AWS EBS Documentation 3 - Learn theory: + EC2 User Data + EC2 Metadata 09/23/2025 09/23/2025 AWS EC2 User Guide 4 - Learn theory: + EC2 Auto Scaling + Related services: EFS, FSx, Lightsail, MGN 09/24/2025 09/24/2025 AWS Auto Scaling 5 - Labs: + Lab 57: Start with Amazon S3 09/25/2025 09/25/2025 AWS Study Group - Lab57 : STARTING WITH AMAZON S3 6 - Labs: + Lab 13: Deploy AWS Backup to the System + Lab 24: Using AWS Storage Gateway 09/26/2025 09/26/2025 AWS Study Group - Lab13 :Deploy AWS Backup to the System AWS Study Group - Lab24:Using File Storage Gateway Week 3 Achievements: Made a solid theoretical knowledge foundation of Amazon EC2, including:\nAMI and backup strategies for resilience. Key Pair usage for secure SSH authentication. Differences between EBS (persistent storage) and Instance Store (ephemeral storage). How User Data and Metadata scripts automate instance initialization and provide dynamic configuration. The role of EC2 Auto Scaling in maintaining performance and cost efficiency. Learnt the related services:\nAmazon EFS and FSx for shared and high-performance file storage. Amazon Lightsail as a simplified alternative for small-scale workloads. AWS MGN for migrating workloads into AWS. Completed practical labs:\nLaunched and managed an S3 bucket (Lab57). Implemented AWS Backup to protect workloads (Lab13). Integrated on-premises systems with AWS using Storage Gateway (Lab24). Key skills acquired:\nConfidently distinguish storage types (EBS vs Instance Store vs EFS vs FSx). Automate EC2 lifecycle with User Data and Auto Scaling. Combine backup and hybrid storage solutions to create more resilient architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand further about AWS’s core storage service Amazon S3. Further comprehension of key concepts: bucket, object, storage class, access point, static website hosting, and CORS. Study hybrid storage and data migration solutions such as AWS Storage Gateway and AWS Snow Family. experience with Amazon FSx for Windows File Server and the automated backup service AWS Backup. Practice deploying, managing, and integrating AWS storage services in a real-world environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn the theory of AWS Storage Service (S3) - Get familiar with the concepts of Bucket, Object, and the storage mechanism. 09/29/2025 09/29/2025 1. AWS S3 Documentation 3 - Learn about Access Point and Storage Class - Distinguish between storage classes: Standard, IA, Glacier, Deep Archive. 09/30/2025 09/30/2025 AWS S3 Storage Class info 4 - Explore S3 Static Website \u0026amp; CORS, Access Control, Object Key, Performance, and Glacier 10/01/2025 10/01/2025 AWS S3 web hosting User guide 5 - Labs: + Lab14 – VM Import/Export. 10/02/2025 10/02/2025 AWS Study Group - Lab14 : VM Import/Export 6 - Labs: + Lab25 – Amazon FSx for Windows File Server. - Review and consolidate all AWS storage services. 10/03/2025 10/03/2025 AWS Study Group - Lab25 :Amazon FSx for Windows File Server Week 4 Achievements: Understood architecture and operating principles of Amazon S3, including:\nHow to create and manage Buckets, Objects, and Access Policies. Different Storage Classes and strategies for optimizing storage costs. How to configure S3 Static Website Hosting and handle CORS for web applications. Experienced with S3 Glacier – a cold storage service that helps save costs for infrequently accessed data.\nUnderstood of Hybrid Storage \u0026amp; Data Migration through:\nAWS Snow Family (Snowcone, Snowball, Snowmobile). AWS Storage Gateway – a solution to connect on-premises systems with AWS Cloud. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand the Shared Responsibility Model of AWS. Learn that the key AWS security services are: IAM, Cognito, Security Hub, KMS, Identity Center. Reinforce resource management and security through IAM Permissions Boundaries, Resource Tags, and encryption techniques. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn the theory of the Shared Responsibility Model and AWS security principles. - Review documentation for AWS security services: + Amazon IAM + Amazon Cognito + AWS Identity Center + AWS KMS + AWS Security Hub 10/06/2025 10/06/2025 AWS Study group: Deploy AWS Backup to the System 3 - Labs: + Configure and use AWS Security Hub to monitor and detect security issues. + Create and manage IAM Users, Roles, and Policies for AWS accounts. + Create IAM Groups and manage access permissions for user groups. 10/07/2025 10/07/2025 AWS Study group: Optimizing EC2 Costs with Lambda 4 - Labs: + Optimize EC2 costs using Lambda for automated start/stop of EC2 instances. + Manage EC2 access via Resource Tags using IAM. 10/08/2025 10/08/2025 AWS Study Group :Manage Resources Using Tags and Resource Groups 5 - Labs: + Configure IAM Permission Boundaries to limit user privileges. + Encrypt data using AWS KMS. 10/09/2025 10/09/2025 AWS Study Group : Manage Resources Using Tags and Resource Groups 6 - Further Experiment + Learn and apply security methods in AWS Organizations for multi-account management. + Enhance proficiency in AWS Identity Center for managing and synchronizing users and groups across AWS services. 10/10/2025 10/10/2025 AWS Study Group - : LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY Week 5 Achievements: Understanding and Applying the AWS Shared Responsibility Model\nUnderstood that AWS is responsible for the security of the cloud infrastructure, while users are responsible for securing their own data and applications. Clarified the role in ensuring compliance and protection when deploying services on AWS. Theoretical Knowledge of AWS Core Security Services\nAmazon IAM: Learned how to create and manage Users, Roles, and Policies to control user and group access. Amazon Cognito: Studied user management and authentication for AWS applications. AWS Identity Center: Understood how to link and manage user access across multiple AWS services. AWS Security Hub: Configured and utilized it to monitor and detect security threats. AWS KMS: Practiced encrypting data at rest and securing sensitive data using encryption keys. Practical Implementation of AWS Security Services\nSuccessfully installed and configured AWS Security Hub for continuous monitoring and vulnerability detection. Configured IAM Permissions Boundaries to restrict user privileges and prevent unauthorized access. EC2 Cost Optimization with Lambda: Automated the shutdown of unused EC2 instances to minimize operational costs. EC2 Access Control via IAM \u0026amp; Resource Tags: Applied IAM Policies that use Tags to precisely define access scope. Enhanced AWS Resource Management Skills\nCreated and managed IAM Groups and Policies, improving group-based access control. Learned how to manage multiple AWS accounts using AWS Organizations, ensuring consistent security policies across the organization. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Review fundamental Database Concepts: relational model, primary/foreign keys, ACID, normalization, OLTP vs OLAP. Understand Amazon RDS as a managed relational database service on AWS: engines, Multi-AZ, read replicas, backup, and scaling. Learn the benefits of Amazon Aurora compared to standard RDS engines: performance, high availability, automatic storage scaling, MySQL/PostgreSQL compatibility. Get familiar with Amazon Redshift as a petabyte-scale data warehouse for analytics, and distinguish it from RDS (OLTP workloads). Learn how Amazon ElastiCache (Redis / Memcached) provides an in-memory cache layer to reduce latency and offload backend databases. Practice Database Schema Conversion \u0026amp; Migration using AWS DMS and AWS Schema Conversion Tool (SCT) for moving databases to AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Database Concepts : relational model, ACID, transactions, indexing, normalization, OLTP vs OLAP. - Map traditional on-premises database concepts to AWS cloud services. 10/13/2025 10/13/2025 AWS Study group Youtube Video 3 - Study Amazon RDS \u0026amp; Amazon Aurora theory . - Learn about supported engines, Multi-AZ, automated backups, snapshots, read replicas, and scaling. - Compare RDS vs Aurora in terms of performance, availability, and cost. 10/14/2025 10/14/2025 AWS AmazonRDS user guide AWS AmazonRDS Aurora document 4 - Study Amazon Redshift \u0026amp; Amazon ElastiCache . - Distinguish OLTP (RDS/Aurora) vs OLAP (Redshift) and in-memory cache layer (ElastiCache). - Explore common use cases: data warehouse \u0026amp; BI, caching sessions, leaderboard, rate limiting, etc. 10/15/2025 10/15/2025 Info about AWS Redshift Info about AWS Elasticache 5 - Practice: + Module 06-Lab 5 – Amazon Relational Database Service (Amazon RDS). + Create an RDS instance, configure security group, parameter group, backups. + Connect from a client, run queries, and test behavior 10/16/2025 10/16/2025 Amazon Relational Database Service (Amazon RDS) 6 - Practice: + Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration. + Use AWS Schema Conversion Tool (SCT) to analyze and convert schema from source DB to RDS/Aurora/Redshift target. + Use AWS Database Migration Service (DMS) to migrate data . - Summarize and review all AWS Database Services . 10/17/2025 10/17/2025 Database Schema Conversion \u0026amp; Migration Week 6 Achievements: Consolidated understanding of core database concepts:\nRelational tables, primary/foreign keys, relational integrity, and basic indexing. ACID properties of transactions and why they matter in OLTP workloads. Clear distinction between OLTP vs OLAP and how this maps to AWS services. Gained hands-on familiarity with Amazon RDS:\nCreated and managed RDS instances via AWS Management Console. Reviewed supported engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) and their typical use cases. Practiced configuring Multi-AZ, automated backups, snapshots, monitoring, and basic scaling options. Understood the strengths of Amazon Aurora:\nAurora as a cloud-native, MySQL/PostgreSQL-compatible database with significantly improved performance over standard engines. Aurora DB cluster architecture, with a distributed storage layer across multiple AZs. Reader and writer endpoints, automatic storage scaling, and high availability design. Understand the usage of Amazon Redshift \u0026amp; ElastiCache:\nRedshift as a columnar, petabyte-scale data warehouse optimized for analytics and BI workloads. How Redshift differs from RDS/Aurora: optimized for complex queries over large datasets rather than transactional workloads. ElastiCache (Redis/Memcached) as a fully managed, low-latency in-memory cache layer to increase throughput and reduce load on backend databases. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand Amazon DynamoDB as a fully managed NoSQL database service: key-value and document data models, partition keys, sort keys, global secondary indexes (GSI), and on-demand vs provisioned capacity modes. Learn how to build and manage Data Lakes on AWS using services like Amazon S3, AWS Glue, Amazon Athena, and Amazon QuickSight for analytics workloads. Explore AWS Analytics services: Amazon Athena for serverless SQL queries on S3, AWS Glue for ETL operations, and Amazon QuickSight for business intelligence and visualization. Practice data ingestion, transformation, and analysis workflows in the AWS cloud environment. Understand cost optimization and performance tuning strategies for analytics workloads on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Data Lake on AWS. + Understand data lake architecture on AWS using S3 as the data lake storage layer. + Learn about data ingestion, cataloging, and querying patterns. + Explore integration between S3, Glue, and Athena for analytics. 10/20/2025 10/20/2025 Data Lake on AWS 3 - Practice: Amazon DynamoDB Immersion Day. + Deep dive into DynamoDB core concepts: tables, items, attributes, primary keys, and indexes. + Practice creating tables, inserting data, and querying with partition keys and sort keys. + Understand DynamoDB capacity modes (on-demand vs provisioned) and pricing models. 10/21/2025 10/21/2025 Amazon DynamoDB Immersion Day 4 Practice: Cost and performance analysis with AWS Glue and Amazon Athena. + Use AWS Glue to catalog data stored in S3 and create data catalogs. + Run SQL queries on S3 data using Amazon Athena. + Analyze cost implications and optimize query performance. + Understand partitioning strategies for cost-effective analytics. Practice: Work with Amazon DynamoDB. + Create DynamoDB tables with appropriate key schemas. + Perform CRUD operations (Create, Read, Update, Delete) on DynamoDB items. + Work with Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI). + Practice querying and scanning operations, understanding the differences. 10/22/2025 10/22/2025 Cost and performance analysis with AWS Glue and Amazon Athena Work with Amazon DynamoDB 5 - Practice: Building a Datalake with Your Data. + Build a complete data lake solution using AWS services. + Implement data ingestion pipelines. + Set up data transformation workflows with AWS Glue. + Create analytics-ready datasets for downstream consumption. - Practice: Analytics on AWS workshop. + Comprehensive workshop covering the full analytics stack on AWS. + Integrate multiple services: S3, Glue, Athena, and visualization tools. + Build end-to-end analytics solutions from raw data to insights. 10/23/2025 10/23/2025 Building a Datalake with Your Data Analytics on AWS workshop 6 - Practice: Get started with QuickSight. + Create visualizations and dashboards using Amazon QuickSight. + Connect QuickSight to various data sources (S3, Athena, RDS, etc.). + Use AWS Database Migration Service (DMS) to migrate data . + Build interactive reports and share insights with stakeholders. 10/24/2025 10/24/2025 Get started with Quick Sight Week 7 Achievements: Gained comprehensive understanding of Amazon DynamoDB:\nDynamoDB as a fully managed, serverless NoSQL database service with single-digit millisecond latency. Key concepts: tables, items, attributes, primary keys (partition key + optional sort key), and data modeling best practices. Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI) for flexible query patterns. Capacity modes: on-demand (pay-per-request) vs provisioned (reserved capacity) and when to use each. DynamoDB Streams for real-time data processing and change data capture. Got more experience on Data Lake architecture on AWS:\nAmazon S3 as the foundation for data lake storage with lifecycle policies, versioning, and encryption. Data lake architecture patterns: raw zone, processed zone, and curated zone. Data ingestion strategies: batch uploads, streaming data, and integration with various data sources. Best practices for organizing data in S3: partitioning, naming conventions, and folder structures. Understood more about AWS Analytics services:\nAWS Glue: Serverless ETL service for discovering, cataloging, and transforming data. Glue Data Catalog as a centralized metadata repository. Glue ETL jobs for data transformation using Apache Spark. Glue Crawlers for automatic schema discovery. Amazon Athena: Serverless interactive SQL query service for analyzing data in S3. Pay-per-query pricing model and cost optimization strategies. Integration with Glue Data Catalog for schema-on-read queries. Query performance optimization through partitioning and columnar formats (Parquet, ORC). Amazon QuickSight: Cloud-native business intelligence and visualization service. Creating dashboards, visualizations, and reports. Connecting to various data sources (S3, Athena, RDS, Redshift, etc.). Sharing insights with teams and embedding analytics in applications. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Complete the Edge Layer and Frontend Storage: Route 53, S3, CloudFront, AWS WAF, and ACM Certificate. Set up DNS management with Route 53 hosted zone and domain configuration. Configure S3 bucket for static frontend hosting with proper access policies. Deploy CloudFront distribution for global content delivery with Origin Access Control. Implement AWS WAF protection with security rules (SQL injection, XSS, bot control). Set up ACM Certificate and enable HTTPS for secure content delivery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - System Requirements Analysis \u0026amp; Architecture Design: + Analyze system requirements and review the complete architecture diagram. + Identify all components: Route 53, S3, CloudFront, WAF, ACM, VPC, EC2, RDS, API Gateway. + Create High-Level Design (HLD) document with architecture overview. + Document data flow: Users → Route 53 → CloudFront → WAF → S3 (Frontend). + Plan IP addressing scheme and resource naming conventions. 10/26/2025 10/26/2025 Architecture diagram 3 - Route 53 Setup: + Create Route 53 hosted zone for domain management. + Create A record pointing to CloudFront distribution + Create CNAME records for subdomains if needed. + Configure DNS settings and verify domain ownership. + Document DNS configuration and record types. - S3 Frontend Bucket Configuration: + Create S3 bucket for frontend static assets (FE Bucket) with appropriate naming. + Enable static website hosting on S3 bucket. + Configure public access policy for CloudFront access (block public access, allow CloudFront via OAC). + Upload test frontend files (HTML, CSS, JS, images) to S3 bucket. + Test static website hosting endpoint and verify file accessibility. 10/27/2025 10/27/2025 Route 53 documentation S3 documentation 4 - CloudFront Distribution Setup: + Create CloudFront distribution with S3 bucket as origin. + Configure Origin Access Control (OAC) for secure S3 access (replacing OAI). + Set up cache policies (CachingOptimized, CachingDisabled, etc.). + Configure default root object (index.html). + Map Route 53 domain to CloudFront distribution . + Test CloudFront distribution and verify content delivery. 10/28/2025 10/28/2025 CloudFront documentation 5 - AWS WAF Integration: + Create AWS WAF WebACL for CloudFront protection. + Add managed rules: AWS Managed Rules for SQL injection protection. + Add managed rules: AWS Managed Rules for XSS (Cross-Site Scripting) protection. + Configure bot control rules to block common bots and scrapers. + Associate WAF WebACL with CloudFront distribution. + Test WAF rules by attempting common attack patterns and verify blocking. 10/29/2025 10/29/2025 AWS WAF documentation 6 - AWS WAF Integration: + Request ACM Certificate in us-east-1 region (required for CloudFront). + Validate certificate using DNS validation method (add CNAME records to Route 53). + Wait for certificate validation and issuance. + Bind ACM certificate to CloudFront distribution. + Configure CloudFront to use HTTPS only (redirect HTTP to HTTPS). + Test HTTPS connection and verify SSL/TLS certificate is working correctly. 10/30/2025 10/30/2025 ACM documentation Week 8 Achievements: Successfully completed system analysis and architecture design:\nAnalyzed system requirements and reviewed complete architecture diagram. Created High-Level Design (HLD) document with architecture overview and component relationships. Documented data flow from users through edge services to frontend storage. Established resource naming conventions and planning documentation. Set up Route 53 DNS management:\nCreated Route 53 hosted zone for domain management. Configured A and CNAME records for domain routing. Established DNS foundation for connecting domain to CloudFront distribution. Configured S3 for static frontend hosting:\nCreated S3 bucket for frontend static assets with proper naming conventions. Enabled static website hosting on S3 bucket. Configured public access policies: blocked public access, allowed CloudFront access via Origin Access Control. Uploaded test frontend files and verified static website hosting functionality. Deployed CloudFront distribution:\nCreated CloudFront distribution with S3 bucket as origin. Configured Origin Access Control (OAC) for secure S3 access (modern replacement for OAI). Set up cache policies for optimized content delivery. Mapped Route 53 domain to CloudFront distribution. Verified content delivery through CloudFront CDN globally. Implemented AWS WAF protection:\nCreated AWS WAF WebACL with comprehensive security rules. Added AWS Managed Rules for SQL injection protection. Added AWS Managed Rules for XSS (Cross-Site Scripting) protection. Configured bot control rules to block malicious bots and scrapers. Associated WAF WebACL with CloudFront distribution. Tested WAF rules and verified protection against common attack patterns. Enabled HTTPS with ACM Certificate:\nRequested and validated ACM Certificate in us-east-1 region (required for CloudFront). Used DNS validation method with CNAME records in Route 53. Bound ACM certificate to CloudFront distribution. Configured CloudFront to enforce HTTPS (redirect HTTP to HTTPS). Verified SSL/TLS certificate is working correctly and secure connections are established. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Build VPC and Networking Core: Create VPC with public and private subnets, Internet Gateway, and NAT Gateway. Establish secure network boundaries with proper routing and subnet segmentation. Configure Security Groups for EC2, RDS, and ALB following least-privilege principles. Set up IAM roles and policies for EC2 instances with custom permissions. Enable VPC Flow Logs for network traffic monitoring and auditing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - VPC Creation \u0026amp; Subnet Configuration: + Create VPC with CIDR block 10.0.0.0/16 in selected AWS region. + Create public subnet (10.0.1.0/24) in one Availability Zone with appropriate tags. + Create private subnet (10.0.2.0/24) in the same Availability Zone with appropriate tags. + Apply consistent tagging strategy (Name, Environment, Project, etc.) to all resources. + Document subnet allocation and IP addressing scheme. 02/11/2025 02/11/2025 AWS VPC documentation 3 - Internet Gateway Setup: + Create and attach Internet Gateway to VPC. + Configure public subnet route table to route internet-bound traffic (0.0.0.0/0) to Internet Gateway. + Verify public subnet route table configuration. + Test internet connectivity from public subnet (launch test EC2 instance if needed). + Document routing configuration and gateway associations. 03/11/2025 03/11/2025 Internet Gateway guide 4 - NAT Gateway Configuration: + Allocate Elastic IP address for NAT Gateway. + Create NAT Gateway in public subnet (10.0.1.0/24). + Configure private subnet route table to route internet-bound traffic (0.0.0.0/0) through NAT Gateway. + Verify private subnet route table configuration. + Test outbound internet connectivity from private subnet (launch test EC2 instance in private subnet). + Verify private subnet instances can reach internet while remaining isolated from inbound connections. 04/11/2025 04/11/2025 NAT Gateway documentation 5 - Security Groups Design \u0026amp; Implementation: + Create Security Group for EC2 instances: allow inbound from API Gateway/ALB, outbound to RDS and internet via NAT. + Create Security Group for RDS: allow inbound only from EC2 Security Group on database port (3306/5432). + Create Security Group for ALB (if used): allow inbound HTTP/HTTPS from internet, outbound to EC2 Security Group. + Apply least-privilege principle: grant minimum necessary permissions. + Document security group rules and relationships. 05/11/2025 05/11/2025 Security Groups guide 6 - IAM Roles \u0026amp; Policies for EC2: + Create IAM role for EC2 instances with descriptive name. + Create custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch, etc.). + Attach IAM role to EC2 instance profile. + Test IAM role permissions from EC2 instance (use AWS CLI or SDK). + Verify EC2 can access Secrets Manager to retrieve database credentials. + Document IAM roles and their permissions. 06/11/2025 06/11/2025 IAM best practices 7 - Network ACLs \u0026amp; VPC Flow Logs: + Review and configure Network ACLs (optional, default ACLs are usually sufficient). + Test Network ACL rules if custom rules are implemented. + Enable VPC Flow Logs to capture IP traffic flow information. + Configure Flow Logs destination (CloudWatch Logs or S3 bucket). + Review Flow Logs to audit network traffic patterns. + Audit and document all network configurations for security review. - Week 9 Summary: VPC and networking core complete, ready for backend and database deployment in Week 10. 07/11/2025 07/11/2025 VPC Flow Logs documentation Week 9 Achievements: Successfully created VPC and subnet infrastructure:\nCreated VPC with CIDR block 10.0.0.0/16 in selected AWS region.\nConfigured public subnet (10.0.1.0/24) for internet*facing resources with proper tagging.\nConfigured private subnet (10.0.2.0/24) for application servers with proper tagging.\nApplied consistent tagging strategy across all network resources for better management.\nSet up Internet Gateway for public subnet connectivity:\nCreated and attached Internet Gateway to VPC. Configured public subnet route table to route internet traffic (0.0.0.0/0) to Internet Gateway. Verified public subnet instances can access internet directly. Documented routing configuration and gateway associations. Configured NAT Gateway for private subnet outbound access:\nAllocated Elastic IP address and created NAT Gateway in public subnet. Configured private subnet route table to route internet traffic through NAT Gateway. Verified private subnet instances can reach internet for outbound connections (updates, downloads, API calls). Confirmed private subnet remains isolated from inbound internet connections (security best practice). Implemented Security Groups following least*privilege principles:\nCreated Security Group for EC2: allows inbound from API Gateway/ALB, outbound to RDS and internet. Created Security Group for RDS: allows inbound only from EC2 Security Group on database port. Created Security Group for ALB (if used): allows inbound HTTP/HTTPS, outbound to EC2. Applied least-privilege principle: granted minimum necessary permissions for each component. Documented security group rules and relationships for maintainability. Configured IAM roles and policies for EC2:\nCreated IAM role for EC2 instances with descriptive naming. Created custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch). Attached IAM role to EC2 instance profile. Tested IAM permissions from EC2 instance and verified access to Secrets Manager. Documented IAM roles and permissions for security audit. Enabled VPC Flow Logs for network monitoring:\nEnabled VPC Flow Logs to capture IP traffic flow information. Configured Flow Logs destination (CloudWatch Logs or S3 bucket). Reviewed Flow Logs to audit network traffic patterns and identify anomalies. Audited all network configurations for security compliance. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "This worklog is what I have been documenting my journey in AWS First Cloud Journey internship program, After 12 Exciting weeks learning and doing labs , I finally gained a fundamental AWS concepts,its core values. This help me deploy web applicaiton architechture on AWS\nDuration: 12 weeks (approximately 3 months) Completion Date: November 2025 Final Project: Production-ready AWS web application with CI/CD, monitoring, and security Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Basic AWS services exploration\nWeek 3: Advanced AWS concepts\nWeek 4: Hands-on labs and practice\nWeek 5: Advanced labs and workshops\nWeek 6: Database Services on AWS\nWeek 7: Analytics and Data Lake Services\nWeek 8: Edge Layer and Frontend Infrastructure Week 9: VPC and Networking Core\nWeek 10: Backend and Database Deployment\nWeek 11: CI/CD Pipeline and Monitoring\nWeek 12: Worklog finalization and presetation preparation\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/5.3.1-build-upload/",
	"title": "Build and Upload Backend",
	"tags": [],
	"description": "",
	"content": "Build Spring Boot Application In this section, you will build the Spring Boot backend application into a JAR file and upload it to S3 for deployment.\nStep 1: Build the JAR File Navigate to the backend directory: cd BE/workshop_BE Clean and build the project: On Windows:\n.\\mvnw.cmd clean package -DskipTests On Linux/Mac:\n./mvnw clean package -DskipTests Verify the JAR file was created: # Windows dir target\\workshop-0.0.1-SNAPSHOT.jar # Linux/Mac ls -lh target/workshop-0.0.1-SNAPSHOT.jar Expected Result: The file workshop-0.0.1-SNAPSHOT.jar should be in the target directory.\nStep 2: Upload JAR to S3 Get the backend bucket name from CloudFormation outputs: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BackendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Upload the JAR file to S3: # Windows aws s3 cp BE\\workshop_BE\\target\\workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 # Linux/Mac aws s3 cp BE/workshop_BE/target/workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Expected Result: upload: .../workshop-0.0.1-SNAPSHOT.jar to s3://...\nVerify the upload: aws s3 ls s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Step 3: Get EC2 Instance ID You\u0026rsquo;ll need an EC2 instance ID to deploy the application. Get it from the Auto Scaling Group:\naws autoscaling describe-auto-scaling-groups \\ --region ap-southeast-1 \\ --query \u0026#34;AutoScalingGroups[?contains(AutoScalingGroupName, \u0026#39;workshop-aws-dev\u0026#39;)].Instances[0].InstanceId\u0026#34; \\ --output text Or list all instances:\naws ec2 describe-instances \\ --region ap-southeast-1 \\ --filters \u0026#34;Name=tag:Name,Values=*workshop-aws-dev*\u0026#34; \\ --query \u0026#34;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#34; \\ --output table Note: Save the instance ID for the next section.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.1-prepare/",
	"title": "Prepare Frontend Environment",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before building and deploying the frontend, ensure you have:\nNode.js and npm installed (Node.js 18+ recommended) AWS CLI configured with appropriate credentials Frontend bucket name from CloudFormation outputs CloudFront Distribution ID from CloudFormation outputs Get Required Information Get the frontend S3 bucket name: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Get the CloudFront Distribution ID: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text Get the API Gateway URL: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text Verify Frontend Environment Variables Check that the .env file in the FE directory contains the correct API URL:\n# Windows type FE\\.env # Linux/Mac cat FE/.env The file should contain:\nVITE_API_URL=https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service VITE_COGNITO_USER_POOL_ID=ap-southeast-1_4osSduRDx VITE_COGNITO_CLIENT_ID=51alb0b6n4h5unrojbshmqv12r VITE_COGNITO_REGION=ap-southeast-1 Note: Update VITE_API_URL with the actual API Gateway URL from step 3 above if different.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequisite/",
	"title": "Prerequisites &amp; Preparation",
	"tags": [],
	"description": "",
	"content": "System Requirements 1. AWS Account Active AWS account Administrator access or the following permissions: CloudFormation: Full access EC2: Full access VPC: Full access RDS: Full access S3: Full access CloudFront: Full access IAM: Create roles and policies CloudWatch: Full access 2. AWS CLI Install and configure AWS CLI:\nWindows:\n# Download and install from: https://aws.amazon.com/cli/ # Or use chocolatey: choco install awscli # Verify installation aws --version Linux/Mac:\n# Using pip pip install awscli # Or use package manager # Ubuntu/Debian sudo apt-get install awscli # MacOS brew install awscli # Verify installation aws --version Configure AWS CLI:\naws configure # AWS Access Key ID: \u0026lt;your-access-key\u0026gt; # AWS Secret Access Key: \u0026lt;your-secret-key\u0026gt; # Default region name: ap-southeast-1 # Default output format: json 3. EC2 Key Pair Create an EC2 Key Pair for SSH access:\nVia AWS Console:\nOpen EC2 Console Select region ap-southeast-1 (Singapore) Go to Network \u0026amp; Security → Key Pairs Click Create key pair Name: workshop-aws-key Key pair type: RSA Private key file format: .pem (Linux/Mac) or .ppk (Windows/PuTTY) Click Create key pair Save the .pem file securely Via AWS CLI:\n# Create key pair aws ec2 create-key-pair \\ --key-name workshop-aws-key \\ --query \u0026#39;KeyMaterial\u0026#39; \\ --output text \\ --region ap-southeast-1 \u0026gt; workshop-aws-key.pem # Set permissions (Linux/Mac only) chmod 400 workshop-aws-key.pem 4. Development Tools Java Development Kit (JDK) 17:\n# Windows (chocolatey) choco install openjdk17 # Linux (Ubuntu/Debian) sudo apt-get install openjdk-17-jdk # MacOS brew install openjdk@17 # Verify java -version Maven:\n# Windows choco install maven # Linux sudo apt-get install maven # MacOS brew install maven # Verify mvn -version Node.js and npm:\n# Windows choco install nodejs # Linux curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash - sudo apt-get install -y nodejs # MacOS brew install node # Verify node --version npm --version Prepare Project Files 1. Clone or Download Project # If you have Git repository git clone \u0026lt;your-repo-url\u0026gt; cd aws_project # Or download and extract ZIP file 2. Project Structure aws_project/\r├── aws/\r│ ├── infrastructure.yaml # Main CloudFormation template\r│ ├── cicd-pipeline.yaml # CI/CD pipeline (optional)\r│ ├── parameters.json # Stack parameters\r│ ├── deploy.bat # Deploy script (Windows)\r│ ├── deploy.sh # Deploy script (Linux/Mac)\r│ └── README.md # Detailed instructions\r├── BE/\r│ └── workshop_BE/\r│ ├── src/ # Backend source code\r│ ├── pom.xml # Maven configuration\r│ └── README.md\r└── FE/\r├── src/ # Frontend source code\r├── package.json # npm dependencies\r└── README.md 3. Configure Parameters Open aws/parameters.json and update values:\n[ { \u0026#34;ParameterKey\u0026#34;: \u0026#34;ProjectName\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;workshop-aws\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;Environment\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;dev\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;KeyPairName\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;workshop-aws-key\u0026#34; // ⚠️ Replace with your key pair name }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;DBPassword\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;YourStrongPassword123!\u0026#34; // ⚠️ Use a strong password }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;InstanceType\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;t3.micro\u0026#34; }, { \u0026#34;ParameterKey\u0026#34;: \u0026#34;DBInstanceClass\u0026#34;, \u0026#34;ParameterValue\u0026#34;: \u0026#34;db.t3.micro\u0026#34; } ] Important notes:\nKeyPairName: Must match your created key pair name DBPassword: Minimum 8 characters with uppercase, lowercase, numbers, and special characters Don\u0026rsquo;t commit this file with real passwords to Git 4. Update AMI ID The CloudFormation template uses a default AMI ID. Update it for your region:\nFind AMI ID:\n# Find Amazon Linux 2023 AMI for ap-southeast-1 aws ec2 describe-images \\ --owners amazon \\ --filters \u0026#34;Name=name,Values=al2023-ami-*-x86_64\u0026#34; \\ --query \u0026#39;Images | sort_by(@, \u0026amp;CreationDate) | [-1].[ImageId,Name,CreationDate]\u0026#39; \\ --region ap-southeast-1 \\ --output table Update in infrastructure.yaml:\nFind line ~530:\nLaunchTemplate: Properties: LaunchTemplateData: ImageId: ami-0c55b159cbfafe1f0 # ⚠️ Replace with your AMI ID Preparation Checklist Ensure you have completed all the following steps:\nAWS account configured AWS CLI installed and configured (aws configure) EC2 Key Pair created Java 17 installed Maven installed Node.js and npm installed Project files downloaded parameters.json file updated AMI ID updated in infrastructure.yaml Validate AWS Credentials # Check AWS credentials aws sts get-caller-identity # Expected output: # { # \u0026#34;UserId\u0026#34;: \u0026#34;AIDAXXXXXXXXXXXXXXXXX\u0026#34;, # \u0026#34;Account\u0026#34;: \u0026#34;123456789012\u0026#34;, # \u0026#34;Arn\u0026#34;: \u0026#34;arn:aws:iam::123456789012:user/your-username\u0026#34; # } Validate CloudFormation Template cd aws aws cloudformation validate-template \\ --template-body file://infrastructure.yaml \\ --region ap-southeast-1 If successful, you\u0026rsquo;ll see output with template parameters and outputs information.\nCost Estimation Before deployment, understand the costs:\nService Instance Type Cost/month (USD) EC2 t3.nano $3.50 RDS MySQL db.t3.micro $2.80 API Gateway - $0.50 S3 + CloudFront - $0.80 Route 53 - $0.50 Cognito - $0.10 CloudWatch - $0.30 CI/CD (CodePipeline) - $0.40 Total $8.90 For workshop (2-3 hours): ~$0.50-1.00\nNote:\nCosts apply to ap-southeast-1 region Use AWS Free Tier if account is eligible NAT Gateway (~$32/month) can be disabled to save costs Next Steps After completing all preparation steps, you\u0026rsquo;re ready to:\n➡️ Deploy Infrastructure with CloudFormation\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/2-proposal/",
	"title": "Project Proposal",
	"tags": [],
	"description": "",
	"content": "Blood Donation Support System (BDSS) 📄 Download Full Proposal Document (Word)\n1. Executive Summary Blood Donation Support System (BDSS) is a web platform that supports blood donation management and connects blood donors with healthcare facilities. The project is developed by a student team in Ho Chi Minh City to optimize the blood donation process, reduce the burden of finding donors, and improve healthcare communication efficiency.\nThe system is built on AWS Cloud architecture, utilizing Amazon EC2, Amazon RDS, API Gateway, Cognito, and CI/CD Pipeline (GitLab + CodePipeline) for automated deployment. BDSS supports four user groups (Guest, Member, Staff, Admin), providing features for lookup, blood donation registration, blood bank management, donation process tracking, and visual reporting.\n2. Problem Statement Current Problem: Healthcare facilities are currently managing blood donation processes manually or through disparate tools. Finding suitable blood donors by blood type or location is challenging, especially in emergency situations. Additionally, the data storage system is not synchronized, making it difficult to analyze, report, and optimize blood donation campaigns.\nProposed Solution: Develop a comprehensive blood donation support platform on AWS Cloud, with features for blood donation management, finding donors and recipients by blood type or geographic location, integrating user authentication via Amazon Cognito, and data management on Amazon RDS.\nThe frontend is deployed via Route 53 + CloudFront, backend through API Gateway – EC2, MySQL database on Amazon RDS, and automated CI/CD pipeline using GitLab – CodePipeline.\nBenefits and ROI: Reduce 60–70% of time searching for suitable blood donors. Increase accuracy of blood type and location information. Optimize operational costs with flexible cloud architecture, pay-as-you-go pricing. Improve response capability in emergency blood situations. 3. Solution Architecture Overall Architecture: The system is designed with a 3-tier architecture on AWS Cloud with the following main components:\n1. Frontend \u0026amp; Content Delivery Layer: Users: Access the system via web browsers or mobile devices. Route 53: DNS service managing domain names and routing traffic to CloudFront. CloudFront: CDN distributing static content with low latency, cached at edge locations. Amazon S3: Stores static assets (HTML, CSS, JS, images) for frontend application. 2. Application \u0026amp; Compute Layer: API Gateway: REST API endpoint, handling requests/responses between frontend and backend. VPC (Virtual Private Cloud): Isolated virtual network with configuration: Internet Gateway: Allows public subnet to connect to the Internet. Public Subnet: Contains EC2 instances processing business logic. Private Subnet: Contains RDS database, no direct Internet access. NAT Gateway: Allows private subnet to access Internet outbound only. Amazon EC2: Compute instances running backend API (Node.js/Express). Amazon RDS (MySQL): Relational database storing blood donor data, blood types, donation history. 3. CI/CD \u0026amp; DevOps Pipeline: GitLab: Source code repository and version control. AWS CodePipeline: Orchestrates automated CI/CD workflow. AWS CodeBuild: Builds and tests code before deployment. Automated Deployment: Automatically deploys to EC2 on code changes. 4. Monitoring, Security \u0026amp; Management Layer: Amazon Cognito: User authentication and authorization (Guest, Member, Staff, Admin roles). AWS IAM: Manages access permissions for users and services. AWS Secrets Manager: Securely stores database credentials and API keys. Amazon CloudWatch: Monitors metrics, logs, and creates alarms. AWS CloudTrail: Audit logs for all API calls and user activities. Amazon Athena: Queries and analyzes logs from S3. Amazon SNS: Sends notifications (email/SMS) for critical events (emergency blood needs, matching donors). System Workflow: User Access: Users → Route 53 → CloudFront → S3 (Frontend) API Requests: Frontend → API Gateway → EC2 (Backend) → RDS (Database) Data Flow: EC2 instances in public subnet connect to RDS in private subnet Outbound Traffic: Private subnet → NAT Gateway → Internet Gateway CI/CD Flow: GitLab → CodePipeline → CodeBuild → EC2 deployment Monitoring: CloudWatch collects metrics → SNS sends alerts → Athena analyzes logs 4. Technical Implementation Implementation Phases: 1. Analysis \u0026amp; Design (Month 1) Gather requirements, define use cases, design ERD and AWS architecture. 2. Infrastructure \u0026amp; Pipeline Setup (Month 2) Configure Route 53, CloudFront, EC2, RDS, and CI/CD on AWS. 3. Development \u0026amp; Testing (Month 3–4) Build main modules: blood donation registration, search, blood bank management. Integrate Cognito and SNS alert system. 4. Deployment \u0026amp; Operations (Month 5) Deploy production system and monitor with CloudWatch. Key Technical Requirements: Frontend: React/Next.js or Angular (deployed via S3/CloudFront). Backend: Node.js/Express on EC2, communicating via REST API Gateway. Database: Amazon RDS MySQL, optimized queries and periodic backups. CI/CD: GitLab → CodeBuild → CodePipeline → EC2. Auth: Cognito (4 roles: Guest, Member, Staff, Admin). Alert \u0026amp; Logs: SNS + CloudWatch + CloudTrail. 5. Roadmap \u0026amp; Milestones Timeline Phase Key Deliverables Month 1 Requirements Analysis \u0026amp; Design AWS architecture + use case diagrams Month 2 Infrastructure \u0026amp; Pipeline Setup EC2, RDS, API Gateway operational Month 3–4 Development \u0026amp; Testing Complete main modules Month 5 Production Deployment System operational and stable with Dashboard reports 6. Budget Estimation Service Estimated Cost/Month (USD) Notes EC2 (t3.nano) 3.50 Backend REST API Amazon RDS (MySQL) 2.80 20 GB storage API Gateway 0.50 5,000 requests CloudFront + S3 0.80 Website + CDN Route 53 0.50 Domain \u0026amp; DNS Cognito 0.10 \u0026lt;100 users CloudWatch + Logs 0.30 Monitoring and alerts CI/CD (CodePipeline, CodeBuild) 0.40 Automated deployment Total 8.9 USD/month ~106.8 USD/year Total costs can be adjusted based on AWS Free Tier or using spot instances.\n7. Risk Assessment Risk Impact Probability Mitigation Measures Internet Connection Loss Medium Medium Backup on EC2 instances DDoS Attack High Low AWS WAF + CloudFront User Data Errors High Low RDS backup + IAM access restrictions Budget Overrun Medium Low AWS budget alerts CI/CD Pipeline Disruption Low Medium Test pipeline before merge 8. Expected Outcomes Technical: Cloud-native system, automated CI/CD, multi-user support with high security. Application: Helps healthcare facilities manage blood donations efficiently, minimizing manual processes. Scalability: Can be scaled to multiple hospitals, integrate AI for blood type demand analysis or predict upcoming donation drives. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy Backend Layer: EC2 instances in private subnet with application runtime and Auto Scaling configuration. Set up Amazon RDS database in private subnet with proper configuration and parameter groups. Deploy backend application and establish connectivity between EC2 and RDS using Secrets Manager. Configure API Gateway REST API with integration to EC2 backend. Integrate Amazon Cognito User Pool with API Gateway for authentication and authorization. Configure Auto Scaling Group for EC2 instances with Launch Template for scalability. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - RDS Database Setup: + Create RDS subnet group spanning private subnet (10.0.2.0/24). + Launch RDS instance (MySQL/PostgreSQL) in private subnet with appropriate instance class. + Configure RDS parameter group with database-specific settings (character set, timezone, etc.). + Set up automated backups, encryption at rest, and Multi-AZ deployment (optional for cost optimization). + Configure RDS security group to allow connections only from EC2 Security Group. + Store initial database credentials in AWS Secrets Manager. 09/11/2025 09/11/2025 RDS documentation 3 - EC2 Backend Instance Setup: + Launch EC2 instance in private subnet (10.0.2.0/24) with appropriate instance type. + Install application runtime environment: Java/Python/Node.js based on application requirements. + Install and configure application dependencies and libraries. + Configure EC2 instance with IAM role (created in Week 9) for AWS service access. + Create base AMI from configured EC2 instance for Auto Scaling Group (to be used on Day 18). + Document EC2 configuration and application setup steps. 10/11/2025 10/11/2025 EC2 documentation 4 - Backend Application Deployment: + Deploy backend application code to EC2 instance (manual deployment for initial setup). + Configure application to connect to RDS database using credentials from Secrets Manager. + Test database connectivity from EC2 instance (verify connection string, credentials retrieval). + Configure application environment variables and configuration files. + Test basic application functionality and database operations (CRUD operations). + Document deployment process and application configuration. 11/11/2025 11/11/2025 Application deployment guide 5 - API Gateway REST API Configuration: + Create REST API in API Gateway with appropriate name and description. + Define API resources and methods (GET, POST, PUT, DELETE) based on application requirements. + Configure API Gateway integration with EC2 backend (HTTP/HTTPS integration or VPC Link for private resources). + Set up API Gateway VPC Link to connect to private subnet resources (EC2). + Enable CORS for frontend access (configure CORS headers: Access-Control-Allow-Origin, etc.). + Test API endpoints and verify integration with EC2 backend. 12/11/2025 12/11/2025 API Gateway documentation 6 - Amazon Cognito Integration: + Create Cognito User Pool for user authentication with appropriate name. + Configure user pool settings: password policies (minimum length, complexity), MFA (optional), email verification. + Create Cognito User Pool App Client for application integration. + Configure Cognito Authorizer in API Gateway for authenticated API access. + Test user registration flow: create test user in Cognito User Pool. + Test login flow: authenticate user and obtain JWT tokens. + Test authenticated API access: use JWT token to access protected API endpoints. 13/11/2025 13/11/2025 Cognito documentation 7 - Auto Scaling Group Configuration: + Create Launch Template based on base AMI created on Day 14. + Configure Launch Template with instance type, security groups, IAM role, and user data scripts. + Create Auto Scaling Group with Launch Template in private subnet. + Configure Auto Scaling policies: target tracking (CPU utilization, network in/out), step scaling, or scheduled scaling. + Set minimum, desired, and maximum capacity for Auto Scaling Group. + Test scale-out: trigger scaling by increasing load (or manually adjust desired capacity). + Test scale-in: reduce load and verify instances are terminated automatically. - Week 10 Summary: Backend and database layer complete, ready for CI/CD and monitoring setup in Week 11. 14/11/2025 14/11/2025 Auto Scaling documentation Week 10 Achievements: Successfully deployed Amazon RDS database:\nCreated RDS subnet group in private subnet for database isolation. Launched RDS instance (MySQL/PostgreSQL) with appropriate instance class and configuration. Configured RDS parameter group with database-specific settings. Set up automated backups, encryption at rest, and monitoring. Configured RDS security group to allow connections only from EC2 Security Group. Stored database credentials securely in AWS Secrets Manager. Set up EC2 backend infrastructure:\nLaunched EC2 instance in private subnet with appropriate instance type. Installed and configured application runtime environment (Java/Python/Node.js). Configured EC2 instance with IAM role for AWS service access. Created base AMI from configured EC2 instance for Auto Scaling Group. Documented EC2 configuration and application setup procedures. Deployed backend application:\nDeployed backend application code to EC2 instance. Configured application to connect to RDS using credentials from Secrets Manager. Tested database connectivity and verified connection functionality. Tested basic application functionality and database operations (CRUD). Documented deployment process and application configuration. Configured API Gateway REST API:\nCreated REST API with resources, methods, and integration points. Set up API Gateway VPC Link to connect to private subnet resources (EC2). Configured API Gateway integration with EC2 backend using HTTP/HTTPS. Enabled CORS for frontend access with proper headers. Tested API endpoints and verified integration with EC2 backend. Integrated Amazon Cognito for authentication:\nCreated Cognito User Pool with password policies, MFA, and email verification. Created Cognito User Pool App Client for application integration. Configured Cognito Authorizer in API Gateway for authenticated API access. Tested user registration, login, and authenticated API access flows. Established secure user authentication and authorization. Configured Auto Scaling Group for scalability:\nCreated Launch Template based on base AMI for consistent instance configuration. Created Auto Scaling Group with Launch Template in private subnet. Configured Auto Scaling policies (target tracking, step scaling) for automatic scaling. Set appropriate capacity limits (minimum, desired, maximum). Tested scale-out and scale-in functionality to verify automatic scaling. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Set up CI/CD Pipeline: Connect GitLab repository to AWS CodePipeline for automated deployments. Configure AWS CodeBuild for frontend and backend builds with automatic S3 upload and CloudFront invalidation. Implement SSH-less deployment for backend using AWS Systems Manager or CodeDeploy. Set up comprehensive monitoring with CloudWatch logs, metrics, and enhanced monitoring for EC2 and RDS. Configure AWS CloudTrail for audit logging and security compliance. Set up SNS Alerts with CloudWatch alarms for critical metrics (EC2 CPU, RDS connections, API 5xx errors). Perform end-to-end testing and create final project documentation with complete architecture diagram. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - GitLab to CodePipeline Integration: + Create GitLab repository for the project (if not already created). + Set up AWS CodePipeline with source stage connected to GitLab repository. + Configure webhook or polling for automatic pipeline triggers on code commits. + Create S3 bucket for pipeline artifacts storage. + Test pipeline trigger by making a test commit to GitLab repository. + Verify CodePipeline can successfully connect to GitLab and retrieve source code. 16/11/2025 16/11/2025 CodePipeline documentation 3 - CodeBuild for Frontend: + Create CodeBuild project for frontend build process. + Configure buildspec.yml file for frontend build steps (install dependencies, build assets, optimize). + Set up CodeBuild environment with appropriate Docker image (Node.js, npm, etc.). + Configure build output to upload built files to S3 bucket (FE Bucket). + Set up automatic CloudFront invalidation after S3 upload (invalidate cache for updated files). + Test frontend build process and verify files are uploaded to S3 and CloudFront cache is invalidated. 17/11/2025 17/11/2025 CodeBuild documentation 4 - CodeBuild for Backend \u0026amp; SSH-less Deployment: + Create CodeBuild project for backend build process. + Configure buildspec.yml for backend build steps (compile, test, package artifacts). + Set up CodeBuild environment for backend (Java/Python/Node.js based on application). + Configure artifact upload to S3 or CodeDeploy. + Implement SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy: - Option 1: Use SSM Run Command to deploy to EC2 instances without SSH. - Option 2: Use CodeDeploy to deploy application to Auto Scaling Group. + Test backend build and deployment process end-to-end. 18/11/2025 18/11/2025 CodeDeploy / SSM documentation 5 - CloudWatch Logs \u0026amp; Metrics Setup: + Create CloudWatch log groups for EC2 application logs. + Configure CloudWatch agent on EC2 instances to send logs and custom metrics. + Set up CloudWatch metrics for EC2: CPU utilization, memory, disk I/O, network. + Enable RDS Enhanced Monitoring for detailed database metrics. + Configure API Gateway access logs to CloudWatch Logs. + Create CloudWatch dashboards for monitoring application health and performance. + Configure log retention policies for cost optimization. 19/11/2025 19/11/2025 CloudWatch documentation 6 - CloudTrail \u0026amp; Audit Dashboard: + Enable AWS CloudTrail for API call logging across all AWS services. + Create CloudTrail trail with S3 bucket for log storage. + Configure CloudTrail log file validation and encryption. + Set up CloudWatch Logs integration for CloudTrail events (optional). + Create CloudWatch dashboard for audit and security monitoring. + Review CloudTrail logs to verify API call logging is working correctly. + Document CloudTrail configuration and log retention policies. 20/11/2025 20/11/2025 CloudTrail documentation 7 - SNS Alerts \u0026amp; CloudWatch Alarms: + Create SNS topic for alarm notifications. + Subscribe email/SMS endpoints to SNS topic. + Create CloudWatch alarm for EC2 CPU utilization (threshold: \u0026gt;80% for 5 minutes). + Create CloudWatch alarm for RDS database connections (threshold: \u0026gt;80% of max connections). + Create CloudWatch alarm for API Gateway 5xx errors (threshold: \u0026gt;10 errors in 5 minutes). + Configure alarm actions to send notifications via SNS. + Test alarms by triggering conditions and verify email/SMS notifications are received. 21/11/2025 21/11/2025 CloudWatch Alarms \u0026amp; SNS 8 - End-to-End Testing \u0026amp; Final Documentation: + Perform comprehensive end-to-end testing: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. + Test CI/CD pipeline with code changes: verify automated frontend and backend deployments. + Test monitoring and alerting: trigger alarms and verify SNS notifications are received. + Test security: verify IAM permissions, Cognito authentication, Secrets Manager access, WAF protection. + Test scalability: verify Auto Scaling Group responds to load changes. + Create final architecture diagram with all components, data flows, and resource relationships. + Write comprehensive project documentation: deployment procedures, troubleshooting guides, runbooks, and architecture overview. + Prepare Worklog summary for all 4 weeks (Week 8-11). - Week 11 Summary: Complete AWS web application architecture deployed with CI/CD, monitoring, security, and automation. Project ready for production use. 22/11/2025 22/11/2025 Project documentation Week 11 Achievements: Successfully set up CI/CD Pipeline:\nConnected GitLab repository to AWS CodePipeline for automated deployments. Configured automatic pipeline triggers on code commits (webhook or polling). Created S3 bucket for pipeline artifacts storage. Verified end-to-end pipeline connectivity and source code retrieval. Configured CodeBuild for Frontend:\nCreated CodeBuild project with buildspec.yml for frontend build automation. Configured build environment with appropriate Docker image and dependencies. Set up automatic S3 upload of built frontend files. Implemented automatic CloudFront cache invalidation after deployments. Verified frontend build and deployment process works correctly. Implemented CodeBuild for Backend with SSH-less Deployment:\nCreated CodeBuild project for backend build automation. Configured buildspec.yml for backend compilation, testing, and packaging. Implemented SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy. Eliminated need for SSH keys and improved security posture. Verified backend build and deployment process works end-to-end. Set up comprehensive CloudWatch Monitoring:\nCreated CloudWatch log groups for EC2 application logs. Configured CloudWatch agent on EC2 instances for logs and custom metrics. Set up CloudWatch metrics for EC2 (CPU, memory, disk, network). Enabled RDS Enhanced Monitoring for detailed database insights. Configured API Gateway access logs to CloudWatch Logs. Created CloudWatch dashboards for real-time monitoring. Configured log retention policies for cost optimization. Configured CloudTrail for Audit and Compliance:\nEnabled CloudTrail for comprehensive API call logging. Created CloudTrail trail with S3 bucket for secure log storage. Configured log file validation and encryption. Set up CloudWatch dashboard for audit monitoring. Established audit trail for security and compliance requirements. Implemented SNS Alerts and CloudWatch Alarms:\nCreated SNS topic for alarm notifications with email/SMS subscriptions. Created CloudWatch alarm for EC2 CPU utilization monitoring. Created CloudWatch alarm for RDS database connection monitoring. Created CloudWatch alarm for API Gateway 5xx error detection. Configured alarm actions to send notifications via SNS. Tested alarms and verified notification delivery. Performed comprehensive end-to-end testing:\nVerified complete application flow: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. Tested CI/CD pipeline with code changes and verified automated deployments. Tested monitoring and alerting: triggered alarms and verified SNS notifications. Tested security: verified IAM permissions, Cognito authentication, Secrets Manager, WAF protection. Tested scalability: verified Auto Scaling Group responds to load changes. Created final project documentation:\nCreated comprehensive architecture diagram with all components, data flows, and resource relationships. Documented deployment procedures, troubleshooting guides, and runbooks. Prepared architecture overview and system design documentation. Completed Worklog summary for all 4 weeks (Week 8-11). After Week 11, the complete AWS web application architecture is fully deployed, monitored, secured, and automated:\nEdge Layer: Route 53, CloudFront, AWS WAF, ACM Certificate, S3 (Frontend). Networking Layer: VPC, public/private subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs. Compute \u0026amp; Database Layer: EC2 (with Auto Scaling), RDS, API Gateway, Cognito. CI/CD Pipeline: GitLab, CodePipeline, CodeBuild (Frontend \u0026amp; Backend), SSH-less deployment. Monitoring \u0026amp; Security: CloudWatch (Logs, Metrics, Dashboards, Alarms), CloudTrail, SNS Alerts, IAM, Secrets Manager. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Finalize Worklog and deploy it Preparate for Presentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Worklog related + Reviewing to check for mistake + Pushing worklog to Github + Deploying it as a Github pages 11/24/2025 08/24/2025 3 - Prepare Presentation 11/25/2025 08/25/2025 Week 12 Achievements: Completed Worklog and Deployed it\nPresentation prepared\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.2-build/",
	"title": "Build Frontend Application",
	"tags": [],
	"description": "",
	"content": "Build React Application In this section, you will build the React frontend application using Vite.\nNavigate to the frontend directory: cd FE Install dependencies (if not already installed): npm install Verify the .env file exists and contains the correct API URL: # Windows type .env # Linux/Mac cat .env Build the application for production: npm run build Expected Result: The dist directory should be created with production-ready files:\nindex.html assets/ folder with JavaScript and CSS files Verify the build output: # Windows dir dist # Linux/Mac ls -lh dist/ Note: The build process will use the VITE_API_URL from the .env file and embed it into the JavaScript bundle.\nBuild Troubleshooting If you encounter build errors:\nError: VITE_API_URL is not defined: Ensure the .env file exists and contains VITE_API_URL Error: Module not found: Run npm install to install dependencies Error: Port already in use: Stop any running development server "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/5.3.2-deploy-ec2/",
	"title": "Deploy to EC2",
	"tags": [],
	"description": "",
	"content": "Connect to EC2 with Session Manager For this workshop, you will use AWS Session Manager to access EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances through an interactive browser-based shell without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance from your Auto Scaling Group (use the instance ID you saved from the previous section). Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nDeploy Backend Application Switch to the ec2-user: sudo su - ec2-user Navigate to the application directory: cd /opt/workshop Download the JAR file from S3: aws s3 cp s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/workshop-0.0.1-SNAPSHOT.jar . --region ap-southeast-1 Stop any existing application (if running): pkill -f workshop-0.0.1-SNAPSHOT.jar || true Get the RDS endpoint from CloudFormation: RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;RDSEndpoint\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;RDS Endpoint: $RDS_ENDPOINT\u0026#34; Create the application.properties file: cat \u0026gt; /opt/workshop/application.properties \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; spring.application.name=workshop-aws # AWS RDS Database Configuration spring.datasource.url=jdbc:mysql://${RDS_ENDPOINT}:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=phatsieuqua123 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 spring.datasource.hikari.connection-timeout=20000 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ # CORS Configuration cors.allowed.origins=https://d3gmmg22uirq0t.cloudfront.net,https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log # Actuator Configuration management.endpoints.web.exposure.include=health,info,metrics management.endpoint.health.show-details=when-authorized EOF Note: Replace ${RDS_ENDPOINT} with the actual RDS endpoint value.\nStart the application: nohup java -jar workshop-0.0.1-SNAPSHOT.jar \\ --spring.config.location=file:/opt/workshop/application.properties \\ \u0026gt;\u0026gt; /opt/workshop/application.log 2\u0026gt;\u0026amp;1 \u0026amp; Wait a few seconds and check if the application is running: sleep 10 ps aux | grep java tail -20 /opt/workshop/application.log Verify Backend Deployment Test the health endpoint: curl http://localhost:8080/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nTest through the API Gateway (get the URL from CloudFormation outputs): API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Section Summary Congratulations! You have successfully deployed the Spring Boot backend application to EC2. The application is now running in a private subnet, accessible through the Application Load Balancer and API Gateway. The VPC endpoints allow the EC2 instance to access S3 (for downloading the JAR) and Systems Manager (for Session Manager) without traversing the public internet.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-deploy-infrastructure/",
	"title": "Deploy Infrastructure with CloudFormation",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will deploy the complete AWS infrastructure using CloudFormation template. The template will create VPC, subnets, EC2 instances, RDS database, Load Balancer, S3 buckets, CloudFront distribution, and all necessary resources.\nValidate Template Before deployment, validate the template to ensure no syntax errors:\ncd aws aws cloudformation validate-template \\ --template-body file://infrastructure.yaml \\ --region ap-southeast-1 Expected result: Information about parameters, outputs, and template description.\nDeploy Stack Method 1: Using Deploy Script (Recommended) Windows:\ncd aws deploy.bat create Linux/Mac:\ncd aws chmod +x deploy.sh ./deploy.sh create The script will automatically:\nValidate template Create CloudFormation stack Monitor deployment progress Display outputs when complete Method 2: Using AWS CLI aws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://infrastructure.yaml \\ --parameters file://parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Monitor Progress Via AWS Console Open CloudFormation Console Select stack workshop-aws-dev Events tab: View resources being created Resources tab: View resource list Outputs tab: View outputs (after completion) Via AWS CLI # Check status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # View events aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --max-items 10 Deployment Time Stack creation takes approximately 15-20 minutes:\nVPC and Networking: 2-3 minutes NAT Gateway: 2-3 minutes RDS Database: 5-7 minutes EC2 Auto Scaling Group: 3-5 minutes Load Balancer: 2-3 minutes CloudFront Distribution: 5-10 minutes VPC Endpoints: 2-3 minutes View Outputs After stack creation succeeds (Status: CREATE_COMPLETE), get outputs:\naws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs\u0026#39; \\ --output table Important Outputs:\nOutput Key Description Example VPCId VPC ID vpc-0123456789abcdef0 FrontendBucketName S3 bucket for frontend workshop-aws-dev-frontend-123456789012-ap-southeast-1 CloudFrontDomainName CloudFront URL d1234567890abc.cloudfront.net ALBDNSName Load Balancer DNS workshop-aws-dev-alb-123456789.ap-southeast-1.elb.amazonaws.com RDSEndpoint Database endpoint workshop-aws-dev-db.xxxxx.ap-southeast-1.rds.amazonaws.com APIGatewayURL API Gateway URL https://xxxxx.execute-api.ap-southeast-1.amazonaws.com/dev CognitoUserPoolId Cognito User Pool ID ap-southeast-1_xxxxxxxxx Save these values - you\u0026rsquo;ll need them for next steps!\nVerify Created Resources 1. VPC and Networking # Get VPC ID VPC_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`VPCId`].OutputValue\u0026#39; \\ --output text) # View VPC details aws ec2 describe-vpcs --vpc-ids $VPC_ID --region ap-southeast-1 # View Subnets aws ec2 describe-subnets \\ --filters \u0026#34;Name=vpc-id,Values=$VPC_ID\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Subnets[*].[SubnetId,CidrBlock,AvailabilityZone,Tags[?Key==`Name`].Value|[0]]\u0026#39; \\ --output table 2. EC2 Instances # View EC2 instances in Auto Scaling Group aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress,PublicIpAddress]\u0026#39; \\ --output table 3. RDS Database # View RDS instance aws rds describe-db-instances \\ --db-instance-identifier workshop-aws-dev-db \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[0].[DBInstanceIdentifier,DBInstanceStatus,Endpoint.Address,Endpoint.Port]\u0026#39; \\ --output table Troubleshooting Stack Creation Failed If stack creation fails:\nView Events to find error: aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;StackEvents[?ResourceStatus==`CREATE_FAILED`].[LogicalResourceId,ResourceStatusReason]\u0026#39; \\ --output table Common errors: Error: \u0026ldquo;Key pair does not exist\u0026rdquo;\nCheck key pair name in parameters.json Ensure key pair exists in ap-southeast-1 region Error: \u0026ldquo;Invalid AMI ID\u0026rdquo;\nUpdate AMI ID in infrastructure.yaml Use AMI ID appropriate for your region Error: \u0026ldquo;Insufficient permissions\u0026rdquo;\nCheck IAM permissions of user Need CloudFormation, EC2, VPC, RDS, S3, IAM permissions Rollback and retry: # Delete failed stack aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 # Wait for stack deletion aws cloudformation wait stack-delete-complete \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 # Try creating again aws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://infrastructure.yaml \\ --parameters file://parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Confirm Successful Deployment Checklist to confirm infrastructure is ready:\nStack status is CREATE_COMPLETE VPC and subnets created EC2 instances running (State: running) RDS database status is available Load Balancer status is active S3 buckets created CloudFront distribution status is Deployed All outputs have values Next Steps After infrastructure is ready, you can:\n➡️ Configure and Deploy Backend Application\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI Myelo is a powerful example of how generative AI + cloud infrastructure + nonprofit mission can combine to make a real difference in healthcare support — especially for complex, global diseases like multiple myeloma. By giving patients — and their caregivers — instant access to well-organized, trustworthy, multilingual information, at any time and anywhere, IMF helps lower barriers in understanding, decision-making, and navigating treatment. As Myelo and the MKP evolve, we’re seeing a shift toward more personalized, accessible, and patient-centered care — not replacing doctors, but empowering patients with knowledge and support.\nTransforming SAP Technical Documentation with GenAI: Accelerating knowledge generation for SAP Notes with Amazon Bedrock TSAP is using generative AI powered by Amazon Bedrock to automatically summarize and restructure its massive technical documentation (SAP Notes and KBAs). This makes complex support information faster to find, easier to understand, and far more usable on mobile devices. It reduces troubleshooting time, boosts productivity, and moves SAP’s documentation from static reference material toward an intelligent, AI-driven knowledge system that can provide more proactive and personalized guidance in the future.\nMOSIP on AWS: Transforming digital identity for modern governments MOSIP on AWS provides governments with a secure, scalable, and cost-efficient platform to issue and manage digital identities at national scale, replacing traditional on-premises systems. Its modular, open-source design allows customization and integration while maintaining data security and compliance. By enabling inclusive, accessible identity systems — even in remote or low-resource areas — MOSIP helps expand access to essential services like healthcare, education, and social programs, making digital identity a foundation for more efficient, equitable public services.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/",
	"title": "Backend Deployment",
	"tags": [],
	"description": "",
	"content": "Deploy Backend Application In this section, you will build and deploy the Spring Boot backend application to EC2 instances. The backend will run in private subnets and be accessible through the Application Load Balancer and API Gateway.\nThe deployment process includes:\nBuilding the Spring Boot JAR file Uploading the JAR to S3 Deploying to EC2 instances via Session Manager Configuring application properties Starting the application service Content Build and Upload Backend Deploy to EC2 "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.3-deploy/",
	"title": "Deploy to S3 and CloudFront",
	"tags": [],
	"description": "",
	"content": "Upload Frontend to S3 Get the frontend bucket name (if you don\u0026rsquo;t have it): BUCKET_NAME=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Bucket: $BUCKET_NAME\u0026#34; Upload the built files to S3: # Windows aws s3 sync dist\\ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 # Linux/Mac aws s3 sync dist/ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 Expected Result: Files are uploaded to S3 with output like:\nupload: dist/index.html to s3://...\rupload: dist/assets/... Verify the upload: aws s3 ls s3://$BUCKET_NAME/ --recursive --region ap-southeast-1 Invalidate CloudFront Cache Get the CloudFront Distribution ID: DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Distribution ID: $DIST_ID\u0026#34; Create a CloudFront invalidation: aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 Expected Result: An invalidation ID is returned. The invalidation typically takes 1-2 minutes to complete.\nCheck invalidation status (optional): INVALIDATION_ID=$(aws cloudfront list-invalidations \\ --distribution-id $DIST_ID \\ --region ap-southeast-1 \\ --query \u0026#34;InvalidationList.Items[0].Id\u0026#34; \\ --output text) aws cloudfront get-invalidation \\ --distribution-id $DIST_ID \\ --id $INVALIDATION_ID \\ --region ap-southeast-1 Verify Frontend Deployment Get the CloudFront domain name: CLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDomainName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Frontend URL: https://$CLOUDFRONT_URL\u0026#34; Open the frontend URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Test the application: The frontend should load Try logging in or registering a new user Verify API calls are working (check browser console for errors) Section Summary Congratulations! You have successfully deployed the React frontend application to S3 and CloudFront. The frontend is now accessible globally via CloudFront CDN, providing low latency and high availability. The application communicates with the backend through API Gateway.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-deploy-backend/",
	"title": "Deploy Backend Application",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will build and deploy the Spring Boot backend application to EC2 instances. The backend provides RESTful API for DNA analysis, user authentication, and data management.\nStep 1: Configure Database Connection Get RDS endpoint from CloudFormation outputs:\nRDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;RDS Endpoint: $RDS_ENDPOINT\u0026#34; Update file BE/workshop_BE/src/main/resources/application.properties:\n# Database Configuration spring.datasource.url=jdbc:mysql://${RDS_ENDPOINT}:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=YourStrongPassword123! spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 spring.datasource.hikari.connection-timeout=20000 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.MySQL8Dialect # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ jwt.expiration=86400000 # CORS Configuration cors.allowed.origins=* # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log Step 2: Build Backend JAR cd BE/workshop_BE # Clean and build mvn clean package -DskipTests # Or use Maven Wrapper ./mvnw clean package -DskipTests # Verify JAR file ls -lh target/workshop-0.0.1-SNAPSHOT.jar Expected result: JAR file approximately 50-80MB in target/ directory\nStep 3: Upload JAR to S3 Create S3 bucket for backend artifacts (if not exists):\nACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-${ACCOUNT_ID}-ap-southeast-1\u0026#34; # Create bucket aws s3 mb s3://${BACKEND_BUCKET} --region ap-southeast-1 # Upload JAR aws s3 cp target/workshop-0.0.1-SNAPSHOT.jar \\ s3://${BACKEND_BUCKET}/jars/ \\ --region ap-southeast-1 # Verify upload aws s3 ls s3://${BACKEND_BUCKET}/jars/ Step 4: Deploy to EC2 Get EC2 Instance ID INSTANCE_ID=$(aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[0].Instances[0].InstanceId\u0026#39; \\ --output text) echo \u0026#34;Instance ID: $INSTANCE_ID\u0026#34; Connect to EC2 via Session Manager aws ssm start-session --target $INSTANCE_ID --region ap-southeast-1 On EC2 Instance, run the following commands: # Switch to ec2-user sudo su - ec2-user # Navigate to application directory cd /opt/workshop # Download JAR from S3 ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text) BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-${ACCOUNT_ID}-ap-southeast-1\u0026#34; aws s3 cp s3://${BACKEND_BUCKET}/jars/workshop-0.0.1-SNAPSHOT.jar . \\ --region ap-southeast-1 # Verify file ls -lh workshop-0.0.1-SNAPSHOT.jar Create Application Properties cat \u0026gt; application.properties \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; spring.application.name=workshop-aws # Database Configuration spring.datasource.url=jdbc:mysql://REPLACE_WITH_RDS_ENDPOINT:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=YourStrongPassword123! spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log EOF # Replace RDS endpoint RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) sed -i \u0026#34;s/REPLACE_WITH_RDS_ENDPOINT/${RDS_ENDPOINT}/g\u0026#34; application.properties Start Application # Stop old application (if any) sudo systemctl stop workshop.service 2\u0026gt;/dev/null || true pkill -f workshop-0.0.1-SNAPSHOT.jar 2\u0026gt;/dev/null || true # Start application nohup java -jar workshop-0.0.1-SNAPSHOT.jar \\ --spring.config.location=file:/opt/workshop/application.properties \\ \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp; # Save PID echo $! \u0026gt; application.pid # Wait for application to start sleep 10 # Check process ps aux | grep java Step 5: Verify Application Test Health Endpoint # On EC2 curl http://localhost:8080/dna_service/actuator/health # Expected result: # {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;} Test via Load Balancer # On local machine ALB_DNS=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ALBDNSName`].OutputValue\u0026#39; \\ --output text) curl http://${ALB_DNS}/dna_service/actuator/health Test via API Gateway API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) curl ${API_URL}/dna_service/actuator/health Step 6: Configure Systemd Service To automatically start application when EC2 restarts:\n# On EC2 sudo tee /etc/systemd/system/workshop.service \u0026gt; /dev/null \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; [Unit] Description=Workshop DNA Analysis Backend After=network.target [Service] Type=simple User=ec2-user Group=ec2-user WorkingDirectory=/opt/workshop ExecStart=/usr/bin/java -jar /opt/workshop/workshop-0.0.1-SNAPSHOT.jar --spring.config.location=file:/opt/workshop/application.properties Restart=always RestartSec=10 StandardOutput=append:/opt/workshop/application.log StandardError=append:/opt/workshop/application.log [Install] WantedBy=multi-user.target EOF # Reload systemd sudo systemctl daemon-reload # Enable service sudo systemctl enable workshop.service # Start service sudo systemctl start workshop.service # Check status sudo systemctl status workshop.service Step 7: View Logs # View application logs tail -f /opt/workshop/application.log # View systemd logs sudo journalctl -u workshop.service -f # View CloudWatch Logs (on local machine) aws logs tail /aws/workshop-aws/dev/application \\ --follow \\ --region ap-southeast-1 Troubleshooting Application Won\u0026rsquo;t Start Check logs:\ntail -100 /opt/workshop/application.log Common errors:\nDatabase connection failed\nCheck RDS endpoint in application.properties Verify Security Group allows EC2 to connect to RDS Verify database credentials Port 8080 already in use\n# Kill process using port 8080 sudo lsof -ti:8080 | xargs kill -9 Out of memory\n# Increase heap size java -Xmx512m -jar workshop-0.0.1-SNAPSHOT.jar Health Check Failed # Check application is running ps aux | grep java # Check port listening sudo netstat -tulpn | grep 8080 # Test locally curl -v http://localhost:8080/dna_service/actuator/health Load Balancer Health Check Failed # Check Target Group health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;target-group-arn\u0026gt; \\ --region ap-southeast-1 # Check Security Group # EC2 SG must allow traffic from ALB SG on port 8080 Confirm Successful Deployment Checklist:\nJAR file built successfully JAR uploaded to S3 Application running on EC2 Health endpoint returns {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;} Accessible via Load Balancer Accessible via API Gateway Systemd service enabled Logs being written to CloudWatch Next Steps After backend is ready:\n➡️ Deploy Frontend to S3 and CloudFront\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the complete AWS AI/ML ecosystem, from Amazon SageMaker for traditional ML to Amazon Bedrock for Generative AI. I gained deep understanding of prompt engineering techniques like Chain-of-Thought reasoning and Few-shot learning. I also learned about RAG (Retrieval-Augmented Generation) architecture and how it\u0026rsquo;s crucial for building accurate GenAI applications. The importance of guardrails for AI safety and content filtering in production applications was also emphasized. New Skills: I developed skills in using Amazon SageMaker Studio for ML model development and deployment. I learned how to implement RAG architecture for knowledge base integration. I gained practical knowledge of prompt engineering and how to build Bedrock Agents for multi-step workflows. I also learned about different foundation models (Claude, Llama, Titan) and when to use each one. Contribution to Team/Project: I shared comprehensive notes about SageMaker capabilities and Bedrock features with my team. I identified opportunities to implement RAG solutions for our domain-specific applications. I proposed pilot projects using Bedrock Agents for customer service automation. I also created guidelines for prompt engineering best practices and guardrail implementation for our GenAI projects. Event 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about DevOps culture and principles, including DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) for measuring DevOps maturity. I gained deep understanding of AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and different deployment strategies (Blue/Green, Canary, Rolling). I also learned about Infrastructure as Code with CloudFormation and CDK, and when to use each approach. New Skills: I developed skills in building complete CI/CD pipelines using AWS DevOps services. I learned how to implement Infrastructure as Code with both CloudFormation and CDK. I gained practical knowledge of container services (ECR, ECS, EKS, App Runner) and when to use each one. I also learned how to set up monitoring and observability using CloudWatch and X-Ray. Contribution to Team/Project: I shared comprehensive notes about AWS DevOps services and best practices with my team. I proposed implementing CI/CD pipelines using CodePipeline for automated deployments. I suggested adopting Infrastructure as Code for all our infrastructure using CloudFormation or CDK. I also created guidelines for containerization strategies and monitoring best practices. The knowledge gained helps our team implement modern DevOps practices and improve deployment frequency and reliability. Event 3 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS Well-Architected Framework Security Pillar and its five core pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. I gained deep understanding of core security principles including Least Privilege, Zero Trust, and Defense in Depth. I also learned about the Shared Responsibility Model and top security threats in cloud environments in Vietnam. The importance of building security into architecture from the start, not as an afterthought, was emphasized. New Skills: I developed skills in modern IAM architecture using IAM Identity Center, Service Control Policies, and permission boundaries. I learned how to implement comprehensive detection and monitoring using CloudTrail, GuardDuty, and Security Hub. I gained practical knowledge of network security with VPC segmentation, Security Groups, NACLs, WAF, and Shield. I also learned about encryption at rest and in transit, KMS key management, and secrets management with Secrets Manager and Parameter Store. Contribution to Team/Project: I shared comprehensive security best practices and the five pillars framework with my team. I proposed implementing modern IAM patterns with IAM Identity Center for SSO. I suggested setting up comprehensive monitoring using CloudTrail, GuardDuty, and Security Hub. I created incident response playbooks for common scenarios like compromised IAM keys, S3 public exposure, and EC2 malware detection. I also developed guidelines for encryption strategies and secrets management. The knowledge gained helps our team build secure cloud architectures following AWS Well-Architected best practices. Event 4 Event Name: Building Agentic AI: Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 5, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about Building Agentic AI and Context Optimization with Amazon Bedrock. I gained deep understanding of how to build autonomous AI agents with Amazon Bedrock through hands-on techniques. I learned about agentic orchestration patterns and advanced context optimization techniques. I also understood the importance of context optimization in reducing costs and improving performance. The workshop emphasized that agentic AI is the future of AI applications and context optimization is key to scaling effectively. New Skills: I developed skills in building Bedrock Agents from scratch with guidance from experts. I learned about context optimization techniques such as compression, summarization, and relevant information extraction. I gained practical knowledge of agentic orchestration patterns and how to coordinate multiple agents. I also learned about the CloudThinker platform and how it simplifies building agentic systems. The hands-on workshop gave me opportunities to practice with real AWS environments. Contribution to Team/Project: I shared comprehensive notes about Building Agentic AI and Context Optimization with my team. I proposed pilot projects using Bedrock Agents for automation tasks. I created guidelines for context optimization best practices to reduce costs and improve performance. I also documented CloudThinker platform capabilities and integration patterns. The knowledge gained helps our team explore agentic AI solutions and optimize costs in AI/ML projects. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/",
	"title": "Frontend Deployment",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will build and deploy the React frontend application to S3 and CloudFront. The frontend will be served via CloudFront CDN for global content delivery and low latency.\nThe deployment process includes:\nBuilding the React application with Vite Uploading static files to S3 Invalidating CloudFront cache Verifying the frontend is accessible Content Prepare Frontend Environment Build Frontend Application Deploy to S3 and CloudFront Verify Full Stack Deployment "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.4-verify/",
	"title": "Verify Full Stack Deployment",
	"tags": [],
	"description": "",
	"content": "Test Complete Application Flow In this section, you will verify that the entire application stack is working correctly, from frontend to backend to database.\nStep 1: Verify Backend Health Test the backend health endpoint through API Gateway:\nAPI_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nStep 2: Test Frontend Access Open the CloudFront URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Open browser Developer Tools (F12) and check the Console tab for any errors.\nVerify the API URL is correctly configured by checking network requests.\nStep 3: Test User Registration/Login Try to register a new user or log in with existing credentials.\nVerify that:\nAPI calls are successful (check Network tab in DevTools) JWT token is stored in localStorage User is redirected to the appropriate page after login Step 4: Verify Database Connection Connect to EC2 via Session Manager and check application logs:\n# Connect to EC2 aws ssm start-session --target \u0026lt;INSTANCE_ID\u0026gt; --region ap-southeast-1 # On EC2, check logs tail -50 /opt/workshop/application.log | grep -i \u0026#34;database\\|connection\\|error\u0026#34; Expected Result: No database connection errors in logs.\nStep 5: Monitor Application Check CloudWatch Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 metrics in CloudWatch console: CPU utilization Network in/out Application health Troubleshooting Common Issues Frontend can\u0026rsquo;t connect to API:\nVerify VITE_API_URL in .env matches API Gateway URL Check CORS configuration in backend Verify API Gateway integration with ALB Backend not responding:\nCheck EC2 instance is running Verify application is running: ps aux | grep java Check application logs: tail -100 /opt/workshop/application.log Database connection errors:\nVerify RDS security group allows traffic from EC2 security group Check RDS endpoint is correct in application.properties Verify database credentials Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.5-deploy-frontend/",
	"title": "Deploy Frontend",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will build the React frontend and deploy it to S3, then distribute it via CloudFront CDN to ensure high performance and low latency for global users.\nStep 1: Configure API Endpoint Get API Gateway URL from CloudFormation outputs:\nAPI_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;API URL: $API_URL\u0026#34; Update file FE/.env:\ncd FE cat \u0026gt; .env \u0026lt;\u0026lt;EOF VITE_API_URL=${API_URL}/dna_service VITE_APP_NAME=DNA Analysis Workshop VITE_APP_VERSION=1.0.0 EOF Or update config file directly FE/src/config/api.ts:\nexport const API_BASE_URL = process.env.VITE_API_URL || \u0026#39;http://localhost:8080/dna_service\u0026#39;; export const API_TIMEOUT = 30000; export const API_ENDPOINTS = { AUTH: { LOGIN: \u0026#39;/auth/login\u0026#39;, REGISTER: \u0026#39;/auth/register\u0026#39;, LOGOUT: \u0026#39;/auth/logout\u0026#39;, REFRESH: \u0026#39;/auth/refresh\u0026#39;, }, DNA: { ANALYZE: \u0026#39;/dna/analyze\u0026#39;, HISTORY: \u0026#39;/dna/history\u0026#39;, RESULT: \u0026#39;/dna/result\u0026#39;, }, USER: { PROFILE: \u0026#39;/user/profile\u0026#39;, UPDATE: \u0026#39;/user/update\u0026#39;, }, }; Step 2: Install Dependencies cd FE # Install npm packages npm install # Verify installation npm list --depth=0 Step 3: Build Frontend # Build production bundle npm run build # Check build output ls -lh dist/ # View file structure tree dist/ -L 2 Expected result:\ndist/\r├── index.html\r├── assets/\r│ ├── index-[hash].js\r│ ├── index-[hash].css\r│ └── [other assets]\r└── vite.svg Step 4: Upload to S3 Get S3 bucket name from outputs:\nFRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend Bucket: $FRONTEND_BUCKET\u0026#34; Upload files to S3:\n# Sync all files aws s3 sync dist/ s3://${FRONTEND_BUCKET}/ \\ --delete \\ --region ap-southeast-1 # Set cache control for static assets aws s3 cp dist/assets/ s3://${FRONTEND_BUCKET}/assets/ \\ --recursive \\ --cache-control \u0026#34;max-age=31536000\u0026#34; \\ --region ap-southeast-1 # Set no-cache for index.html aws s3 cp dist/index.html s3://${FRONTEND_BUCKET}/ \\ --cache-control \u0026#34;no-cache,no-store,must-revalidate\u0026#34; \\ --region ap-southeast-1 # Verify upload aws s3 ls s3://${FRONTEND_BUCKET}/ --recursive Step 5: Invalidate CloudFront Cache After upload, need to invalidate CloudFront cache so users receive the new version:\n# Get CloudFront Distribution ID DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Distribution ID: $DIST_ID\u0026#34; # Create invalidation aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 # Monitor invalidation status aws cloudfront get-invalidation \\ --distribution-id $DIST_ID \\ --id \u0026lt;invalidation-id\u0026gt; Note: Invalidation takes 5-10 minutes to complete.\nStep 6: Access Frontend Get CloudFront domain name:\nCLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend URL: https://${CLOUDFRONT_URL}\u0026#34; Open browser and access the URL above.\nStep 7: Verify Frontend Test Basic Functionality Home Page: Check page loads correctly Navigation: Test menus and routing API Connection: Open Developer Tools → Network tab Console Errors: Check for no JavaScript errors Test Authentication # Test login endpoint curl -X POST https://${CLOUDFRONT_URL}/api/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;username\u0026#34;:\u0026#34;test\u0026#34;,\u0026#34;password\u0026#34;:\u0026#34;test123\u0026#34;}\u0026#39; Test CORS Open Developer Tools → Console and run:\nfetch(\u0026#39;${API_URL}/dna_service/actuator/health\u0026#39;) .then(res =\u0026gt; res.json()) .then(data =\u0026gt; console.log(\u0026#39;API Response:\u0026#39;, data)) .catch(err =\u0026gt; console.error(\u0026#39;CORS Error:\u0026#39;, err)); Step 8: Configure Custom Domain (Optional) If you have a domain name:\n1. Create SSL Certificate in ACM # Certificate must be created in us-east-1 for CloudFront aws acm request-certificate \\ --domain-name yourdomain.com \\ --subject-alternative-names www.yourdomain.com \\ --validation-method DNS \\ --region us-east-1 2. Validate Certificate Add CNAME records to DNS as instructed by ACM.\n3. Update CloudFront Distribution aws cloudfront update-distribution \\ --id $DIST_ID \\ --distribution-config file://cloudfront-config.json 4. Update Route 53 # Create A record alias to CloudFront aws route53 change-resource-record-sets \\ --hosted-zone-id \u0026lt;zone-id\u0026gt; \\ --change-batch file://route53-changes.json Deployment Script Create script to automate deployment:\ncat \u0026gt; deploy-frontend.sh \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; #!/bin/bash set -e echo \u0026#34;Building frontend...\u0026#34; cd FE npm run build echo \u0026#34;Getting S3 bucket name...\u0026#34; FRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Uploading to S3...\u0026#34; aws s3 sync dist/ s3://${FRONTEND_BUCKET}/ --delete --region ap-southeast-1 echo \u0026#34;Invalidating CloudFront cache...\u0026#34; DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDistributionId`].OutputValue\u0026#39; \\ --output text) aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 echo \u0026#34;Deployment complete!\u0026#34; echo \u0026#34;Frontend URL: https://$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text)\u0026#34; EOF chmod +x deploy-frontend.sh Use script:\n./deploy-frontend.sh Troubleshooting Build Failed Error: \u0026ldquo;Module not found\u0026rdquo;\n# Delete node_modules and reinstall rm -rf node_modules package-lock.json npm install Error: \u0026ldquo;Out of memory\u0026rdquo;\n# Increase memory for Node.js export NODE_OPTIONS=\u0026#34;--max-old-space-size=4096\u0026#34; npm run build Upload Failed Error: \u0026ldquo;Access Denied\u0026rdquo;\nCheck AWS credentials Verify IAM permissions for S3 Error: \u0026ldquo;Bucket does not exist\u0026rdquo;\nCheck bucket name Verify CloudFormation stack created bucket CloudFront Issues Page won\u0026rsquo;t load:\nWait for CloudFront distribution to deploy (5-10 minutes) Check distribution status: Deployed Receiving old version:\nCreate new invalidation Clear browser cache (Ctrl+Shift+R) CORS errors:\nCheck API Gateway CORS configuration Verify backend CORS settings API Connection Failed # Test API from browser console fetch(\u0026#39;${API_URL}/dna_service/actuator/health\u0026#39;) .then(res =\u0026gt; res.text()) .then(data =\u0026gt; console.log(data)) If error:\nCheck API Gateway URL is correct Verify backend is running Check Security Groups Confirm Successful Deployment Checklist:\nFrontend built successfully Files uploaded to S3 CloudFront invalidation completed Website loads correctly via CloudFront URL No errors in browser console API calls working (check Network tab) Authentication flow working Routing between pages working Next Steps After frontend is ready:\n➡️ Testing and Validation\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Deploy DNA Analysis Application on AWS Overview In this workshop, you will learn how to deploy a production-ready full-stack DNA Analysis application on AWS using Infrastructure as Code (IaC) with CloudFormation. The application consists of a React frontend, Spring Boot backend, and MySQL database, all deployed with AWS best practices for security, scalability, and cost optimization.\nAWS Services Used:\nVPC \u0026amp; Networking: VPC, Subnets, Internet Gateway, NAT Gateway, VPC Endpoints Compute: EC2 Auto Scaling Group, Application Load Balancer Storage \u0026amp; CDN: S3 for frontend hosting, CloudFront for global content delivery Database: RDS MySQL for data persistence with automated backups Security: Security Groups, IAM Roles, AWS Cognito for user authentication Monitoring: CloudWatch Logs, Alarms, and SNS notifications API Management: API Gateway for secure backend API exposure What You Will Learn Infrastructure as Code: Deploy complete AWS infrastructure using CloudFormation templates VPC Design: Create a secure VPC with public and private subnets across multiple availability zones Cost Optimization: Use VPC Endpoints to reduce NAT Gateway costs (~$20-25/month savings) Auto Scaling: Configure EC2 Auto Scaling based on CPU metrics for high availability Database Management: Deploy and configure RDS MySQL with security best practices Frontend Deployment: Host static React website on S3 with CloudFront CDN Backend Deployment: Deploy Spring Boot application on EC2 with systemd service Security Best Practices: Implement security groups, IAM roles, and Cognito authentication Monitoring \u0026amp; Logging: Set up CloudWatch for application monitoring and alerting Architecture Diagram Internet\r│\r├─── CloudFront (CDN) ──\u0026gt; S3 (Frontend)\r│\r└─── API Gateway ──\u0026gt; ALB ──\u0026gt; EC2 (Backend) ──\u0026gt; RDS MySQL\r│\r└─── VPC Endpoints (S3, CloudWatch, SSM) Prerequisites AWS Account with appropriate permissions (Administrator or equivalent) AWS CLI installed and configured (aws configure) EC2 Key Pair created in your AWS region (ap-southeast-1) Basic understanding of AWS services and command line interface Familiarity with CloudFormation concepts Estimated Cost Running this workshop infrastructure will cost approximately $8.90/month (if running 24/7):\nService Instance Type Cost/month (USD) EC2 t3.nano $3.50 RDS MySQL db.t3.micro $2.80 API Gateway - $0.50 S3 + CloudFront - $0.80 Route 53 - $0.50 Cognito - $0.10 CloudWatch - $0.30 CI/CD (CodePipeline) - $0.40 Total $8.90 For workshop (2-3 hours): ~$0.50-1.00\n💡 Cost Saving Tips:\nDelete the stack immediately after workshop completion Use AWS Free Tier for eligible services Disable NAT Gateway when not in use (saves ~$32/month) Use VPC Endpoints instead of NAT Gateway for production Workshop Duration Total Time: 2-3 hours Infrastructure Deployment: 15-20 minutes Application Configuration: 30-45 minutes Testing \u0026amp; Validation: 15-30 minutes Cleanup: 5-10 minutes Content Workshop Overview Prerequisites \u0026amp; Preparation Deploy Infrastructure with CloudFormation Configure and Deploy Backend Application Deploy Frontend to S3 and CloudFront Testing and Validation Monitoring and Troubleshooting Clean Up Resources "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.5-testing-monitoring/",
	"title": "Testing and Monitoring",
	"tags": [],
	"description": "",
	"content": "Application Testing and Monitoring In this section, you will learn how to test the application and monitor its performance using AWS CloudWatch and other monitoring tools.\nTesting the Application Health Check Endpoints:\nBackend: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/actuator/health Frontend: https://d3gmmg22uirq0t.cloudfront.net API Testing:\nUse Swagger UI: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/swagger-ui.html Test endpoints using curl or Postman End-to-End Testing:\nRegister a new user Login and verify JWT token Create and manage resources Verify data persistence in database CloudWatch Monitoring View Application Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 Metrics:\nCPU Utilization Network In/Out Status Check Failed Monitor RDS:\nDatabase connections CPU utilization Storage space API Gateway Metrics:\nRequest count Latency Error rates Auto Scaling The Auto Scaling Group will automatically:\nScale up when CPU \u0026gt; 70% for 5 minutes Scale down when CPU \u0026lt; 30% for 5 minutes Maintain between 1-2 instances (configurable) Monitor scaling activities:\naws autoscaling describe-scaling-activities \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --region ap-southeast-1 Performance Optimization CloudFront Cache:\nStatic assets are cached at edge locations Invalidate cache when deploying updates Database Optimization:\nMonitor slow queries Optimize indexes Consider read replicas for high traffic Application Optimization:\nMonitor JVM heap usage Optimize database queries Use connection pooling effectively "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.6-testing/",
	"title": "Testing and Validation",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will perform test cases to verify the application works correctly end-to-end, from frontend through API Gateway, Load Balancer, to backend and database.\nStep 1: Verify Infrastructure Verify All Services Running # Check CloudFormation stack status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Expected: CREATE_COMPLETE # Check EC2 instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name,PrivateIpAddress]\u0026#39; \\ --output table # Check RDS status aws rds describe-db-instances \\ --db-instance-identifier workshop-aws-dev-db \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[0].[DBInstanceIdentifier,DBInstanceStatus]\u0026#39; \\ --output table # Expected: available # Check Load Balancer aws elbv2 describe-load-balancers \\ --names workshop-aws-dev-alb \\ --region ap-southeast-1 \\ --query \u0026#39;LoadBalancers[0].[LoadBalancerName,State.Code]\u0026#39; \\ --output table # Expected: active Step 2: Test Backend API Get API URLs ALB_DNS=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`ALBDNSName`].OutputValue\u0026#39; \\ --output text) API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`APIGatewayURL`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;ALB DNS: $ALB_DNS\u0026#34; echo \u0026#34;API Gateway URL: $API_URL\u0026#34; Test Health Endpoint # Test via ALB curl -v http://${ALB_DNS}/dna_service/actuator/health # Test via API Gateway curl -v ${API_URL}/dna_service/actuator/health # Expected response: # {\u0026#34;status\u0026#34;:\u0026#34;UP\u0026#34;} Test User Registration # Register new user curl -X POST ${API_URL}/dna_service/auth/register \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;email\u0026#34;: \u0026#34;test@example.com\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!@#\u0026#34;, \u0026#34;fullName\u0026#34;: \u0026#34;Test User\u0026#34; }\u0026#39; # Expected: 200 OK with user data Test User Login # Login curl -X POST ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;testuser\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;Test123!@#\u0026#34; }\u0026#39; # Expected: 200 OK with JWT token # Save token for next requests TOKEN=\u0026#34;\u0026lt;jwt-token-from-response\u0026gt;\u0026#34; Test Protected Endpoints # Get user profile curl -X GET ${API_URL}/dna_service/user/profile \\ -H \u0026#34;Authorization: Bearer ${TOKEN}\u0026#34; # Expected: 200 OK with user profile data Step 3: Test Frontend Access Frontend CLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`CloudFrontDomainName`].OutputValue\u0026#39; \\ --output text) echo \u0026#34;Frontend URL: https://${CLOUDFRONT_URL}\u0026#34; Open browser and access the URL above.\nManual Testing Checklist 1. Home Page\nPage loads successfully Logo and branding display correctly Navigation menu works No errors in Console 2. User Registration\nForm validation works Can register new user Success message displays Redirects to login page 3. User Login\nCan login with created credentials JWT token saved in localStorage Redirects to dashboard after login User menu displays username 4. DNA Analysis\nCan upload DNA sequence file File validation works Analysis progress displays Analysis results display correctly Can view history 5. User Profile\nDisplays user information Can update profile Avatar upload works (if available) Logout works correctly 6. Responsive Design\nMobile view works well Tablet view works well Desktop view works well Step 4: Test Database Connection Connect to RDS # Get RDS endpoint RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`RDSEndpoint`].OutputValue\u0026#39; \\ --output text) # Connect via MySQL client (from EC2 or local if public access) mysql -h ${RDS_ENDPOINT} -u admin -p workshop_aws Verify Tables Created -- Show all tables SHOW TABLES; -- Expected tables: -- users, dna_sequences, analysis_results, etc. -- Check user data SELECT id, username, email, created_at FROM users; -- Check DNA analysis data SELECT id, user_id, sequence_name, status, created_at FROM dna_sequences; -- Exit EXIT; Step 5: Load Testing (Optional) Install Apache Bench # Ubuntu/Debian sudo apt-get install apache2-utils # MacOS brew install httpd # Windows # Download from Apache website Run Load Test # Test health endpoint ab -n 1000 -c 10 http://${ALB_DNS}/dna_service/actuator/health # Test login endpoint ab -n 100 -c 5 -p login-data.json -T application/json \\ ${API_URL}/dna_service/auth/login Monitor During Load Test # Watch CloudWatch metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;5 minutes ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average \\ --region ap-southeast-1 Step 6: Security Testing Test HTTPS Enforcement # CloudFront should redirect HTTP to HTTPS curl -I http://${CLOUDFRONT_URL} # Expected: 301 redirect to https:// Test CORS # Test preflight request curl -X OPTIONS ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Origin: https://${CLOUDFRONT_URL}\u0026#34; \\ -H \u0026#34;Access-Control-Request-Method: POST\u0026#34; \\ -H \u0026#34;Access-Control-Request-Headers: Content-Type\u0026#34; \\ -v # Expected: CORS headers in response Test SQL Injection Protection # Try SQL injection in login curl -X POST ${API_URL}/dna_service/auth/login \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;username\u0026#34;: \u0026#34;admin\u0026#39;\\\u0026#39;\u0026#39; OR \u0026#39;\\\u0026#39;\u0026#39;1\u0026#39;\\\u0026#39;\u0026#39;=\u0026#39;\\\u0026#39;\u0026#39;1\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;anything\u0026#34; }\u0026#39; # Expected: 401 Unauthorized (not SQL error) Test XSS Protection In browser console:\n// Try XSS in input fields document.querySelector(\u0026#39;input[name=\u0026#34;username\u0026#34;]\u0026#39;).value = \u0026#39;\u0026lt;script\u0026gt;alert(\u0026#34;XSS\u0026#34;)\u0026lt;/script\u0026gt;\u0026#39;; Expected: Script not executed, escaped or sanitized.\nStep 7: Performance Testing Measure Page Load Time Open Chrome DevTools → Network tab:\nFirst Contentful Paint (FCP): \u0026lt; 1.5s Largest Contentful Paint (LCP): \u0026lt; 2.5s Time to Interactive (TTI): \u0026lt; 3.5s Total Page Size: \u0026lt; 2MB Test API Response Time # Measure API response time time curl -s ${API_URL}/dna_service/actuator/health \u0026gt; /dev/null # Expected: \u0026lt; 200ms Test CloudFront Caching # First request (MISS) curl -I https://${CLOUDFRONT_URL}/assets/index.js # Second request (HIT) curl -I https://${CLOUDFRONT_URL}/assets/index.js # Check X-Cache header: Hit from cloudfront Troubleshooting Common Issues API Returns 502 Bad Gateway # Check backend health curl http://${ALB_DNS}/dna_service/actuator/health # Check target group health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;arn\u0026gt; \\ --region ap-southeast-1 # Check EC2 logs aws ssm start-session --target \u0026lt;instance-id\u0026gt; tail -f /opt/workshop/application.log Frontend Shows CORS Error Verify API Gateway CORS configuration Check backend CORS settings in application.properties Ensure CloudFront origin is whitelisted Database Connection Timeout Check RDS Security Group allows EC2 traffic Verify RDS endpoint in application.properties Test connection from EC2: telnet ${RDS_ENDPOINT} 3306 Slow Page Load Check CloudFront cache hit ratio Optimize images and assets Enable gzip compression Review CloudWatch metrics Test Results Documentation Create test-results.md file:\n# Workshop Test Results ## Date: 2025-12-08 ## Tester: [Your Name] ### Infrastructure Tests - [x] CloudFormation stack: CREATE_COMPLETE - [x] EC2 instances: Running - [x] RDS database: Available - [x] Load Balancer: Active ### Backend API Tests - [x] Health endpoint: OK - [x] User registration: OK - [x] User login: OK - [x] Protected endpoints: OK ### Frontend Tests - [x] Page load: OK - [x] User registration: OK - [x] User login: OK - [x] DNA analysis: OK - [x] Responsive design: OK ### Performance Tests - [x] Page load time: 1.2s - [x] API response time: 150ms - [x] CloudFront cache: Working ### Security Tests - [x] HTTPS enforcement: OK - [x] CORS: OK - [x] SQL injection protection: OK - [x] XSS protection: OK ## Issues Found None ## Recommendations - Enable CloudFront compression - Add more CloudWatch alarms - Implement rate limiting Confirm Testing Complete Checklist:\nAll infrastructure services running Backend API endpoints working Frontend loads and works correctly Database connection working User authentication flow working DNA analysis features working Performance meets requirements Security tests pass Test results documented Next Steps After testing is complete:\n➡️ Monitoring and Troubleshooting\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from September 2025 to November 2025, I had the opportunity to learn, practice, and apply cloud computing knowledge to a real-world AWS environment.\nI participated in the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning journey and deployed a production-ready web application architecture on AWS. Through this project, I improved my skills in cloud architecture design, AWS services, Infrastructure as Code (CloudFormation), CI/CD pipelines, monitoring and observability, security best practices, and problem-solving in cloud environments.\nThe main project involved designing and implementing a complete AWS web application architecture including:\nEdge Layer: Route 53, CloudFront CDN, AWS WAF, ACM Certificate, S3 static hosting Networking: VPC, subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs Compute \u0026amp; Database: EC2 with Auto Scaling, RDS, API Gateway, Amazon Cognito CI/CD: GitLab, CodePipeline, CodeBuild with automated deployments Monitoring \u0026amp; Security: CloudWatch, CloudTrail, SNS alerts, IAM, Secrets Manager In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ☐ ✅ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ☐ ✅ 12 Overall General evaluation of the entire internship period ☐ ☐ ✅ Strengths Ability to learn: I find that I can learn and easily understand AWS services and concepts. When encountering something I am not sure of, I determine to search and find answer till I am satisfied discipline: I read and respectfully follow company rules and regulation. Progressive mindset: I am not be demotivated by any kind of criticism. instead, I will reflect what I do wrong, and strive to improve better and not to repeat the same mistake Professional Conduct: I respect everyone in the company, including my fellow interns and attend work seriously Needs Improvement Professional knowledge \u0026amp; skills: Due to the lack of time management, I wasn\u0026rsquo;t able to absorb knowledge fully and deeply. Time Management and Discipline: While I completed tasks, I greetly struggled with time management when dealing with balancing life and work. I need to improve my ability to manage,schedule and priortize what must be done first and urgently. Communication: I am fully aware of how passive I am when it comes to communication. I am still struggle to proactively with my teammates and people around me Cost Optimization Awareness: I focused more on functionality than cost optimization because it was so exciting to explore AWS services. I should consider cost implications Contribution to project/team: due to my Incompetence in communication. I found myself not contriubting much to the team and project. I am ashamed of myself Reflection This internship provided invaluable hands-on experience with AWS cloud services and best practices. It has strengthened my problem-solving skills and technical knowledge. Broaden my mind just how difficult work is, in which I believe it is but just a small fraction of what to come in future\nI am grateful for the opportunity to work in AWS intership program and look forward to applying skills I have learnt in future cloud architecture and DevOps roles.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have successfully completed the Workshop AWS Project deployment! In this workshop, you learned:\nHow to deploy a full-stack application on AWS using CloudFormation Architecture patterns for high availability and scalability Security best practices with private subnets and VPC endpoints Monitoring and auto-scaling configurations Frontend and backend deployment processes Clean Up Resources To avoid incurring charges, delete all resources created during this workshop.\nMethod 1: Delete CloudFormation Stack (Recommended) The easiest way to clean up is to delete the CloudFormation stack:\naws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Note: Some resources may need manual deletion if they have deletion protection or dependencies.\nMethod 2: Manual Cleanup (if needed) If the stack deletion fails or leaves some resources, manually delete:\nStop EC2 Instances:\naws autoscaling update-auto-scaling-group \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --min-size 0 \\ --desired-capacity 0 \\ --region ap-southeast-1 Delete S3 Buckets:\n# Empty frontend bucket aws s3 rm s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1 # Empty backend bucket aws s3 rm s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-backend-502310717700-ap-southeast-1 Disable and Delete CloudFront Distribution:\nFirst disable the distribution (wait for it to be disabled) Then delete it Delete RDS Database:\nTake a final snapshot if needed Delete the database instance (may take 10-15 minutes) Delete API Gateway:\nDelete the REST API Delete Load Balancer:\nDelete the Application Load Balancer Delete VPC Endpoints:\nDelete all VPC endpoints Delete CloudWatch Logs:\naws logs delete-log-group \\ --log-group-name /aws/workshop-aws/dev/application \\ --region ap-southeast-1 Verify Cleanup Check the following services to ensure all resources are deleted:\nEC2: No instances, security groups, or load balancers RDS: No database instances S3: No buckets CloudFront: No distributions API Gateway: No APIs VPC: VPC and related resources (if not managed by CloudFormation) CloudWatch: No log groups IAM: Review and delete custom roles/policies if created manually Cost Verification After cleanup, verify in AWS Cost Explorer that no charges are accruing for the deleted resources.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.7-monitoring/",
	"title": "Monitoring and Troubleshooting",
	"tags": [],
	"description": "",
	"content": "Overview In this step, you will learn how to monitor the application, view logs, and troubleshoot common issues using CloudWatch, CloudWatch Logs, and other AWS tools.\nCloudWatch Metrics EC2 Metrics # CPU Utilization aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # Memory Utilization (if CloudWatch Agent installed) aws cloudwatch get-metric-statistics \\ --namespace CWAgent \\ --metric-name mem_used_percent \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 RDS Metrics # Database Connections aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name DatabaseConnections \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # CPU Utilization aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name CPUUtilization \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 Application Load Balancer Metrics # Request Count aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name RequestCount \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Sum \\ --region ap-southeast-1 # Target Response Time aws cloudwatch get-metric-statistics \\ --namespace AWS/ApplicationELB \\ --metric-name TargetResponseTime \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 300 \\ --statistics Average \\ --region ap-southeast-1 CloudWatch Logs View Application Logs # List log streams aws logs describe-log-streams \\ --log-group-name /aws/workshop-aws/dev/application \\ --order-by LastEventTime \\ --descending \\ --max-items 5 \\ --region ap-southeast-1 # Tail logs (real-time) aws logs tail /aws/workshop-aws/dev/application \\ --follow \\ --region ap-southeast-1 # Filter logs by pattern aws logs filter-log-events \\ --log-group-name /aws/workshop-aws/dev/application \\ --filter-pattern \u0026#34;ERROR\u0026#34; \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s)000 \\ --region ap-southeast-1 # Search for specific errors aws logs filter-log-events \\ --log-group-name /aws/workshop-aws/dev/application \\ --filter-pattern \u0026#34;NullPointerException\u0026#34; \\ --start-time $(date -d \u0026#39;1 hour ago\u0026#39; +%s)000 \\ --region ap-southeast-1 View EC2 System Logs # Get instance ID INSTANCE_ID=$(aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ \u0026#34;Name=instance-state-name,Values=running\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[0].Instances[0].InstanceId\u0026#39; \\ --output text) # Get console output aws ec2 get-console-output \\ --instance-id $INSTANCE_ID \\ --region ap-southeast-1 \\ --output text CloudWatch Alarms View Existing Alarms # List all alarms aws cloudwatch describe-alarms \\ --alarm-name-prefix workshop-aws-dev \\ --region ap-southeast-1 # Get alarm history aws cloudwatch describe-alarm-history \\ --alarm-name workshop-aws-dev-cpu-high \\ --max-records 10 \\ --region ap-southeast-1 Create Custom Alarms # Alarm for High Error Rate aws cloudwatch put-metric-alarm \\ --alarm-name workshop-aws-dev-high-error-rate \\ --alarm-description \u0026#34;Alert when error rate exceeds 5%\u0026#34; \\ --metric-name 5XXError \\ --namespace AWS/ApplicationELB \\ --statistic Sum \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 10 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=LoadBalancer,Value=app/workshop-aws-dev-alb/xxxxx \\ --alarm-actions arn:aws:sns:ap-southeast-1:123456789012:workshop-aws-dev-alarms \\ --region ap-southeast-1 # Alarm for Database Connections aws cloudwatch put-metric-alarm \\ --alarm-name workshop-aws-dev-high-db-connections \\ --alarm-description \u0026#34;Alert when DB connections exceed 80\u0026#34; \\ --metric-name DatabaseConnections \\ --namespace AWS/RDS \\ --statistic Average \\ --period 300 \\ --evaluation-periods 2 \\ --threshold 80 \\ --comparison-operator GreaterThanThreshold \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --alarm-actions arn:aws:sns:ap-southeast-1:123456789012:workshop-aws-dev-alarms \\ --region ap-southeast-1 CloudWatch Dashboards Create Dashboard # Create dashboard JSON cat \u0026gt; dashboard.json \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; { \u0026#34;widgets\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/EC2\u0026#34;, \u0026#34;CPUUtilization\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;EC2 CPU Utilization\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/RDS\u0026#34;, \u0026#34;DatabaseConnections\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Average\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;RDS Connections\u0026#34; } }, { \u0026#34;type\u0026#34;: \u0026#34;metric\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;metrics\u0026#34;: [ [\u0026#34;AWS/ApplicationELB\u0026#34;, \u0026#34;RequestCount\u0026#34;, {\u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;}] ], \u0026#34;period\u0026#34;: 300, \u0026#34;stat\u0026#34;: \u0026#34;Sum\u0026#34;, \u0026#34;region\u0026#34;: \u0026#34;ap-southeast-1\u0026#34;, \u0026#34;title\u0026#34;: \u0026#34;ALB Request Count\u0026#34; } } ] } EOF # Create dashboard aws cloudwatch put-dashboard \\ --dashboard-name workshop-aws-dev-dashboard \\ --dashboard-body file://dashboard.json \\ --region ap-southeast-1 View Dashboard Open CloudWatch Console → Dashboards → workshop-aws-dev-dashboard\nTroubleshooting Common Issues Issue 1: High CPU Usage Symptoms:\nEC2 CPU \u0026gt; 80% Slow response times CloudWatch alarm triggered Diagnosis:\n# Check CPU metrics aws cloudwatch get-metric-statistics \\ --namespace AWS/EC2 \\ --metric-name CPUUtilization \\ --dimensions Name=AutoScalingGroupName,Value=workshop-aws-dev-asg \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # SSH to EC2 and check processes aws ssm start-session --target $INSTANCE_ID top -bn1 | head -20 Solutions:\nScale up: Increase instance type Scale out: Increase number of instances Optimize code: Profile and optimize application Add caching: Implement Redis/ElastiCache Issue 2: Database Connection Pool Exhausted Symptoms:\n\u0026ldquo;Too many connections\u0026rdquo; errors Slow database queries Application timeouts Diagnosis:\n# Check DB connections aws cloudwatch get-metric-statistics \\ --namespace AWS/RDS \\ --metric-name DatabaseConnections \\ --dimensions Name=DBInstanceIdentifier,Value=workshop-aws-dev-db \\ --start-time $(date -u -d \u0026#39;1 hour ago\u0026#39; +%Y-%m-%dT%H:%M:%S) \\ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \\ --period 60 \\ --statistics Average,Maximum \\ --region ap-southeast-1 # Connect to DB and check mysql -h $RDS_ENDPOINT -u admin -p SHOW PROCESSLIST; SHOW STATUS LIKE \u0026#39;Threads_connected\u0026#39;; Solutions:\nIncrease connection pool size in application.properties Fix connection leaks in code Scale up RDS instance Implement connection pooling best practices Issue 3: 502 Bad Gateway Symptoms:\nUsers receive 502 errors ALB cannot reach backend Health checks failing Diagnosis:\n# Check target health aws elbv2 describe-target-health \\ --target-group-arn \u0026lt;arn\u0026gt; \\ --region ap-southeast-1 # Check backend logs aws logs tail /aws/workshop-aws/dev/application --follow # Check Security Groups aws ec2 describe-security-groups \\ --group-ids \u0026lt;ec2-sg-id\u0026gt; \\ --region ap-southeast-1 Solutions:\nVerify backend is running: systemctl status workshop Check Security Group allows ALB traffic Verify health check path is correct Check application logs for errors Best Practices 1. Set Up Alerts CPU \u0026gt; 80% for 5 minutes Memory \u0026gt; 85% for 5 minutes Disk \u0026gt; 90% 5XX errors \u0026gt; 10 in 5 minutes Database connections \u0026gt; 80% of max 2. Regular Health Checks # Daily health check script #!/bin/bash echo \u0026#34;=== Daily Health Check ===\u0026#34; echo \u0026#34;Date: $(date)\u0026#34; # Check stack status echo \u0026#34;CloudFormation Stack:\u0026#34; aws cloudformation describe-stacks --stack-name workshop-aws-dev --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Check EC2 echo \u0026#34;EC2 Instances:\u0026#34; aws ec2 describe-instances --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#39; # Check RDS echo \u0026#34;RDS Database:\u0026#34; aws rds describe-db-instances --db-instance-identifier workshop-aws-dev-db --query \u0026#39;DBInstances[0].DBInstanceStatus\u0026#39; # Check API echo \u0026#34;API Health:\u0026#34; curl -s $API_URL/dna_service/actuator/health | jq . Monitoring Checklist CloudWatch metrics being collected CloudWatch Logs configured Alarms set up SNS notifications working Dashboard created Log retention configured Custom metrics published Health checks working Performance baseline established Next Steps After setting up monitoring:\n➡️ Clean Up Resources\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company offers flexible working hours when needed. In addition, having the opportunity to join event to experience new trending technology is a big plus.\nAdditional Questions What did you find most satisfying during your internship? +Mentor and FCJ team members are all wonderful people as I stated in section 1 and 2 What do you think the company should improve for future interns? Due to the limitation of my knowledge and Experience, I am not able to point out If recommending to a friend, would you suggest they intern here? Why or why not? I will only recommend them if they truly treat internship as a learning opportunity, A value chance. If they only treat internship program as a subject to pass to advance to next semester, I will not. However anyone who thirsts for knowledge and love IT, I will definitely recommend them. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? It would be nice if there is an area to refill water for hydration Would you like to continue this program in the future? Yes Any other comments (free sharing): I simply can\u0026rsquo;t think of any. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.8-cleanup/",
	"title": "Clean Up Resources",
	"tags": [],
	"description": "",
	"content": "Overview After completing the workshop, you need to delete all resources to avoid unexpected charges. CloudFormation will automatically delete most resources, but some require manual deletion.\nStep 1: Delete S3 Bucket Contents CloudFormation cannot delete S3 buckets containing objects. Delete contents first:\n# Get bucket names from outputs FRONTEND_BUCKET=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].Outputs[?OutputKey==`FrontendBucketName`].OutputValue\u0026#39; \\ --output text) # Delete all objects in frontend bucket aws s3 rm s3://$FRONTEND_BUCKET --recursive --region ap-southeast-1 # If backend bucket exists BACKEND_BUCKET=\u0026#34;workshop-aws-dev-backend-$(aws sts get-caller-identity --query Account --output text)-ap-southeast-1\u0026#34; aws s3 rm s3://$BACKEND_BUCKET --recursive --region ap-southeast-1 2\u0026gt;/dev/null || true Step 2: Delete CloudFormation Stack Method 1: Using Deploy Script Windows:\ncd aws deploy.bat delete Linux/Mac:\ncd aws ./deploy.sh delete Method 2: Using AWS CLI aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Step 3: Monitor Deletion Progress # Check status aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;Stacks[0].StackStatus\u0026#39; # Wait for stack deletion (may take 10-15 minutes) aws cloudformation wait stack-delete-complete \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Via AWS Console:\nOpen CloudFormation Console Select stack workshop-aws-dev Events tab: View resources being deleted Stack will disappear from list when deletion completes Step 4: Verify Resources Deleted Check VPC # Should not see workshop VPC aws ec2 describe-vpcs \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 Check EC2 Instances # Should not see workshop instances aws ec2 describe-instances \\ --filters \u0026#34;Name=tag:aws:cloudformation:stack-name,Values=workshop-aws-dev\u0026#34; \\ --region ap-southeast-1 \\ --query \u0026#39;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#39; Check RDS # Should not see RDS instance (may have snapshot) aws rds describe-db-instances \\ --region ap-southeast-1 \\ --query \u0026#39;DBInstances[?DBInstanceIdentifier==`workshop-aws-dev-db`]\u0026#39; Check S3 Buckets # Buckets should be deleted aws s3 ls | grep workshop-aws-dev Step 5: Delete RDS Snapshots (Optional) CloudFormation creates snapshot before deleting RDS. Delete if not needed:\n# List snapshots aws rds describe-db-snapshots \\ --region ap-southeast-1 \\ --query \u0026#39;DBSnapshots[?contains(DBSnapshotIdentifier,`workshop-aws-dev`)].DBSnapshotIdentifier\u0026#39; # Delete snapshot aws rds delete-db-snapshot \\ --db-snapshot-identifier \u0026lt;snapshot-id\u0026gt; \\ --region ap-southeast-1 Step 6: Delete CloudWatch Logs (Optional) Log groups are not automatically deleted:\n# List log groups aws logs describe-log-groups \\ --log-group-name-prefix \u0026#34;/aws/workshop-aws\u0026#34; \\ --region ap-southeast-1 # Delete log groups aws logs delete-log-group \\ --log-group-name \u0026#34;/aws/workshop-aws/dev/application\u0026#34; \\ --region ap-southeast-1 Step 7: Delete EC2 Key Pair (Optional) If key pair is no longer needed:\naws ec2 delete-key-pair \\ --key-name workshop-aws-key \\ --region ap-southeast-1 # Delete local .pem file rm workshop-aws-key.pem Troubleshooting Stack Deletion Failed If stack deletion fails:\nView error: aws cloudformation describe-stack-events \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#39;StackEvents[?ResourceStatus==`DELETE_FAILED`].[LogicalResourceId,ResourceStatusReason]\u0026#39; \\ --output table Common errors: Error: \u0026ldquo;S3 bucket is not empty\u0026rdquo;\nDelete all objects in bucket Retry stack deletion Error: \u0026ldquo;Network interface is in use\u0026rdquo;\nWait a few minutes for ENIs to be released Retry stack deletion Error: \u0026ldquo;Resource being used by another resource\u0026rdquo;\nIdentify resource dependencies Delete dependent resources first Force delete: # Retain problematic resources and delete stack aws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --retain-resources \u0026lt;ResourceLogicalId\u0026gt; # Then delete resources manually Cleanup Checklist Ensure all resources are deleted:\nCloudFormation stack deleted S3 buckets deleted EC2 instances terminated RDS database deleted Load Balancer deleted VPC and subnets deleted CloudFront distribution disabled and deleted NAT Gateway deleted Elastic IPs released RDS snapshots deleted (optional) CloudWatch log groups deleted (optional) EC2 Key Pair deleted (optional) Confirm No Ongoing Charges After 24-48 hours, check AWS Cost Explorer to ensure no charges from workshop.\nConclusion You have successfully completed the workshop and cleaned up all resources!\nWhat you learned: ✅ Deploy full-stack application on AWS ✅ Infrastructure as Code with CloudFormation ✅ AWS networking and security best practices ✅ Cost optimization strategies ✅ Monitoring and troubleshooting\nNext resources:\nAWS Well-Architected Framework AWS Solutions Library AWS Workshops Thank you for participating in this workshop! 🎉\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]