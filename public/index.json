[
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI by Sapna Kumar and Nickee DeLeon on 23 SEP 2025 in Amazon Bedrock, Artificial Intelligence, Customer Solutions, Generative AI, Healthcare, Nonprofit, Public Sector\nFounded in 1990, the International Myeloma Foundation (IMF) is the first and largest global foundation focused specifically on multiple myeloma. The IMF’s mission is to improve the quality of life of myeloma patients while working toward prevention and a cure by focusing on four key areas: research, education, support, and advocacy. In September 2024, the IMF launched Myelo—an artificial intelligence-powered virtual assistant built using Amazon Web Services (AWS) and Max.AI, developed by ZS Associates.\nThis blog post highlights Myelo’s capabilities to provide myeloma patients, caregivers, and healthcare professionals a trustworthy, round-the-clock source of information that is accessible from anywhere in the world, grounded in empathy, and can communicate in multiple languages.\nGenerative AI: Closing the gap by disseminating timely information For 35 years, the IMF has been supporting patients’ informational needs through its extensive library of over 100 multilingual publications, a robust website, and a highly trained InfoLine team that answers questions by phone and email.\nHowever, the IMF needed to keep up with the rapid influx of new information (about research, treatments, drug approvals, etc.), manage its ever-growing content, and deliver quick, personalized information to the myeloma community it serves.\nThe onset of generative AI technology effectively and efficiently filled this need, and Myelo has become the IMF’s dependable AI-powered chatbot—addressing questions and providing information specific to multiple myeloma in a swift, accurate, and empathetic manner.\nMyelo uses generative AI to search and synthesize information across the IMF’s deep knowledge base, including web pages, PDFs, videos, and podcasts. It delivers professional, accurate, and compassionate responses—from treatment options and clinical trials to symptom management and diagnosis—within seconds.\nMyelo is multilingual and available 24/7, 365 days a year—making it a lifeline for patients and caregivers who need to make complex decisions at any time. As myeloma patient and support group leader Oya Gilbert said: “Having that ability to go onto and talk to Myelo and know that you’re getting a trusted resource, that’s the key—a trusted resource from an organization that is trusted throughout the world, not just in the United States.”\nWhile Myelo can break down complex medical terms into more comprehensible information, it is important to note that it does not offer medical advice or diagnoses. Users are still encouraged to consult healthcare professionals for personalized care.\nAWS Bedrock: The strong foundation that sustains Myelo Myelo is built on Amazon Bedrock, a service that allows organizations to build and scale generative AI applications using foundational models (FMs)—large language models pre-trained on vast datasets. Bedrock eliminates the need for infrastructure management or model training from scratch, enabling the IMF to move quickly and securely.\nAt the heart of Myelo’s language processing capabilities is Anthropic’s Claude, a state-of-the-art large language model accessed through AWS Bedrock. Claude’s advanced reasoning abilities and focus on safety make it particularly well-suited for healthcare applications, providing nuanced understanding of medical terminology while maintaining appropriate boundaries around medical advice.\nWith Bedrock and Claude, the IMF can deploy tools for real-time summarization, question-answering, text classification, and information retrieval—all fine-tuned with the foundation’s own data.\nTMax.AI: Delivering smart, empathetic responses At the core of Myelo’s intelligence is Max.AI—an enterprise-grade AI platform created by ZS Associates and integrated with AWS services. Max.AI enables Myelo to process and organize large volumes of unstructured content—from transcripts and PDFs to web articles—so users get rich, accurate responses with citations.\nThe combination of Max.AI’s enterprise capabilities with Claude’s sophisticated language understanding creates a powerful synergy. Max.AI handles the complex data orchestration and content management, while Claude processes natural language queries and generates contextually appropriate responses that maintain the empathetic tone essential for cancer patient support.\nMax.AI also allows Myelo to maintain conversation history, delivering contextual, personalized responses over the course of a chat. The platform supports a seamless user experience across all devices, and users can even download their chat transcripts for reference.\nEquipped with built-in ethical safeguards and responsible AI protocols, Max.AI ensures that every response Myelo gives is both secure and trustworthy. Myelo: Breaking barriers and expanding the IMF’s global reach Since its launch, Myelo has become a staple of the IMF’s website, where it appears as a conspicuous pop-up. Users have asked it everything from “What are the infection prevention guidelines for myeloma patients?” to “What does it mean when my FISH chromosome analysis says, ‘Quantity Not Sufficient’?” In 2024 alone, Myelo fielded 16,883 inquiries. In the first half of 2025, it had already responded to more than 55,000.\nBy addressing many urgent and frequently asked questions, Myelo has enabled the IMF’s InfoLine team to handle more complex, one-on-one needs. Its multilingual capacity has significantly expanded the IMF’s global reach.\nOne US-based physician praised Myelo after testing it in Spanish: “There was not one question in the session that got an incorrect answer. AMAZING!”\nWhat’s next for Myelo? While Myelo has been successfully delivered, it is merely one milestone on IMF’s digital transformation journey. The chatbot continues to improve based on user feedback, including “thumbs down” ratings that trigger internal review and optimization.\nSoon, Myelo will be able to:\nPersonalize responses based on user profiles and prior chats Tailor information using geofencing (location-based customization) Take digital actions on behalf of users via AI agents Understand and respond to more nuanced or complex medical queries Offer a voice interface, adding speech recognition and synthesis capabilities Lay the foundation for the Myeloma Knowledge Platform Additionally, a version of Myelo to serve solely the healthcare professional community is being developed. Furthermore, Myelo is one part of a larger digital ecosystem the IMF is building: the Myeloma Knowledge Platform (MKP). Built on AWS Data Lake for Nonprofits, the MKP connects Myelo with other tools—such as a clinical trial finder—and unlocks insights by analyzing previously siloed data.\nBy identifying patterns across these systems, the MKP will offer personalized recommendations to patients, helping them navigate their unique myeloma journeys with greater clarity and confidence. Front Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate A more connected, personalized, and patient-centered future As the IMF continues to innovate at the intersection of healthcare and technology, Myelo stands as a compelling example of how generative AI can be used responsibly to empower patients, support caregivers, and ease the burden of a complex disease.\nBy delivering timely, trusted information with compassion, Myelo is more than just a chatbot: It’s a digital companion helping people make sense of life with myeloma. As part of the broader Myeloma Knowledge Platform, Myelo marks the beginning of a more connected, personalized, and patient-centered future in cancer care.\nThe IMF believes that by arming patients with knowledge about myeloma, they can make informed decisions about their care, gain access to myeloma specialists in their areas, find the best treatments available to them, and learn practical tools to live well with the disease.\nThis very much aligns with the IMF’s vision: “A world where every myeloma patient can live life to the fullest, unburdened by the disease.”\nAbout Author -Sapna Kumar\nFor six years, Sapna has served as the IMF\u0026rsquo;s manager of marketing and communications. She brings editorial expertise from the publishing industry, from roles at companies including McGraw-Hill, Pearson Education, and Encyclopedia Britannica. In 2021, she served as a digital learning manager with the American Academy of Physical Medicine \u0026amp; Rehabilitation, where she worked with medical professionals to develop continuing medical education courses. She holds a B.A. in writing from Purdue University and certificates in digital marketing, Google Analytics, and web-based communications. In the fall of 2022, she rejoined the IMF team to return to focusing on mission-driven work, always with the patients, caregivers, and the myeloma community at top of mind. Nickee DeLeon\nNickee is an editorial consultant. She is a versatile creative professional with a proven track record across multiple disciplines in communications and design. With extensive experience spanning publishing, editorial work, writing, graphic design, and art direction, Nickee brings a comprehensive understanding of how visual and written content work together to create compelling narratives. Original blog site: IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Transforming SAP Technical Documentation with GenAI: Accelerating knowledge generation for SAP Notes with Amazon Bedrock This blog was made by Zhang Wenju, Abhi Shivaditya, Beth Sharp, and Zhenyu Yin on 23 SEP 2025\nThis post was jointly authored by AWS and SAP\nWhat are SAP Notes? SAP Notes and Knowledge Base Articles (KBA) are fundamental components of SAP’s support ecosystem. These documents provide detailed instructions and solutions for known issues within SAP software products. While SAP Notes primarily focus on coding corrections and technical solutions, KBAs complement them by describing issues in business language, often including screenshots and visual aids. Together, they form a comprehensive knowledge base that helps users and consultants effectively troubleshoot and resolve SAP-related challenges.\nChallenges faced by customers in navigating complex SAP systems and documentation As SAP professionals increasingly adopt mobile-first workflows for on-the-go troubleshooting, accessing critical documentation like SAP Notes poses a growing challenge. While users now expect to search and retrieve SAP Notes instantly via smartphones or tablets, the content remains optimized for desktop browsers—forcing mobile viewers to endure excessive zooming, scrolling, and text-heavy navigation. Key implementation steps or urgent configuration fixes are buried in dense technical jargon and multi-page layouts unsuited to compact screens. This disconnect delays time-sensitive resolutions, as support teams waste minutes deciphering desktop-formatted content on mobile devices, increasing the risk of oversight and cognitive fatigue during critical tasks.\nWhat is the summarization feature that the SAP for Me team introduced? To address these challenges, the SAP for Me team partnered with AWS. Using Amazon Bedrock, they developed a new version of their mobile app featuring AI-generated summaries of SAP Notes and KBAs. This innovation allows users to quickly navigate and understand technical documentation through concise, automatically generated summaries.\nWhy is this summarization feature useful for users of the SAP for Me Mobile app? With the summarization feature, users can quickly assess note relevance for specific business scenarios and access prerequisites and implementation requirements instantly. The feature transforms lengthy technical documentation into mobile-friendly content, highlighting key implementation steps without extensive scrolling thereby significantly reducing time spent during urgent troubleshooting scenarios. End users, for example Basis and functional support teams and consultants, benefit from improved efficiency while on the go, while expert knowledge becomes more accessible in mobile-first contexts, enabling faster decision-making and providing the ability to quickly determine note relevance for investigation. |\nLaying the foundation: from vision to first deployment Think big and start small After the Amazon Bedrock integration into SAP’s Generative AI Hub, featuring Claude 3.5 Sonnet and Amazon Titan, was announced at SAP Sapphire in 2024, AWS initiated Generative AI workshops with SAP China Labs. The AWS and SAP China labs fostered a collaborative environment through regular workshops, identifying SAP notes and KBAs, comprising millions of technical documents, as prime candidates for Generative AI enhancement. The teams established weekly meetings and on-demand support to implement SAP Generative AI Hub and invoking Claude models via Amazon Bedrock APIs. Despite initial technical challenges, AWS technical assistance helped SAP meet their pre-Christmas 2024 deployment deadline.\nWhat were the initial blockers the team faced? The team faced time pressure when attempting to summarize 6 million SAP Notes within tight timeframes. Ensuring technical accuracy of AI-generated summaries proved crucial, as did developing efficient systems for managing summarization updates for both new and existing notes.\nArchitecture Deep Dive: Knowledge bases as a backbone This project uses retrieval-augmented generation (RAG) to provide customers with precise answers to their queries directly from SAP Notes and KBAs, eliminating the need to manually search through multiple documents. Customers benefit from immediate access to accurate and relevant information, significantly accelerating issue resolution.\nThe RAG system begins with data ingestion and processing pipeline tailored to SAP Notes and KBAs. In this phase, unstructured and semi-structured documents like SAP notes in html format were systematically collected, cleansed, and transformed into a retrievable format. Key steps include extracting text, metadata (e.g., note numbers, applicability, release versions), and resolving cross-references between documents. Preprocessing techniques, such as semantic chunking and entity recognition, are applied to segment content into contextually meaningful units, while preserving technical nuances. These chunks were then encoded into high-dimensional vector embeddings using Amazon Titan Embedding model. The processed data was indexed in HANA vector database, optimized for fast similarity searches, ensuring the system can efficiently map user queries to the most relevant SAP content during retrieval.\nDuring the query phase, the RAG service combines retrieval and generation to deliver precise answers. When a user submits a query, the system first leverages the HANA vector database to retrieve the top-k SAP Notes or KBA snippets semantically aligned with the query intent. A reranking step prioritizes results based on relevance scores, publication dates, or applicability criteria to ensure up-to-date and actionable insights. The retrieved context is then fed into Bedrock Claude 3.5 Sonnet model via SAP Generative AI Hub into Amazon Bedrock, which synthesizes a concise, natural language response. Crucially, the response is grounded strictly in the retrieved sources, with built-in citations to original notes/KBAs for transparency. This hybrid approach balances speed and accuracy, allowing customers to resolve issues in real time while minimizing hallucinations, thereby reducing reliance on traditional support channels.\nCustomer value \u0026amp; benefit SAP’s initiatives to enhance the customer experience through the “Summary for SAP Notes/KBA” and “RAG for SAP Notes/KBA” projects provide significant business value. The two projects streamline access to critical information, enabling customers to efficiently resolve issues and maximize their use of SAP software products. By improving clarity and accessibility, customers can quickly identify relevant solutions, thus reducing downtime and optimizing productivity. This enhanced access to precise information not only improves operational efficiency but also strengthens the overall customer support experience.\nNext step: online recommendations with context The SAP for Me team is moving from basic summarization to using agentic RAG and knowledge graphs. This will help provide smarter, context-aware technical guidance and make it easier to visualize system dependencies and knowledge map. The team is also working on adding predictive support and personalized recommendations to improve the user experience even more. These steps will build a more powerful and intuitive support system for SAP users.\nConclusion SAP for Me’s Generative AI journey demonstrates the transformative impact of thoughtfully applying AI to enterprise software challenges. Leveraging the integration of Amazon Bedrock’s Claude 3.5 Sonnet with SAP’s Generative AI Hub, the team tackled the fundamental problem of knowledge accessibility by implementing summarization capabilities for SAP’s vast repository of technical notes. This initial feature addressed the critical need for efficient mobile consumption of complex documentation.\nDespite early implementation challenges, including Generative AI upskilling and ensuring technical accuracy, the collaborative effort between SAP and AWS teams successfully delivered the summarization feature before the targeted 2024 year-end holiday deadline. This achievement represents more than just a technical milestone; it marks a fundamental shift in how SAP customers can access and utilize critical technical knowledge on mobile devices.\nThe roadmap ahead, progressing through agentic RAG, knowledge graph, and proactive recommendations, shows a strategic vision for evolving from static documentation to dynamic, interconnected, and anticipatory knowledge systems. This evolution promises to transform SAP’s customer support model from reactive problem-solving to proactive guidance tailored to each customer’s unique environment.\nWe encourage you to explore these new capabilities of SAP for Me mobile application and see firsthand how Generative AI can transform your technical support and knowledge management. Reach out to us to learn how you can leverage Amazon Bedrock and unlock greater efficiency and insight for your organization.\nOriginal blog site: Transforming SAP Technical Documentation with GenAI\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "MOSIP on AWS: Transforming digital identity for modern governments by Andrew Johnston and Mohamed Heiba on 23 SEP 2025 in AWS Outposts, Government, Public Sector, Security, Identity, \u0026amp; Compliance\nAccording to the World Bank’s Identification for Development (ID4D) initiative, approximately 850 million people globally don’t have official identification. This prevents citizens from access to essential services including healthcare, education, and social benefits. To address these challenges, Atos and Amazon Web Services (AWS) have collaborated on an innovative cloud-based digital identity system using the Modular Open-Source Identity Platform (MOSIP), making these systems more accessible, secure, and scalable than ever before.\nThe need for digital identity systems The COVID-19 pandemic has fundamentally transformed how we approach identification systems, accelerating the need for contactless solutions across various sectors. According to industry data, the integration of biometric technology plays a vital role in this digital transformation. The global biometric technology market is projected to grow from USD $34.27 billion in 2022 to USD $150.58 billion by 2030. Government agencies and citizens recognize the importance of preventing fraud and enhancing data security while ensuring reliable identification methods that promote trust and accessibility in our increasingly digital world.\nThe MOSIP solution on AWS MOSIP—established in 2018 at the International Institute of Information Technology Bangalore—is a not-for-profit initiative helping governments implement digital ID systems. Governments can rely on the platform’s robust, scalable architecture to build effective civil registries and service delivery systems, forming the foundation for digital economies. Successful implementations in multiple governments across Africa demonstrate MOSIP’s ability to facilitate essential services through secure and accessible digital identity systems.\nChallenges with on-premises digital identity systems As various levels of governments throughout the world strive to implement reliable and secure identification systems in service of their mission to provide constituent services, they’re faced with several challenges from maintaining on-premises digital identity systems. There are operational challenges, implementation and social challenges, and technical challenges they must address before finding success.\nHigh infrastructure costs create significant barriers to entry, and limited scalability struggles to handle growing population needs. These systems often face security vulnerabilities, complex maintenance procedures, and integration difficulties with existing government systems.\nOrganizations must maintain specialized IT staff, manage complex backup procedures, and handle high maintenance costs. Manual processes can introduce errors in identity verification, while ensuring 24/7 system availability remains challenging.\nNew system deployments face resistance from traditional system users and require extensive training. Privacy concerns, regulatory compliance, and digital literacy barriers affect system adoption. Additionally, ensuring inclusion for remote and marginalized populations presents ongoing challenges.\nBenefits of a cloud-based digital identity system A cloud-based digital identity system offers benefits to governments and their agencies on multiple fronts. They’ll discover operational and financial benefits, improvements to security and accessibility, and integration and business continuity advantages.\nCloud deployment of MOSIP on AWS transforms service delivery through auto scaling capabilities and high availability. The pay-as-you-go model eliminates large upfront investments, and automated scaling handles population growth efficiently. AWS managed services reduce infrastructure costs and eliminate routine maintenance tasks.\nThe platform enables government services across remote regions through standard internet connectivity, with built-in disaster recovery. AWS provides comprehensive security features, including encryption and identity management, to help meet regulatory requirements and minimize risk management costs.\nStrong integration capabilities allow seamless connection with existing government systems and future services. Automated backup enables business continuity without additional investment. This cloud-based approach accelerates implementation timeframes, reduces operational complexity, and provides enterprise-grade security and performance for modern digital identity systems.\nHybrid deployment solutions To address government requirements, four deployment models are available:\nHybrid deployment – Separates production and nonproduction environments Split security model – Maintains key management and backups on premises Sensitive data protection – Keeps citizen data on premises and uses cloud for other services AWS Outposts – Provides full AWS capabilities within government data centers Cost optimization The implementation of MOSIP on AWS offers significant cost advantages compared to traditional on-premises deployments. Based on our analysis using AWS Pricing Calculator, we’ve identified three deployment tiers plus a foundational layer that accommodates different scale requirements.\nThe foundational infrastructure layer costs approximately $4,000 per month and provides essential shared services, including networking, security, and DevOps tools. This includes Amazon Virtual Private Cloud (Amazon VPC) configurations, AWS Transit Gateway gateways, VPN connectivity, AWS Network Firewall firewall, and development tools such as AWS CodeBuild and Amazon Elastic Container Registry (Amazon ECR), forming the backbone of any MOSIP deployment, regardless of scale.\nThe following table compares three different scalable deployment tiers. All costs are approximate and can vary based on specific requirements, Region, and actual usage patterns.\nSmall scale Medium scale Large scale Estimate cost per month USD $4,168 per month USD $9,896.10 per month USD $14,395.85 per month Enrollments per day Suitable for hundreds of enrollments or authentications per day Handles tens of thousands of enrollments daily Processes hundreds of thousands of enrollments daily Estimate cost per enrollment USD $1.39 per enrollment USD $0.033 per enrollment USD $0.0048 per enrollment Availability -Single Availability Zone deployment High availability across two zones Enterprise-grade high availability Use case Ideal for pilot programs or smaller implementations Production-grade infrastructure Full production capability Cost comparison with on-premises -Traditional on-premises MOSIP implementations typically require the following budget considerations:\nInitial infrastructure investment: USD $2–3 million (estimate) Data center setup and maintenance: USD $500,000–750,000 annually (estimate) Specialized IT staff: USD $250,000–400,000 annually (estimate) Hardware refresh cycles every 3–5 years: USD $1–1.5 million (estimate) The cloud-based approach eliminates approximately 60–70 percent of upfront costs and reduces ongoing operational expenses by 40–50 percent. Implementation time is reduced from 12–18 months to 3–6 months, accelerating time to value. The pay-as-you-grow model enables organizations to only pay for resources they use, with the ability to scale up or down based on demand. Government impact The MOSIP on AWS solution enables:\nImproved public service delivery Enhanced citizen inclusion Stronger data security Better resource utilization Scalable population coverage This cloud-based approach transforms digital identity from a resource-intensive requirement into an efficient public service enabler, which means governments can focus on citizen services rather than infrastructure management.\nFor governments seeking to modernize their identification systems, MOSIP on AWS provides a compelling solution that combines cost-effectiveness, scalability, and security. The platform’s successful implementations in multiple countries demonstrate its reliability and effectiveness in serving diverse population needs, making it an ideal choice for governments committed to digital transformation and improved citizen services.\nConclusion In part two of this blog post series, we will dive deep into the technical architecture of MOSIP on AWS, exploring the intricate details of its deployment models, security frameworks, and integration patterns. We’ll examine the specific AWS services that power this solution, detailed infrastructure configurations, and best practices for implementation. Stay tuned for an in-depth technical exploration of how governments can use this innovative platform to build robust digital identity systems.\nReference: Future of Government Awards 2023 celebrate use of technology to transform people’s lives Original blog site: MOSIP on AWS: Transforming digital identity for modern governments\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event 1 “AWS Cloud Mastery Series #1 – AI/ML/GenAI on AWS” Summary Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Provide comprehensive overview of AWS AI/ML services and capabilities Introduce Amazon SageMaker as an end-to-end ML platform Explore Generative AI with Amazon Bedrock Demonstrate practical applications through live demos Share best practices for AI/ML implementation in Vietnam Event Details Date: Saturday, November 15, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office Duration: 3.5 hours (excluding lunch break) Agenda 8:30 – 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 – 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker – End-to-end ML platform Data preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan – comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Highlights Amazon SageMaker Platform Comprehensive ML Platform: Complete solution for building, training, and deploying machine learning models Data Preparation: Tools for data labeling, feature engineering, and data validation Model Training: Support for various ML frameworks and algorithms with distributed training capabilities Model Deployment: Flexible deployment options including real-time inference, batch processing, and serverless inference MLOps Integration: Built-in capabilities for model monitoring, versioning, and automated workflows Generative AI with Amazon Bedrock Foundation Model Selection: Understanding differences between Claude, Llama, and Titan models Claude: Strong reasoning and conversation capabilities Llama: Open-source models with good performance Titan: AWS-developed models optimized for specific use cases Prompt Engineering Techniques: Chain-of-Thought reasoning for complex problem solving Few-shot learning with examples Context management and prompt optimization RAG Architecture: Combining retrieval with generation for accurate, context-aware responses Knowledge base integration Vector embeddings and similarity search Document chunking strategies Bedrock Agents: Autonomous agents that can perform multi-step tasks Tool integrations and API calling Workflow orchestration Decision-making capabilities Guardrails for AI Safety: Content filtering and safety controls Harmful content detection Custom policy configurations Compliance and governance AI/ML Landscape in Vietnam Current adoption trends and opportunities Use cases specific to Vietnamese market Challenges and solutions for local businesses Success stories and case studies Key Takeaways Machine Learning Best Practices End-to-end Platform Approach: Use SageMaker for complete ML lifecycle management Data Quality First: Invest in data preparation and labeling for better model performance MLOps Integration: Implement monitoring and automated workflows from the start Model Selection Strategy: Choose the right model based on use case, not just performance metrics Generative AI Implementation Foundation Model Selection: Understand strengths of each model (Claude, Llama, Titan) for different scenarios Prompt Engineering Mastery: Chain-of-Thought and Few-shot learning significantly improve results RAG for Accuracy: Use RAG architecture when factual accuracy is critical Agent Design: Build agents that can handle multi-step workflows with proper tool integration Safety First: Always implement guardrails for content filtering and compliance Production Readiness Start Small, Scale Gradually: Begin with pilot projects before full deployment Cost Optimization: Monitor and optimize inference costs with serverless options Security \u0026amp; Compliance: Implement proper access controls and data privacy measures Continuous Improvement: Monitor model performance and iterate based on real-world feedback Applying to Work Explore SageMaker: Start with SageMaker Studio for ML experimentation and model development Implement RAG Solutions: Build knowledge bases for domain-specific applications using RAG architecture Develop Bedrock Agents: Create autonomous agents for customer service or workflow automation Prompt Engineering Practice: Apply Chain-of-Thought and Few-shot techniques to improve AI responses Deploy Guardrails: Implement content filtering and safety controls for production GenAI applications MLOps Setup: Establish model monitoring and automated deployment pipelines using SageMaker capabilities Event Experience Attending the \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; workshop was an exceptional learning experience that provided comprehensive insights into AWS\u0026rsquo;s AI and machine learning capabilities. The event combined theoretical knowledge with practical demonstrations, giving me a clear understanding of how to implement AI/ML solutions on AWS.\nLearning from comprehensive agenda The structured agenda covered everything from foundational ML concepts to advanced Generative AI implementations. Starting with SageMaker platform overview helped me understand the complete ML lifecycle before diving into GenAI specifics. The progression from traditional ML to Generative AI showed the evolution and complementary nature of these technologies. Hands-on technical exposure The SageMaker Studio walkthrough demonstrated the practical workflow of building ML models, from data preparation to deployment. I learned about data labeling tools and how they can significantly improve model accuracy with proper data quality. The MLOps capabilities showed me how to implement continuous integration and monitoring for ML models in production. Generative AI deep dive The Amazon Bedrock session was eye-opening, showing me how to leverage foundation models without training them from scratch. Prompt Engineering techniques like Chain-of-Thought reasoning and Few-shot learning were demonstrated with practical examples. Learning about RAG architecture helped me understand how to build accurate AI applications that combine retrieval with generation. The Bedrock Agents demo showed how to build autonomous AI systems that can perform complex multi-step tasks. Practical demonstrations The live demo of building a Generative AI chatbot using Bedrock gave me a complete picture of implementation from start to finish. Seeing Guardrails in action demonstrated the importance of safety and content filtering in production GenAI applications. The comparison between Claude, Llama, and Titan models helped me understand when to use each model. Networking and discussions The workshop provided excellent networking opportunities with other AI/ML enthusiasts and practitioners in Vietnam. Discussing AI/ML landscape in Vietnam gave me context-specific insights into local market opportunities and challenges. Sharing experiences with peers helped me understand real-world implementation challenges and solutions. Lessons learned SageMaker provides a complete platform that simplifies the entire ML lifecycle, from data preparation to deployment.\nFoundation models in Bedrock eliminate the need to train large models from scratch, significantly reducing time and cost.\nRAG architecture is crucial for building accurate GenAI applications that need to reference specific knowledge bases.\nPrompt engineering is a skill that requires practice and understanding of different techniques for optimal results.\nGuardrails are essential for production GenAI applications to ensure safety and compliance.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Introduce DevOps culture, principles, and key metrics Demonstrate AWS DevOps services for CI/CD pipeline automation Explore Infrastructure as Code (IaC) with CloudFormation and CDK Cover container services and microservices deployment strategies Provide monitoring and observability best practices Share real-world DevOps case studies and best practices Event Details Date: Monday, November 17, 2025 Time: 8:30 AM – 5:00 PM Location: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: Full day (8.5 hours with breaks) Agenda Morning Session (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset\nRecap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline\nSource Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 – 10:45 AM | Break\n10:45 AM – 12:00 PM | Infrastructure as Code (IaC)\nAWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 – 1:00 PM) Afternoon Session (1:00 – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS\nDocker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 – 2:45 PM | Break\n2:45 – 4:00 PM | Monitoring \u0026amp; Observability\nCloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies\nDeployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up\nDevOps career pathways AWS certification roadmap Key Highlights DevOps Culture and Principles DevOps Mindset: Collaboration between development and operations teams Cultural Transformation: Breaking down silos and fostering shared responsibility Key Metrics (DORA): Measuring DevOps performance Deployment Frequency: How often deployments occur Lead Time: Time from code commit to production MTTR (Mean Time To Recovery): Time to recover from failures Change Failure Rate: Percentage of deployments causing failures Benefits: Faster delivery, improved reliability, better collaboration AWS CI/CD Pipeline Services AWS CodeCommit:\nFully managed source control service Git-based version control Integration with other AWS services Git strategies: GitFlow, Trunk-based development, feature branches AWS CodeBuild:\nFully managed build service Scalable build environments Supports multiple programming languages and build tools Build artifacts and test reports Integration with testing frameworks AWS CodeDeploy:\nAutomated deployment service Deployment strategies: Blue/Green: Zero-downtime deployments with instant rollback Canary: Gradual rollout with automatic rollback on errors Rolling: Rolling updates with configurable batch sizes Application deployment across EC2, Lambda, and on-premises AWS CodePipeline:\nFully managed continuous delivery service Visual workflow builder Integration with third-party tools Automated pipeline orchestration Approval gates and manual intervention points Infrastructure as Code (IaC) AWS CloudFormation:\nDeclarative IaC service JSON/YAML template syntax Stack management and resource provisioning Drift detection and stack updates Change sets for previewing changes Nested stacks for modular infrastructure AWS CDK (Cloud Development Kit):\nProgrammatic IaC using familiar programming languages TypeScript, Python, Java, C#, and Go support Constructs for reusable infrastructure patterns Higher-level abstractions and best practices Integration with CloudFormation CLI tools for deployment and management Choosing Between IaC Tools:\nCloudFormation: Declarative, template-based, AWS-native CDK: Programmatic, type-safe, developer-friendly Use cases and when to choose each approach Container Services on AWS Docker Fundamentals:\nContainerization benefits and use cases Microservices architecture with containers Docker image creation and optimization Multi-stage builds and best practices Amazon ECR (Elastic Container Registry):\nFully managed Docker container registry Image storage and versioning Image scanning for vulnerabilities Lifecycle policies for automated cleanup Integration with ECS and EKS Amazon ECS (Elastic Container Service):\nFully managed container orchestration Fargate (serverless) and EC2 launch types Task definitions and service configurations Auto-scaling and load balancing Service discovery and networking Amazon EKS (Elastic Kubernetes Service):\nManaged Kubernetes service Kubernetes-native orchestration Worker nodes management Add-ons and ecosystem integration Multi-tenant and namespace isolation AWS App Runner:\nSimplified container deployment Automatic scaling and load balancing Source code or container image deployment Built-in CI/CD integration Pay-per-use pricing model Monitoring \u0026amp; Observability Amazon CloudWatch:\nMetrics: Application and infrastructure metrics Logs: Centralized log management and analysis Alarms: Automated alerting and notifications Dashboards: Custom visualization of metrics and logs Insights: Automated anomaly detection Composite Alarms: Complex alerting logic AWS X-Ray:\nDistributed tracing for microservices Request flow visualization Performance bottleneck identification Service map generation Integration with Lambda, ECS, and API Gateway Trace analysis and filtering Best Practices:\nSetting up effective alerting strategies Creating meaningful dashboards On-call processes and incident response Log aggregation and analysis Metric collection and retention policies DevOps Best Practices Deployment Strategies:\nFeature Flags: Gradual feature rollouts A/B Testing: Comparing different versions Canary Deployments: Risk mitigation through gradual rollout Blue/Green Deployments: Zero-downtime updates Automated Testing:\nUnit, integration, and end-to-end testing Test automation in CI/CD pipelines Quality gates and test coverage Performance and load testing Incident Management:\nRunbook creation and maintenance Incident response procedures Postmortem analysis and learning Continuous improvement processes Key Takeaways DevOps Culture Transformation Cultural Change is Fundamental: Tools alone don\u0026rsquo;t make DevOps—culture and collaboration are key Measure What Matters: Use DORA metrics to track DevOps maturity Continuous Improvement: DevOps is a journey, not a destination Automation First: Automate repetitive tasks to focus on high-value work CI/CD Best Practices Start Simple, Scale Gradually: Begin with basic pipelines and add complexity over time Git Strategy Matters: Choose GitFlow or Trunk-based based on team size and release cadence Testing is Critical: Integrate automated testing at every stage Deployment Strategies: Use appropriate deployment strategy based on risk tolerance Infrastructure as Code: Always use IaC for reproducible and version-controlled infrastructure Container Orchestration Choose Wisely: ECS for simplicity, EKS for Kubernetes ecosystem Start with Serverless: Fargate eliminates node management overhead Optimize Images: Smaller images mean faster deployments and lower costs Security First: Scan images and use least-privilege IAM policies Observability Strategy Implement Full-Stack Observability: Metrics, logs, and traces together Proactive Monitoring: Set up alarms before incidents occur Meaningful Dashboards: Create dashboards that provide actionable insights Distributed Tracing: Essential for debugging microservices architectures Applying to Work Implement CI/CD Pipelines: Set up CodePipeline for automated deployments Adopt Infrastructure as Code: Use CloudFormation or CDK for all infrastructure Containerize Applications: Start containerizing applications for better portability Set Up Monitoring: Implement CloudWatch and X-Ray for observability Establish DevOps Practices: Create runbooks, incident response procedures, and postmortem templates Measure DevOps Metrics: Track DORA metrics to measure improvement Event Experience Attending the \u0026ldquo;DevOps on AWS\u0026rdquo; full-day workshop was an intensive and comprehensive learning experience that covered the entire DevOps spectrum from culture to implementation. The event provided both theoretical knowledge and practical demonstrations, giving me a complete understanding of implementing DevOps practices on AWS.\nLearning DevOps fundamentals The session started with DevOps mindset and culture, emphasizing that DevOps is more than just tools—it\u0026rsquo;s about collaboration and shared responsibility. I learned about DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) and how to measure DevOps maturity. Understanding the benefits of DevOps helped me see the bigger picture beyond technical implementation. AWS CI/CD pipeline deep dive The CodeCommit, CodeBuild, CodeDeploy, and CodePipeline walkthrough showed me how to build a complete CI/CD pipeline. Learning about different Git strategies (GitFlow vs Trunk-based) helped me understand when to use each approach. The deployment strategies (Blue/Green, Canary, Rolling) demo was eye-opening, showing how to minimize risk and downtime. The live CI/CD pipeline demo demonstrated the entire workflow from code commit to production deployment. Infrastructure as Code mastery CloudFormation demonstrated how to manage infrastructure declaratively with templates. AWS CDK showed me how to write infrastructure code in familiar programming languages, making it more maintainable. The comparison between CloudFormation and CDK helped me understand when to use each tool. Learning about drift detection and change sets gave me confidence in managing infrastructure safely. Container services exploration Docker fundamentals refreshed my understanding of containerization and its benefits. Amazon ECR showed how to manage container images securely with scanning and lifecycle policies. Comparing ECS and EKS helped me understand the trade-offs between managed services and Kubernetes flexibility. AWS App Runner introduced a simpler way to deploy containers without managing infrastructure. The microservices deployment case study provided real-world insights into choosing the right container service. Monitoring and observability setup CloudWatch comprehensive coverage showed me how to collect metrics, logs, and set up alarms. AWS X-Ray distributed tracing demonstrated how to debug complex microservices architectures. The full-stack observability demo showed how to connect all monitoring pieces together. Learning about alerting best practices and on-call processes provided practical operational knowledge. Best practices and case studies Deployment strategies like feature flags and A/B testing showed advanced techniques for safe deployments. Automated testing integration demonstrated how to build quality gates into CI/CD pipelines. Incident management practices and postmortem templates provided structure for handling production issues. Case studies from startups and enterprises showed real-world DevOps transformations and lessons learned. Career and certification guidance The DevOps career pathways discussion helped me understand different roles and skill requirements. The AWS certification roadmap provided clear guidance on certifications relevant to DevOps. Understanding the career progression gave me a roadmap for professional development. Practical demonstrations Every session included live demos that showed real implementations, not just slides. The full CI/CD pipeline walkthrough demonstrated end-to-end automation. CloudFormation and CDK demos showed both approaches to infrastructure management. Container deployment comparison helped me visualize different approaches side by side. Networking and discussions The full-day format allowed for extended networking with other DevOps practitioners. Q\u0026amp;A sessions provided opportunities to get answers to specific questions. Discussing real-world challenges with peers helped me understand common pitfalls and solutions. Lessons learned DevOps is a cultural transformation that requires buy-in from both development and operations teams.\nAutomation is essential but must be implemented thoughtfully to avoid creating technical debt.\nInfrastructure as Code is non-negotiable for modern DevOps practices.\nMonitoring and observability are crucial for maintaining production systems.\nStart simple and iterate rather than trying to implement everything at once.\nMeasure everything using DORA metrics to track improvement over time.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Introduce AWS Well-Architected Framework Security Pillar Demonstrate 5 main Security pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response Share best practices and core principles about cloud security Provide practical knowledge about threats and prevention in Vietnam Guide building security architecture according to AWS Well-Architected standards Connect with security experts and cloud practitioners Event Details Date: Saturday, November 29, 2025 Time: 8:30 AM – 12:00 PM Location: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh Duration: 3.5 hours (including coffee break) Agenda 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation (20 minutes) Role of Security Pillar in Well-Architected Framework Core Principles: Least Privilege: Grant minimum necessary permissions Zero Trust: Never trust, always verify Defense in Depth: Multiple layers of protection Shared Responsibility Model: AWS and customer responsibilities Top threats in cloud environment in Vietnam Q\u0026amp;A ⭐ Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture (40 minutes) IAM Fundamentals:\nUsers, Roles, Policies – avoid long-term credentials Best practices for IAM setup Temporary credentials and session management IAM Identity Center:\nSingle Sign-On (SSO) configuration Permission sets and assignment Multi-account management Advanced IAM:\nService Control Policies (SCP) for multi-account Permission boundaries to limit permissions MFA (Multi-Factor Authentication) requirements Credential rotation strategies Access Analyzer to detect external access Mini Demo: Validate IAM Policy + simulate access\nCheck policy syntax and permissions Simulate access scenarios Troubleshoot common IAM issues ⭐ Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring (25 minutes) AWS Security Services:\nCloudTrail: Organization-level logging and audit GuardDuty: Threat detection and intelligent security Security Hub: Centralized security findings Comprehensive Logging:\nVPC Flow Logs: Network traffic monitoring ALB Access Logs: Application layer monitoring S3 Access Logs: Object access tracking Logging at every layer of infrastructure Alerting \u0026amp; Automation:\nEventBridge rules for security events Automated response workflows Integration with notification systems Detection-as-Code:\nInfrastructure as Code for security rules Version control for detection rules Automated deployment and testing 9:55 – 10:10 AM | Coffee Break (15 minutes) Break time Networking with participants Informal Q\u0026amp;A ⭐ Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security (30 minutes) Network Security:\nVPC Segmentation: Separate network segments Private vs Public placement strategies Network isolation and security zones Security Groups vs NACLs:\nWhen to use Security Groups When to use NACLs Practical application models Best practices and common mistakes Advanced Network Protection:\nAWS WAF: Web Application Firewall AWS Shield: DDoS protection Network Firewall: Managed network firewall service Workload Protection:\nEC2 Security: Instance hardening, patch management ECS Security: Container security best practices EKS Security: Kubernetes security fundamentals Security baselines and compliance ⭐ Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets (30 minutes) AWS KMS (Key Management Service):\nKey policies and access control Grants and delegation Key rotation strategies Multi-region key management Encryption at Rest:\nS3: Server-side encryption (SSE-S3, SSE-KMS, SSE-C) EBS: Volume encryption and snapshots RDS: Database encryption DynamoDB: Table encryption Encryption in Transit:\nTLS/SSL best practices Certificate management End-to-end encryption Secrets Management:\nSecrets Manager: Automated rotation patterns Parameter Store: Secure parameter storage Rotation patterns and best practices Integration with applications Data Classification \u0026amp; Access Guardrails:\nData classification frameworks Access controls based on classification Compliance and regulatory requirements ⭐ Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation (30 minutes) IR Lifecycle according to AWS:\nPrepare: Preparation and planning Detect: Incident detection Respond: Response and containment Recover: Recovery and lessons learned IR Playbooks for Common Scenarios:\n1. Compromised IAM Key:\nDetect compromised credentials Immediate response steps Key rotation and access revocation Investigation and forensics 2. S3 Public Exposure:\nDetect public buckets Immediate remediation Access review and audit Prevention strategies 3. EC2 Malware Detection:\nDetect malware and suspicious activity Isolation procedures Evidence collection Cleanup and recovery Automated Response:\nLambda functions for automated response Step Functions for complex workflows Integration with security services Playbook automation patterns Evidence Collection:\nSnapshot creation for forensics Log preservation Chain of custody Compliance with legal requirements 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A (20 minutes) Summary of 5 Security pillars Common pitfalls and frequent mistakes Vietnamese enterprise reality: Security challenges in Vietnam Compliance requirements Best practices for local context Security learning roadmap: AWS Certified Security – Specialty AWS Certified Solutions Architect – Professional Security training paths Q\u0026amp;A session Commemorative photos Key Highlights Security Foundation Principles Least Privilege:\nGrant only minimum necessary permissions to perform work Regular review and audit permissions Use temporary credentials instead of long-term keys Principle of least privilege at every layer Zero Trust:\nNever trust by default, always verify Verify identity and authorization for every request Network segmentation and micro-segmentation Continuous verification and monitoring Defense in Depth:\nMultiple layers of protection: Network, Application, Data, Identity Don\u0026rsquo;t rely on a single layer of protection Layered security controls Fail-safe defaults Shared Responsibility Model:\nAWS: Security OF the cloud (infrastructure) Customer: Security IN the cloud (data, applications, configurations) Understand responsibilities of each party Best practices for customer responsibilities Pillar 1: Identity \u0026amp; Access Management Modern IAM Architecture:\nUse IAM Roles instead of Users when possible Temporary credentials with STS IAM Identity Center for SSO Permission boundaries and SCPs Best Practices:\nEnable MFA for all users Regular credential rotation Use Access Analyzer to detect external access Least privilege policies Regular access reviews Pillar 2: Detection Comprehensive Monitoring:\nCloudTrail for audit trail GuardDuty for threat detection Security Hub for centralized view VPC Flow Logs for network monitoring Detection-as-Code:\nVersion control for detection rules Automated testing CI/CD for security rules Infrastructure as Code approach Pillar 3: Infrastructure Protection Network Security:\nVPC segmentation Security Groups and NACLs WAF, Shield, Network Firewall Private subnets and NAT gateways Workload Security:\nEC2 hardening Container security Kubernetes security Patch management Pillar 4: Data Protection Encryption:\nEncryption at rest with KMS Encryption in transit with TLS Key management best practices Secrets management Data Classification:\nClassify data by sensitivity Apply appropriate controls Access guardrails Compliance requirements Pillar 5: Incident Response IR Lifecycle:\nPrepare: Planning and tools Detect: Monitoring and alerting Respond: Containment and investigation Recover: Restoration and lessons learned Automation:\nAutomated response with Lambda Step Functions for workflows Integration with security services Playbook automation Key Takeaways Security Foundation Well-Architected Framework: Understanding Security Pillar and its role Core Principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility: AWS and customer responsibilities Threat Landscape: Top threats in Vietnam and prevention methods IAM Best Practices Modern IAM: Use roles, temporary credentials IAM Identity Center: SSO and permission management Advanced Features: SCPs, permission boundaries, Access Analyzer Security: MFA, credential rotation, least privilege Detection \u0026amp; Monitoring Security Services: CloudTrail, GuardDuty, Security Hub Comprehensive Logging: VPC Flow Logs, ALB logs, S3 logs Alerting: EventBridge and automation Detection-as-Code: Infrastructure as Code for security Infrastructure Protection Network Security: VPC segmentation, Security Groups, NACLs Advanced Protection: WAF, Shield, Network Firewall Workload Security: EC2, ECS, EKS security Best Practices: Hardening and patch management Data Protection Encryption: At rest and in transit KMS: Key management and rotation Secrets Management: Secrets Manager and Parameter Store Data Classification: Access guardrails and compliance Incident Response IR Lifecycle: Prepare, Detect, Respond, Recover Playbooks: Common scenarios and response procedures Automation: Lambda and Step Functions Forensics: Evidence collection and preservation Applying to Work Design Security Architecture: Apply 5 pillars to architecture design Implement IAM Best Practices: Use modern IAM patterns Setup Detection: Deploy comprehensive monitoring Protect Infrastructure: Apply network and workload security Protect Data: Implement encryption and secrets management Prepare IR: Build incident response playbooks and automation Security Reviews: Regular security assessments and improvements Event Experience Attending the \u0026ldquo;AWS Well-Architected Security Pillar\u0026rdquo; workshop was an intensive learning experience about cloud security. The event provided comprehensive knowledge about the 5 Security pillars and practical best practices, helping me understand how to build secure cloud architecture.\nSecurity Foundation Opening session introduced Security Pillar in Well-Architected Framework. I learned about core principles: Least Privilege, Zero Trust, Defense in Depth. Shared Responsibility Model helped me understand AWS and customer responsibilities. Top threats in Vietnam provided real-world context for security challenges. Modern IAM Architecture IAM session went deep into modern IAM patterns and best practices. Learning about IAM Identity Center for SSO and multi-account management. Advanced features like SCPs and permission boundaries were very useful. Demo validate IAM policy showed practical approach to test policies. Detection \u0026amp; Continuous Monitoring Detection session covered comprehensive monitoring strategy. CloudTrail, GuardDuty, and Security Hub form a powerful security monitoring stack. Logging at every layer helped me understand defense in depth. Detection-as-Code approach was very innovative and practical. Network \u0026amp; Workload Security Infrastructure Protection session went deep into network security. Clear understanding of when to use Security Groups vs NACLs. Advanced protection with WAF, Shield, Network Firewall. Workload security for EC2, ECS, EKS provided practical guidance. Data Protection Data Protection session covered encryption and secrets management. KMS key management and rotation strategies are very important. Encryption at rest and in transit for all services. Secrets Manager patterns for automated rotation. Incident Response IR session provided practical playbooks for common scenarios. Learning about IR lifecycle and best practices. Automated response with Lambda and Step Functions is very powerful. Evidence collection procedures for forensics and compliance. Wrap-up and Q\u0026amp;A Wrap-up session summarized 5 pillars comprehensively. Common pitfalls helped me avoid frequent mistakes. Vietnamese enterprise reality provided local context. Learning roadmap for security certifications was very useful. Lessons Learned Security is foundation: Must design security from the start, not add-on later.\nDefense in Depth: Don\u0026rsquo;t rely on a single layer of protection.\nAutomation is key: Automated detection and response reduce response time.\nContinuous improvement: Security is an ongoing process, not a one-time setup.\nCompliance matters: Understand regulatory requirements and best practices.\nPractice makes perfect: Need regular practice and review.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Integration with Amazon Bedrock:\nSeamless integration with Bedrock models Custom model selection Prompt engineering and optimization Response formatting and validation Context optimization APIs Key Takeaways Understanding Building Agentic AI What Agentic AI is: Autonomous AI agents capable of performing tasks independently Context Optimization: Techniques to optimize context and reduce costs Use cases: Real-world use cases for agentic AI Architecture: Architecture and design patterns AWS Bedrock Agent Bedrock Agent Core: Understanding how agents work and interact Context Management: Managing and optimizing context Integration: How to integrate with other AWS services Best Practices: Best practices from AWS experts Agentic Workflow Design Workflow patterns: Common patterns for agentic workflows Orchestration: How to manage and coordinate multiple agents Context Optimization: Strategies for context optimization Error handling: Strategies for error handling and recovery CloudThinker Platform Agentic orchestration: How CloudThinker addresses orchestration challenges Context optimization: Advanced techniques to optimize context Platform capabilities: Features and capabilities of CloudThinker Integration patterns: How to integrate CloudThinker into existing systems Hands-on Experience Practical skills: Practical skills in building Bedrock Agents Context optimization: Practicing context optimization techniques Troubleshooting: How to debug and troubleshoot common issues Real-world scenarios: Working with real-world scenarios Applying to Work Build Agentic AI: Use AWS Bedrock Agent to build autonomous AI agents Context Optimization: Apply context optimization techniques to reduce costs and improve performance Design Workflows: Apply agentic workflow patterns to projects Integrate CloudThinker: Evaluate and integrate CloudThinker into existing solutions Best Practices: Apply best practices from the workshop to production systems Event Experience Attending the \u0026ldquo;Building Agentic AI: Context Optimization with Amazon Bedrock\u0026rdquo; workshop was an intensive learning experience about agentic AI and context optimization. The event provided both theoretical knowledge and hands-on practice, giving me a clear understanding of how to build and optimize autonomous AI agents.\nOpening and Introduction Opening session by Nguyen Gia Hung created a professional and inspiring atmosphere. I understood the importance of Building Agentic AI and Context Optimization. Overview of event agenda helped me visualize the learning journey. AWS Bedrock Agent Core Session by Kien Nguyen provided a solid foundation about Bedrock Agent. I learned about architecture, components, and how Bedrock Agent works. Context management was a key highlight in this session. Demo of creating Bedrock Agent showed the actual process from start to finish. Real-World Use Case Use case presentation by Viet Pham illustrated how agentic workflows are used in production. Learning about real-world challenges and how to solve them. Context optimization in production was very practical and insightful. Agentic workflow demo showed performance and capabilities in practice. CloudThinker Platform CloudThinker introduction by Thang Ton introduced the platform and solution. L300 session by Henry Bui went deep into technical details and advanced patterns. Learning about advanced context optimization techniques to improve performance and reduce costs. CloudThinker platform demo showed capabilities and ease of use. Hands-on Workshop Hands-on workshop by Kha Van provided direct practice opportunities. Building Bedrock Agent from scratch with guidance from an expert. Practicing context optimization and agentic orchestration. Troubleshooting session helped me understand how to solve common issues. Direct Q\u0026amp;A provided answers to specific questions. Networking and Connections Networking sessions allowed connections with AWS experts and AI practitioners. Sharing experiences and learnings with other participants. Lunch buffet created opportunities for informal discussions and connections. Meeting experts and receiving advice about career development. Lessons Learned Agentic AI is the future: Autonomous AI agents will change how we build AI applications.\nContext Optimization is key: Optimizing context can significantly reduce costs and improve performance.\nHands-on practice is essential: Practical experience is crucial to truly understand and apply concepts.\nPlatform solutions matter: CloudThinker and similar platforms simplify building agentic systems.\nCommunity is valuable: Networking with experts and practitioners provides valuable insights and opportunities.\nSome event photos "
},
{
	"uri": "http://localhost:1313/fcj-workshop/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Dao Nguyen Khoi\nPhone Number: 0904096480\nEmail: khoi.dao2411@gmail.com\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Giao lưu và tìm hiểu thêm về team First Cloud Journey. Tìm hiểu về các dịch vụ cơ bản của AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get to know about FCJ members\n- Read and try to remember about internship rules and regulations 09/02/2025 09/02/2025 3 - Study AWS and its main service groups:\n+ Compute + Storage + Networking + Database 09/03/2025 09/03/2025 https://000001.awsstudygroup.com/en/ 4 - Register AWS Free Tier account\n- Explore and Test about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + Basic CLI tasks 09/04/2025 09/04/2025 https://000001.awsstudygroup.com/en/ 5 - Configure basic security:\n+ Setup Virtual MFA Device + Create admin group \u0026amp; admin user + Account authentication support + Create Budget 09/05/2025 09/05/2025 https://000007.awsstudygroup.com/en/ 6 -Practice cost management: + Create Cost Budget + Create Usage Budget + Reservation Instance (RI) + Savings Plans Budget 09/06/2025 09/06/2025 https://000007.awsstudygroup.com/en/ 7 -Submit AWS support request and manage responses. -Write worklog \u0026amp; self-assess AWS fundamentals. - Prepare for Week 2 goals. volume 09/07/2025 09/07/2025 https://000009.awsstudygroup.com/en/ Week 1 Result: About internship:\nCompleted onboarding/introduction with Mr. Hung and the FCJ team. Familiarized myself with the internship policies and regulations. Understood what AWS stands for , learned and understood the basic service groups:\nCompute Storage Networking Database Set up account and Finished configuration:\nRegistered account with AWS Free Tier Configured basic security: enabled MFA, created admin group \u0026amp; admin user (IAM user). Created budgets to monitor costs: Cost Budget, Usage Budget, RI, Savings Plans. Management tools:\nTested and used AWS Management Console: navigating and using services via GUI. Installed and configured AWS CLI with: Access Key, Secret Key, default Region. Hands-on labs with AWS CLI:\nChecked account and configuration info. Listed regions. Viewed EC2 information. Created and managed key pairs. Monitored running services. Console \u0026amp; CLI integration:\nManaged AWS resources in parallel using Console and CLI. Compared approaches and gained insights on when to use each tool. Personal reflection:\nFinished Week 1 worklog reflected the lack of current understand about work and AWS fully Will try to understand fully in upcoming weeks "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Understand the concept and structure of VPC (CIDR, Subnet, Route Table, ENI). Learn to configure firewalls in VPC (NACL, Security Group). Understand the concept of networking services: VPN, Direct Connect. Experience on Load Balancer Experience on creating and configuring core components: VPC, Subnet, Route Table, IGW, EBS, Elastic IP. Understand about connection to EC2 using remote via SSH Experience Hybrid DNS using Route 53 Resolver. Experience multiple VPCs using VPC Peering. Testing AWS Transit Gateway to manage inter-VPC connections Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory - What is VPC and how to optimize cloud service usage - Learn about VPC: + Subnet, CIDR + Route table + ENI (Elastic Network Interface)s 09/15/2025 09/15/2025 1. AWS VPC Documentation 2. YouTube - AWS VPC 3 - Configure VPC firewalls: NACL, Security Group - VPN, Direct Connect - Load Balancer - Extra Resources 09/14/2025 09/14/2025 YouTube - AWS Security 4 - Labs: + VPC + Subnet + Route Table + IGW(Internet GateWay) + EBS + \u0026hellip; - Remote SSH into EC2 - Learn Elastic IP 09/15/2025 09/15/2025 AWS Study Group - 000003 : Amazon VPC and AWS Site-to-Site VPN Workshop 5 - Labs: + Set up Hybrid DNS with Route 53 Resolver + Set up VPC Peering 09/16/2025 09/16/2025 AWS Study Group - 000010 : Set up Hybrid DNS with Route 53 Resolver AWS Study Group - 000019 : Setting up VPC Peering 6 - Labs: + Continue on experiencing VPC Peering + Set up AWS Transit Gateway 09/17/2025 09/17/2025 AWS Study Group - 000020 : Set up AWS Transit Gateway Week 2 Result: Understood the fundamental of Amazon VPC, and its key components: CIDR blocks, subnets, route tables, and ENIs. Experienced the activity of securing VPCs using both Security Groups and Network ACLs, and understand their differences in scope and use cases. Explored AWS networking services : VPN and Direct Connect Learnt Elastic Load Balancing and its role in distributing traffic for high availability. Did labs about VPC essentials: creating subnets, configuring route tables, attaching internet gateways, working with EBS, and managing Elastic IPs. Reinforced skills in accessing and managing EC2 instances securely via SSH. Tested and practiced connecting multiple VPCs through VPC Peering. Deployed AWS Transit Gateway to design and manage scalable multi-VPC architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Build a Strong foundation of Amazon EC2 and its ecosystem (AMI, Backup, Key Pair, EBS, Instance Store, User Data, Metadata) knowledge. Learn about EC2 Auto Scaling and its role in elasticity and cost optimization. Search and experience related compute services including EFS, FSx, Lightsail, and AWS MGN. Reinforce AWS storage knowledge with practicing labs covering S3, AWS Backup, and Storage Gateway. Reinforce practical skills in configuring, managing, and scaling EC2 workloads. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn theory: + EC2 overview: AMI, Backup, Key Pair + EBS (Elastic Block Store) + Instance Store 09/22/2025 09/22/2025 1. AWS EC2 Documentation 2. AWS EBS Documentation 3 - Learn theory: + EC2 User Data + EC2 Metadata 09/23/2025 09/23/2025 AWS EC2 User Guide 4 - Learn theory: + EC2 Auto Scaling + Related services: EFS, FSx, Lightsail, MGN 09/24/2025 09/24/2025 AWS Auto Scaling 5 - Labs: + Lab 57: Start with Amazon S3 09/25/2025 09/25/2025 AWS Study Group - Lab57 : STARTING WITH AMAZON S3 6 - Labs: + Lab 13: Deploy AWS Backup to the System + Lab 24: Using AWS Storage Gateway 09/26/2025 09/26/2025 AWS Study Group - Lab13 :Deploy AWS Backup to the System AWS Study Group - Lab24:Using File Storage Gateway Week 3 Achievements: Made a solid theoretical knowledge foundation of Amazon EC2, including:\nAMI and backup strategies for resilience. Key Pair usage for secure SSH authentication. Differences between EBS (persistent storage) and Instance Store (ephemeral storage). How User Data and Metadata scripts automate instance initialization and provide dynamic configuration. The role of EC2 Auto Scaling in maintaining performance and cost efficiency. Learnt the related services:\nAmazon EFS and FSx for shared and high-performance file storage. Amazon Lightsail as a simplified alternative for small-scale workloads. AWS MGN for migrating workloads into AWS. Completed practical labs:\nLaunched and managed an S3 bucket (Lab57). Implemented AWS Backup to protect workloads (Lab13). Integrated on-premises systems with AWS using Storage Gateway (Lab24). Key skills acquired:\nConfidently distinguish storage types (EBS vs Instance Store vs EFS vs FSx). Automate EC2 lifecycle with User Data and Auto Scaling. Combine backup and hybrid storage solutions to create more resilient architectures. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Understand further about AWS’s core storage service Amazon S3. Further comprehension of key concepts: bucket, object, storage class, access point, static website hosting, and CORS. Study hybrid storage and data migration solutions such as AWS Storage Gateway and AWS Snow Family. experience with Amazon FSx for Windows File Server and the automated backup service AWS Backup. Practice deploying, managing, and integrating AWS storage services in a real-world environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn the theory of AWS Storage Service (S3) - Get familiar with the concepts of Bucket, Object, and the storage mechanism. 09/29/2025 09/29/2025 1. AWS S3 Documentation 3 - Learn about Access Point and Storage Class - Distinguish between storage classes: Standard, IA, Glacier, Deep Archive. 09/30/2025 09/30/2025 AWS S3 Storage Class info 4 - Explore S3 Static Website \u0026amp; CORS, Access Control, Object Key, Performance, and Glacier 10/01/2025 10/01/2025 AWS S3 web hosting User guide 5 - Labs: + Lab14 – VM Import/Export. 10/02/2025 10/02/2025 AWS Study Group - Lab14 : VM Import/Export 6 - Labs: + Lab25 – Amazon FSx for Windows File Server. - Review and consolidate all AWS storage services. 10/03/2025 10/03/2025 AWS Study Group - Lab25 :Amazon FSx for Windows File Server Week 4 Achievements: Understood architecture and operating principles of Amazon S3, including:\nHow to create and manage Buckets, Objects, and Access Policies. Different Storage Classes and strategies for optimizing storage costs. How to configure S3 Static Website Hosting and handle CORS for web applications. Experienced with S3 Glacier – a cold storage service that helps save costs for infrequently accessed data.\nUnderstood of Hybrid Storage \u0026amp; Data Migration through:\nAWS Snow Family (Snowcone, Snowball, Snowmobile). AWS Storage Gateway – a solution to connect on-premises systems with AWS Cloud. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Understand the Shared Responsibility Model of AWS. Learn that the key AWS security services are: IAM, Cognito, Security Hub, KMS, Identity Center. Reinforce resource management and security through IAM Permissions Boundaries, Resource Tags, and encryption techniques. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn the theory of the Shared Responsibility Model and AWS security principles. - Review documentation for AWS security services: + Amazon IAM + Amazon Cognito + AWS Identity Center + AWS KMS + AWS Security Hub 10/06/2025 10/06/2025 AWS Study group: Deploy AWS Backup to the System 3 - Labs: + Configure and use AWS Security Hub to monitor and detect security issues. + Create and manage IAM Users, Roles, and Policies for AWS accounts. + Create IAM Groups and manage access permissions for user groups. 10/07/2025 10/07/2025 AWS Study group: Optimizing EC2 Costs with Lambda 4 - Labs: + Optimize EC2 costs using Lambda for automated start/stop of EC2 instances. + Manage EC2 access via Resource Tags using IAM. 10/08/2025 10/08/2025 AWS Study Group :Manage Resources Using Tags and Resource Groups 5 - Labs: + Configure IAM Permission Boundaries to limit user privileges. + Encrypt data using AWS KMS. 10/09/2025 10/09/2025 AWS Study Group : Manage Resources Using Tags and Resource Groups 6 - Further Experiment + Learn and apply security methods in AWS Organizations for multi-account management. + Enhance proficiency in AWS Identity Center for managing and synchronizing users and groups across AWS services. 10/10/2025 10/10/2025 AWS Study Group - : LIMITATION OF USER RIGHTS WITH IAM PERMISSION BOUNDARY Week 5 Achievements: Understanding and Applying the AWS Shared Responsibility Model\nUnderstood that AWS is responsible for the security of the cloud infrastructure, while users are responsible for securing their own data and applications. Clarified the role in ensuring compliance and protection when deploying services on AWS. Theoretical Knowledge of AWS Core Security Services\nAmazon IAM: Learned how to create and manage Users, Roles, and Policies to control user and group access. Amazon Cognito: Studied user management and authentication for AWS applications. AWS Identity Center: Understood how to link and manage user access across multiple AWS services. AWS Security Hub: Configured and utilized it to monitor and detect security threats. AWS KMS: Practiced encrypting data at rest and securing sensitive data using encryption keys. Practical Implementation of AWS Security Services\nSuccessfully installed and configured AWS Security Hub for continuous monitoring and vulnerability detection. Configured IAM Permissions Boundaries to restrict user privileges and prevent unauthorized access. EC2 Cost Optimization with Lambda: Automated the shutdown of unused EC2 instances to minimize operational costs. EC2 Access Control via IAM \u0026amp; Resource Tags: Applied IAM Policies that use Tags to precisely define access scope. Enhanced AWS Resource Management Skills\nCreated and managed IAM Groups and Policies, improving group-based access control. Learned how to manage multiple AWS accounts using AWS Organizations, ensuring consistent security policies across the organization. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Review fundamental Database Concepts: relational model, primary/foreign keys, ACID, normalization, OLTP vs OLAP. Understand Amazon RDS as a managed relational database service on AWS: engines, Multi-AZ, read replicas, backup, and scaling. Learn the benefits of Amazon Aurora compared to standard RDS engines: performance, high availability, automatic storage scaling, MySQL/PostgreSQL compatibility. Get familiar with Amazon Redshift as a petabyte-scale data warehouse for analytics, and distinguish it from RDS (OLTP workloads). Learn how Amazon ElastiCache (Redis / Memcached) provides an in-memory cache layer to reduce latency and offload backend databases. Practice Database Schema Conversion \u0026amp; Migration using AWS DMS and AWS Schema Conversion Tool (SCT) for moving databases to AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Review Database Concepts : relational model, ACID, transactions, indexing, normalization, OLTP vs OLAP. - Map traditional on-premises database concepts to AWS cloud services. 10/13/2025 10/13/2025 AWS Study group Youtube Video 3 - Study Amazon RDS \u0026amp; Amazon Aurora theory . - Learn about supported engines, Multi-AZ, automated backups, snapshots, read replicas, and scaling. - Compare RDS vs Aurora in terms of performance, availability, and cost. 10/14/2025 10/14/2025 AWS AmazonRDS user guide AWS AmazonRDS Aurora document 4 - Study Amazon Redshift \u0026amp; Amazon ElastiCache . - Distinguish OLTP (RDS/Aurora) vs OLAP (Redshift) and in-memory cache layer (ElastiCache). - Explore common use cases: data warehouse \u0026amp; BI, caching sessions, leaderboard, rate limiting, etc. 10/15/2025 10/15/2025 Info about AWS Redshift Info about AWS Elasticache 5 - Practice: + Module 06-Lab 5 – Amazon Relational Database Service (Amazon RDS). + Create an RDS instance, configure security group, parameter group, backups. + Connect from a client, run queries, and test behavior 10/16/2025 10/16/2025 Amazon Relational Database Service (Amazon RDS) 6 - Practice: + Module 06-Lab 43 – Database Schema Conversion \u0026amp; Migration. + Use AWS Schema Conversion Tool (SCT) to analyze and convert schema from source DB to RDS/Aurora/Redshift target. + Use AWS Database Migration Service (DMS) to migrate data . - Summarize and review all AWS Database Services . 10/17/2025 10/17/2025 Database Schema Conversion \u0026amp; Migration Week 6 Achievements: Consolidated understanding of core database concepts:\nRelational tables, primary/foreign keys, relational integrity, and basic indexing. ACID properties of transactions and why they matter in OLTP workloads. Clear distinction between OLTP vs OLAP and how this maps to AWS services. Gained hands-on familiarity with Amazon RDS:\nCreated and managed RDS instances via AWS Management Console. Reviewed supported engines (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora) and their typical use cases. Practiced configuring Multi-AZ, automated backups, snapshots, monitoring, and basic scaling options. Understood the strengths of Amazon Aurora:\nAurora as a cloud-native, MySQL/PostgreSQL-compatible database with significantly improved performance over standard engines. Aurora DB cluster architecture, with a distributed storage layer across multiple AZs. Reader and writer endpoints, automatic storage scaling, and high availability design. Understand the usage of Amazon Redshift \u0026amp; ElastiCache:\nRedshift as a columnar, petabyte-scale data warehouse optimized for analytics and BI workloads. How Redshift differs from RDS/Aurora: optimized for complex queries over large datasets rather than transactional workloads. ElastiCache (Redis/Memcached) as a fully managed, low-latency in-memory cache layer to increase throughput and reduce load on backend databases. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Understand Amazon DynamoDB as a fully managed NoSQL database service: key-value and document data models, partition keys, sort keys, global secondary indexes (GSI), and on-demand vs provisioned capacity modes. Learn how to build and manage Data Lakes on AWS using services like Amazon S3, AWS Glue, Amazon Athena, and Amazon QuickSight for analytics workloads. Explore AWS Analytics services: Amazon Athena for serverless SQL queries on S3, AWS Glue for ETL operations, and Amazon QuickSight for business intelligence and visualization. Practice data ingestion, transformation, and analysis workflows in the AWS cloud environment. Understand cost optimization and performance tuning strategies for analytics workloads on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Practice: Data Lake on AWS. + Understand data lake architecture on AWS using S3 as the data lake storage layer. + Learn about data ingestion, cataloging, and querying patterns. + Explore integration between S3, Glue, and Athena for analytics. 10/20/2025 10/20/2025 Data Lake on AWS 3 - Practice: Amazon DynamoDB Immersion Day. + Deep dive into DynamoDB core concepts: tables, items, attributes, primary keys, and indexes. + Practice creating tables, inserting data, and querying with partition keys and sort keys. + Understand DynamoDB capacity modes (on-demand vs provisioned) and pricing models. 10/21/2025 10/21/2025 Amazon DynamoDB Immersion Day 4 Practice: Cost and performance analysis with AWS Glue and Amazon Athena. + Use AWS Glue to catalog data stored in S3 and create data catalogs. + Run SQL queries on S3 data using Amazon Athena. + Analyze cost implications and optimize query performance. + Understand partitioning strategies for cost-effective analytics. Practice: Work with Amazon DynamoDB. + Create DynamoDB tables with appropriate key schemas. + Perform CRUD operations (Create, Read, Update, Delete) on DynamoDB items. + Work with Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI). + Practice querying and scanning operations, understanding the differences. 10/22/2025 10/22/2025 Cost and performance analysis with AWS Glue and Amazon Athena Work with Amazon DynamoDB 5 - Practice: Building a Datalake with Your Data. + Build a complete data lake solution using AWS services. + Implement data ingestion pipelines. + Set up data transformation workflows with AWS Glue. + Create analytics-ready datasets for downstream consumption. - Practice: Analytics on AWS workshop. + Comprehensive workshop covering the full analytics stack on AWS. + Integrate multiple services: S3, Glue, Athena, and visualization tools. + Build end-to-end analytics solutions from raw data to insights. 10/23/2025 10/23/2025 Building a Datalake with Your Data Analytics on AWS workshop 6 - Practice: Get started with QuickSight. + Create visualizations and dashboards using Amazon QuickSight. + Connect QuickSight to various data sources (S3, Athena, RDS, etc.). + Use AWS Database Migration Service (DMS) to migrate data . + Build interactive reports and share insights with stakeholders. 10/24/2025 10/24/2025 Get started with Quick Sight Week 7 Achievements: Gained comprehensive understanding of Amazon DynamoDB:\nDynamoDB as a fully managed, serverless NoSQL database service with single-digit millisecond latency. Key concepts: tables, items, attributes, primary keys (partition key + optional sort key), and data modeling best practices. Global Secondary Indexes (GSI) and Local Secondary Indexes (LSI) for flexible query patterns. Capacity modes: on-demand (pay-per-request) vs provisioned (reserved capacity) and when to use each. DynamoDB Streams for real-time data processing and change data capture. Got more experience on Data Lake architecture on AWS:\nAmazon S3 as the foundation for data lake storage with lifecycle policies, versioning, and encryption. Data lake architecture patterns: raw zone, processed zone, and curated zone. Data ingestion strategies: batch uploads, streaming data, and integration with various data sources. Best practices for organizing data in S3: partitioning, naming conventions, and folder structures. Understood more about AWS Analytics services:\nAWS Glue: Serverless ETL service for discovering, cataloging, and transforming data. Glue Data Catalog as a centralized metadata repository. Glue ETL jobs for data transformation using Apache Spark. Glue Crawlers for automatic schema discovery. Amazon Athena: Serverless interactive SQL query service for analyzing data in S3. Pay-per-query pricing model and cost optimization strategies. Integration with Glue Data Catalog for schema-on-read queries. Query performance optimization through partitioning and columnar formats (Parquet, ORC). Amazon QuickSight: Cloud-native business intelligence and visualization service. Creating dashboards, visualizations, and reports. Connecting to various data sources (S3, Athena, RDS, Redshift, etc.). Sharing insights with teams and embedding analytics in applications. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Complete the Edge Layer and Frontend Storage: Route 53, S3, CloudFront, AWS WAF, and ACM Certificate. Set up DNS management with Route 53 hosted zone and domain configuration. Configure S3 bucket for static frontend hosting with proper access policies. Deploy CloudFront distribution for global content delivery with Origin Access Control. Implement AWS WAF protection with security rules (SQL injection, XSS, bot control). Set up ACM Certificate and enable HTTPS for secure content delivery. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - System Requirements Analysis \u0026amp; Architecture Design: + Analyze system requirements and review the complete architecture diagram. + Identify all components: Route 53, S3, CloudFront, WAF, ACM, VPC, EC2, RDS, API Gateway. + Create High-Level Design (HLD) document with architecture overview. + Document data flow: Users → Route 53 → CloudFront → WAF → S3 (Frontend). + Plan IP addressing scheme and resource naming conventions. 10/26/2025 10/26/2025 Architecture diagram 3 - Route 53 Setup: + Create Route 53 hosted zone for domain management. + Create A record pointing to CloudFront distribution + Create CNAME records for subdomains if needed. + Configure DNS settings and verify domain ownership. + Document DNS configuration and record types. - S3 Frontend Bucket Configuration: + Create S3 bucket for frontend static assets (FE Bucket) with appropriate naming. + Enable static website hosting on S3 bucket. + Configure public access policy for CloudFront access (block public access, allow CloudFront via OAC). + Upload test frontend files (HTML, CSS, JS, images) to S3 bucket. + Test static website hosting endpoint and verify file accessibility. 10/27/2025 10/27/2025 Route 53 documentation S3 documentation 4 - CloudFront Distribution Setup: + Create CloudFront distribution with S3 bucket as origin. + Configure Origin Access Control (OAC) for secure S3 access (replacing OAI). + Set up cache policies (CachingOptimized, CachingDisabled, etc.). + Configure default root object (index.html). + Map Route 53 domain to CloudFront distribution . + Test CloudFront distribution and verify content delivery. 10/28/2025 10/28/2025 CloudFront documentation 5 - AWS WAF Integration: + Create AWS WAF WebACL for CloudFront protection. + Add managed rules: AWS Managed Rules for SQL injection protection. + Add managed rules: AWS Managed Rules for XSS (Cross-Site Scripting) protection. + Configure bot control rules to block common bots and scrapers. + Associate WAF WebACL with CloudFront distribution. + Test WAF rules by attempting common attack patterns and verify blocking. 10/29/2025 10/29/2025 AWS WAF documentation 6 - AWS WAF Integration: + Request ACM Certificate in us-east-1 region (required for CloudFront). + Validate certificate using DNS validation method (add CNAME records to Route 53). + Wait for certificate validation and issuance. + Bind ACM certificate to CloudFront distribution. + Configure CloudFront to use HTTPS only (redirect HTTP to HTTPS). + Test HTTPS connection and verify SSL/TLS certificate is working correctly. 10/30/2025 10/30/2025 ACM documentation Week 8 Achievements: Successfully completed system analysis and architecture design:\nAnalyzed system requirements and reviewed complete architecture diagram. Created High-Level Design (HLD) document with architecture overview and component relationships. Documented data flow from users through edge services to frontend storage. Established resource naming conventions and planning documentation. Set up Route 53 DNS management:\nCreated Route 53 hosted zone for domain management. Configured A and CNAME records for domain routing. Established DNS foundation for connecting domain to CloudFront distribution. Configured S3 for static frontend hosting:\nCreated S3 bucket for frontend static assets with proper naming conventions. Enabled static website hosting on S3 bucket. Configured public access policies: blocked public access, allowed CloudFront access via Origin Access Control. Uploaded test frontend files and verified static website hosting functionality. Deployed CloudFront distribution:\nCreated CloudFront distribution with S3 bucket as origin. Configured Origin Access Control (OAC) for secure S3 access (modern replacement for OAI). Set up cache policies for optimized content delivery. Mapped Route 53 domain to CloudFront distribution. Verified content delivery through CloudFront CDN globally. Implemented AWS WAF protection:\nCreated AWS WAF WebACL with comprehensive security rules. Added AWS Managed Rules for SQL injection protection. Added AWS Managed Rules for XSS (Cross-Site Scripting) protection. Configured bot control rules to block malicious bots and scrapers. Associated WAF WebACL with CloudFront distribution. Tested WAF rules and verified protection against common attack patterns. Enabled HTTPS with ACM Certificate:\nRequested and validated ACM Certificate in us-east-1 region (required for CloudFront). Used DNS validation method with CNAME records in Route 53. Bound ACM certificate to CloudFront distribution. Configured CloudFront to enforce HTTPS (redirect HTTP to HTTPS). Verified SSL/TLS certificate is working correctly and secure connections are established. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Build VPC and Networking Core: Create VPC with public and private subnets, Internet Gateway, and NAT Gateway. Establish secure network boundaries with proper routing and subnet segmentation. Configure Security Groups for EC2, RDS, and ALB following least-privilege principles. Set up IAM roles and policies for EC2 instances with custom permissions. Enable VPC Flow Logs for network traffic monitoring and auditing. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - VPC Creation \u0026amp; Subnet Configuration: + Create VPC with CIDR block 10.0.0.0/16 in selected AWS region. + Create public subnet (10.0.1.0/24) in one Availability Zone with appropriate tags. + Create private subnet (10.0.2.0/24) in the same Availability Zone with appropriate tags. + Apply consistent tagging strategy (Name, Environment, Project, etc.) to all resources. + Document subnet allocation and IP addressing scheme. 02/11/2025 02/11/2025 AWS VPC documentation 3 - Internet Gateway Setup: + Create and attach Internet Gateway to VPC. + Configure public subnet route table to route internet-bound traffic (0.0.0.0/0) to Internet Gateway. + Verify public subnet route table configuration. + Test internet connectivity from public subnet (launch test EC2 instance if needed). + Document routing configuration and gateway associations. 03/11/2025 03/11/2025 Internet Gateway guide 4 - NAT Gateway Configuration: + Allocate Elastic IP address for NAT Gateway. + Create NAT Gateway in public subnet (10.0.1.0/24). + Configure private subnet route table to route internet-bound traffic (0.0.0.0/0) through NAT Gateway. + Verify private subnet route table configuration. + Test outbound internet connectivity from private subnet (launch test EC2 instance in private subnet). + Verify private subnet instances can reach internet while remaining isolated from inbound connections. 04/11/2025 04/11/2025 NAT Gateway documentation 5 - Security Groups Design \u0026amp; Implementation: + Create Security Group for EC2 instances: allow inbound from API Gateway/ALB, outbound to RDS and internet via NAT. + Create Security Group for RDS: allow inbound only from EC2 Security Group on database port (3306/5432). + Create Security Group for ALB (if used): allow inbound HTTP/HTTPS from internet, outbound to EC2 Security Group. + Apply least-privilege principle: grant minimum necessary permissions. + Document security group rules and relationships. 05/11/2025 05/11/2025 Security Groups guide 6 - IAM Roles \u0026amp; Policies for EC2: + Create IAM role for EC2 instances with descriptive name. + Create custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch, etc.). + Attach IAM role to EC2 instance profile. + Test IAM role permissions from EC2 instance (use AWS CLI or SDK). + Verify EC2 can access Secrets Manager to retrieve database credentials. + Document IAM roles and their permissions. 06/11/2025 06/11/2025 IAM best practices 7 - Network ACLs \u0026amp; VPC Flow Logs: + Review and configure Network ACLs (optional, default ACLs are usually sufficient). + Test Network ACL rules if custom rules are implemented. + Enable VPC Flow Logs to capture IP traffic flow information. + Configure Flow Logs destination (CloudWatch Logs or S3 bucket). + Review Flow Logs to audit network traffic patterns. + Audit and document all network configurations for security review. - Week 9 Summary: VPC and networking core complete, ready for backend and database deployment in Week 10. 07/11/2025 07/11/2025 VPC Flow Logs documentation Week 9 Achievements: Successfully created VPC and subnet infrastructure:\nCreated VPC with CIDR block 10.0.0.0/16 in selected AWS region.\nConfigured public subnet (10.0.1.0/24) for internet*facing resources with proper tagging.\nConfigured private subnet (10.0.2.0/24) for application servers with proper tagging.\nApplied consistent tagging strategy across all network resources for better management.\nSet up Internet Gateway for public subnet connectivity:\nCreated and attached Internet Gateway to VPC. Configured public subnet route table to route internet traffic (0.0.0.0/0) to Internet Gateway. Verified public subnet instances can access internet directly. Documented routing configuration and gateway associations. Configured NAT Gateway for private subnet outbound access:\nAllocated Elastic IP address and created NAT Gateway in public subnet. Configured private subnet route table to route internet traffic through NAT Gateway. Verified private subnet instances can reach internet for outbound connections (updates, downloads, API calls). Confirmed private subnet remains isolated from inbound internet connections (security best practice). Implemented Security Groups following least*privilege principles:\nCreated Security Group for EC2: allows inbound from API Gateway/ALB, outbound to RDS and internet. Created Security Group for RDS: allows inbound only from EC2 Security Group on database port. Created Security Group for ALB (if used): allows inbound HTTP/HTTPS, outbound to EC2. Applied least-privilege principle: granted minimum necessary permissions for each component. Documented security group rules and relationships for maintainability. Configured IAM roles and policies for EC2:\nCreated IAM role for EC2 instances with descriptive naming. Created custom IAM policy for EC2 to access required AWS services (S3, Secrets Manager, CloudWatch). Attached IAM role to EC2 instance profile. Tested IAM permissions from EC2 instance and verified access to Secrets Manager. Documented IAM roles and permissions for security audit. Enabled VPC Flow Logs for network monitoring:\nEnabled VPC Flow Logs to capture IP traffic flow information. Configured Flow Logs destination (CloudWatch Logs or S3 bucket). Reviewed Flow Logs to audit network traffic patterns and identify anomalies. Audited all network configurations for security compliance. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "This worklog is what I have been documenting my journey in AWS First Cloud Journey internship program, After 12 Exciting weeks learning and doing labs , I finally gained a fundamental AWS concepts,its core values. This help me deploy web applicaiton architechture on AWS\nDuration: 12 weeks (approximately 3 months) Completion Date: November 2025 Final Project: Production-ready AWS web application with CI/CD, monitoring, and security Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Basic AWS services exploration\nWeek 3: Advanced AWS concepts\nWeek 4: Hands-on labs and practice\nWeek 5: Advanced labs and workshops\nWeek 6: Database Services on AWS\nWeek 7: Analytics and Data Lake Services\nWeek 8: Edge Layer and Frontend Infrastructure Week 9: VPC and Networking Core\nWeek 10: Backend and Database Deployment\nWeek 11: CI/CD Pipeline and Monitoring\nWeek 12: Worklog finalization and presetation preparation\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/5.3.1-build-upload/",
	"title": "Build and Upload Backend",
	"tags": [],
	"description": "",
	"content": "Build Spring Boot Application In this section, you will build the Spring Boot backend application into a JAR file and upload it to S3 for deployment.\nStep 1: Build the JAR File Navigate to the backend directory: cd BE/workshop_BE Clean and build the project: On Windows:\n.\\mvnw.cmd clean package -DskipTests On Linux/Mac:\n./mvnw clean package -DskipTests Verify the JAR file was created: # Windows dir target\\workshop-0.0.1-SNAPSHOT.jar # Linux/Mac ls -lh target/workshop-0.0.1-SNAPSHOT.jar Expected Result: The file workshop-0.0.1-SNAPSHOT.jar should be in the target directory.\nStep 2: Upload JAR to S3 Get the backend bucket name from CloudFormation outputs: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;BackendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Upload the JAR file to S3: # Windows aws s3 cp BE\\workshop_BE\\target\\workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 # Linux/Mac aws s3 cp BE/workshop_BE/target/workshop-0.0.1-SNAPSHOT.jar s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Expected Result: upload: .../workshop-0.0.1-SNAPSHOT.jar to s3://...\nVerify the upload: aws s3 ls s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/ --region ap-southeast-1 Step 3: Get EC2 Instance ID You\u0026rsquo;ll need an EC2 instance ID to deploy the application. Get it from the Auto Scaling Group:\naws autoscaling describe-auto-scaling-groups \\ --region ap-southeast-1 \\ --query \u0026#34;AutoScalingGroups[?contains(AutoScalingGroupName, \u0026#39;workshop-aws-dev\u0026#39;)].Instances[0].InstanceId\u0026#34; \\ --output text Or list all instances:\naws ec2 describe-instances \\ --region ap-southeast-1 \\ --filters \u0026#34;Name=tag:Name,Values=*workshop-aws-dev*\u0026#34; \\ --query \u0026#34;Reservations[*].Instances[*].[InstanceId,State.Name]\u0026#34; \\ --output table Note: Save the instance ID for the next section.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop AWS Project Overview This workshop demonstrates how to deploy a complete full-stack web application on AWS using Infrastructure as Code (CloudFormation). You will learn to build a production-ready architecture with:\nBackend: Spring Boot REST API running on EC2 instances in private subnets Frontend: React application served via CloudFront from S3 Database: MySQL RDS instance for data persistence API Gateway: RESTful API Gateway for frontend-backend communication Load Balancer: Application Load Balancer for high availability Architecture Components VPC: Custom VPC with public and private subnets across 2 Availability Zones EC2 Auto Scaling Group: Backend application servers with auto-scaling capabilities RDS MySQL: Managed database service for application data S3 Buckets: Frontend static hosting and backend artifact storage CloudFront: CDN for global content delivery API Gateway: RESTful API endpoint with CORS support VPC Endpoints: Private connectivity to AWS services (S3 Gateway, SSM, SSM Messages, EC2 Messages, CloudWatch Logs) Systems Manager: Secure access to EC2 instances without SSH keys Key Features Infrastructure as Code: Entire infrastructure defined in CloudFormation High Availability: Multi-AZ deployment with Auto Scaling Security: Private subnets, security groups, IAM roles, VPC endpoints Monitoring: CloudWatch logs, alarms, and metrics Cost Optimization: VPC endpoints to reduce NAT Gateway data transfer costs Scalability: Auto Scaling based on CPU metrics "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.1-prepare/",
	"title": "Prepare Frontend Environment",
	"tags": [],
	"description": "",
	"content": "Prerequisites Before building and deploying the frontend, ensure you have:\nNode.js and npm installed (Node.js 18+ recommended) AWS CLI configured with appropriate credentials Frontend bucket name from CloudFormation outputs CloudFront Distribution ID from CloudFormation outputs Get Required Information Get the frontend S3 bucket name: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text Get the CloudFront Distribution ID: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text Get the API Gateway URL: aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text Verify Frontend Environment Variables Check that the .env file in the FE directory contains the correct API URL:\n# Windows type FE\\.env # Linux/Mac cat FE/.env The file should contain:\nVITE_API_URL=https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service VITE_COGNITO_USER_POOL_ID=ap-southeast-1_4osSduRDx VITE_COGNITO_CLIENT_ID=51alb0b6n4h5unrojbshmqv12r VITE_COGNITO_REGION=ap-southeast-1 Note: Update VITE_API_URL with the actual API Gateway URL from step 3 above if different.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "In this section, you need to summarize the contents of the workshop that you plan to conduct.\nBlood Donation Support System AWS Solution for Blood Donation Support Software 1. Executive Summary Blood Donation Support System (BDSS) is a web platform that supports the management and connection of blood donors with medical facilities. The project was developed by a group of students in Ho Chi Minh City to optimize the blood donation process, reduce the burden of searching for donors and improve the efficiency of medical communication.\nThe system is built on AWS Cloud architecture, using Amazon EC2, Amazon RDS, API Gateway, Cognito and CI/CD Pipeline (GitLab + CodePipeline) for automatic deployment. BDSS supports four user groups (Guest, Member, Staff, Admin), providing features for searching, registering for blood donation, managing blood banks, tracking blood donation processes and visual reporting.\n2. Problem Statement What’s the Problem? Healthcare facilities currently manage blood donation processes manually or through disparate tools. Finding donors who match blood type or region is difficult, especially in emergency situations. In addition, data storage systems are not synchronized, making it difficult to analyze, report and optimize blood donation campaigns.\nThe Solution Developed a comprehensive blood donation support platform on AWS Cloud, with functions for blood donation management, searching for donors and blood needers by blood type or geographic location, integrating user authentication via Amazon Cognito, and data governance on Amazon RDS. The frontend was deployed via Route 53 + CloudFront, the backend via API Gateway – EC2, a MySQL database on Amazon RDS, and an automated CI/CD pipeline using GitLab – CodePipeline.\nBenefits and Return on Investment Reduce the time it takes to find a matching donor by 60–70%. Increase the accuracy of blood type and location information. Optimize operating costs with a flexible, pay-as-you-go cloud architecture. Improve response to blood emergencies\n3. Solution Architecture The platform employs a comprehensive AWS cloud architecture to support blood donation management, connecting donors with medical facilities efficiently. The system integrates multiple AWS services to provide a scalable, secure, and cost-effective solution. The architecture is detailed below:\nThe system is divided into 4 main layers:\nEdge Networking Layer: Route 53 manages domain and DNS routing. CloudFront increases page loading speed and delivers static content. AWS WAF protects against web attacks (SQL injection, DDoS).\nApplication \u0026amp; Data Layer: Amazon EC2: Deploys backend API and handles main business. Amazon RDS (MySQL): Stores blood donor data, blood types, donation history. API Gateway: Communicates between frontend and backend. Elastic Load Balancer (ELB): Distributes load to EC2 instances. NAT Gateway \u0026amp; Internet Gateway: Supports secure Internet connection.\nCI/CD \u0026amp; DevOps Layer: GitLab: Source code management. AWS CodePipeline, CodeBuild: Deploy and update automatically.\nMonitoring \u0026amp; Security Layer: Amazon Cognito: Authentication and authorization (Guest, Member, Staff, Admin). CloudWatch, CloudTrail, IAM, Secrets Manager: Monitoring, security, system alerts. SNS: Send notifications when there is an event (blood emergency, suitable donor).\n4. Technical Implementation Implementation Phases\nAnalysis \u0026amp; Design (January) Gather requirements, define use cases, design ERD and AWS architecture. Infrastructure \u0026amp; Pipeline Setup (February) Configure Route 53, CloudFront, EC2, RDS and CI/CD on AWS. Development \u0026amp; Testing (March-April) Build main modules: blood donation registration, search, blood bank management. Integrate Cognito and SNS alert system. Deployment \u0026amp; Operation (May) Deploy the official product and monitor with CloudWatch. Key Technical Requirements: Frontend: React/Next.js or Angular (deploy via S3/CloudFront). Backend: Spring Boot on EC2, communicate via REST API Gateway. Database: Amazon RDS MySQL, optimize queries and periodic backups. CI/CD: GitLab → CodeBuild → CodePipeline → EC2. Auth: Cognito (4 roles: Guest, Member, Staff, Admin). Alert \u0026amp; Logs: SNS + CloudWatch + CloudTrail.\n5. Timeline \u0026amp; Milestones Timeline Phase Key Results Month 1 Requirements analysis \u0026amp; design AWS architecture + use case diagram Month 2 Infrastructure \u0026amp; pipeline setup EC2, RDS, API Gateway operational Month 3–4 Development \u0026amp; testing Key modules finalized Month 5 Live deployment System stable, with Dashboard reporting 6. Budget Estimation Services Estimated Cost/Month (USD) Notes EC2 (t3.nano) 3.50 Backend REST API Amazon RDS (MySQL) 2.80 20 GB storage API Gateway 0.50 5,000 requests CloudFront + S3 0.80 Website + CDN Route 53 0.50 Domain \u0026amp; DNS Cognito 0.10 \u0026lt;100 users CloudWatch + Logs 0.30 Monitoring \u0026amp; Alerting CI/CD (CodePipeline, CodeBuild) 0.40 Automated Deployment Total 8.9 USD/month ~106.8 USD/year Total costs may vary based on AWS Free Tier or spot instance usage.\n7. Risk Assessment Risk Impact Probability Mitigation Internet Outage Medium Medium Redundancy on EC2 Backup DDoS Attack High Low AWS WAF + CloudFront User Data Corruption High Low RDS Backup + IAM Restricted Access Cost Overrun Medium Low AWS Budget Alert CI/CD Deployment Disruption Low Medium Pipeline Testing Before Merging 8. Expected Outcomes Technology: Cloud-native system, automatic CI/CD, multi-user support and high security. Application: Helps medical facilities manage blood donations effectively, minimizing manual processes. Expansion: Can be replicated for many other hospitals, integrating AI to analyze blood group needs or predict upcoming blood donations.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Deploy Backend Layer: EC2 instances in private subnet with application runtime and Auto Scaling configuration. Set up Amazon RDS database in private subnet with proper configuration and parameter groups. Deploy backend application and establish connectivity between EC2 and RDS using Secrets Manager. Configure API Gateway REST API with integration to EC2 backend. Integrate Amazon Cognito User Pool with API Gateway for authentication and authorization. Configure Auto Scaling Group for EC2 instances with Launch Template for scalability. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - RDS Database Setup: + Create RDS subnet group spanning private subnet (10.0.2.0/24). + Launch RDS instance (MySQL/PostgreSQL) in private subnet with appropriate instance class. + Configure RDS parameter group with database-specific settings (character set, timezone, etc.). + Set up automated backups, encryption at rest, and Multi-AZ deployment (optional for cost optimization). + Configure RDS security group to allow connections only from EC2 Security Group. + Store initial database credentials in AWS Secrets Manager. 09/11/2025 09/11/2025 RDS documentation 3 - EC2 Backend Instance Setup: + Launch EC2 instance in private subnet (10.0.2.0/24) with appropriate instance type. + Install application runtime environment: Java/Python/Node.js based on application requirements. + Install and configure application dependencies and libraries. + Configure EC2 instance with IAM role (created in Week 9) for AWS service access. + Create base AMI from configured EC2 instance for Auto Scaling Group (to be used on Day 18). + Document EC2 configuration and application setup steps. 10/11/2025 10/11/2025 EC2 documentation 4 - Backend Application Deployment: + Deploy backend application code to EC2 instance (manual deployment for initial setup). + Configure application to connect to RDS database using credentials from Secrets Manager. + Test database connectivity from EC2 instance (verify connection string, credentials retrieval). + Configure application environment variables and configuration files. + Test basic application functionality and database operations (CRUD operations). + Document deployment process and application configuration. 11/11/2025 11/11/2025 Application deployment guide 5 - API Gateway REST API Configuration: + Create REST API in API Gateway with appropriate name and description. + Define API resources and methods (GET, POST, PUT, DELETE) based on application requirements. + Configure API Gateway integration with EC2 backend (HTTP/HTTPS integration or VPC Link for private resources). + Set up API Gateway VPC Link to connect to private subnet resources (EC2). + Enable CORS for frontend access (configure CORS headers: Access-Control-Allow-Origin, etc.). + Test API endpoints and verify integration with EC2 backend. 12/11/2025 12/11/2025 API Gateway documentation 6 - Amazon Cognito Integration: + Create Cognito User Pool for user authentication with appropriate name. + Configure user pool settings: password policies (minimum length, complexity), MFA (optional), email verification. + Create Cognito User Pool App Client for application integration. + Configure Cognito Authorizer in API Gateway for authenticated API access. + Test user registration flow: create test user in Cognito User Pool. + Test login flow: authenticate user and obtain JWT tokens. + Test authenticated API access: use JWT token to access protected API endpoints. 13/11/2025 13/11/2025 Cognito documentation 7 - Auto Scaling Group Configuration: + Create Launch Template based on base AMI created on Day 14. + Configure Launch Template with instance type, security groups, IAM role, and user data scripts. + Create Auto Scaling Group with Launch Template in private subnet. + Configure Auto Scaling policies: target tracking (CPU utilization, network in/out), step scaling, or scheduled scaling. + Set minimum, desired, and maximum capacity for Auto Scaling Group. + Test scale-out: trigger scaling by increasing load (or manually adjust desired capacity). + Test scale-in: reduce load and verify instances are terminated automatically. - Week 10 Summary: Backend and database layer complete, ready for CI/CD and monitoring setup in Week 11. 14/11/2025 14/11/2025 Auto Scaling documentation Week 10 Achievements: Successfully deployed Amazon RDS database:\nCreated RDS subnet group in private subnet for database isolation. Launched RDS instance (MySQL/PostgreSQL) with appropriate instance class and configuration. Configured RDS parameter group with database-specific settings. Set up automated backups, encryption at rest, and monitoring. Configured RDS security group to allow connections only from EC2 Security Group. Stored database credentials securely in AWS Secrets Manager. Set up EC2 backend infrastructure:\nLaunched EC2 instance in private subnet with appropriate instance type. Installed and configured application runtime environment (Java/Python/Node.js). Configured EC2 instance with IAM role for AWS service access. Created base AMI from configured EC2 instance for Auto Scaling Group. Documented EC2 configuration and application setup procedures. Deployed backend application:\nDeployed backend application code to EC2 instance. Configured application to connect to RDS using credentials from Secrets Manager. Tested database connectivity and verified connection functionality. Tested basic application functionality and database operations (CRUD). Documented deployment process and application configuration. Configured API Gateway REST API:\nCreated REST API with resources, methods, and integration points. Set up API Gateway VPC Link to connect to private subnet resources (EC2). Configured API Gateway integration with EC2 backend using HTTP/HTTPS. Enabled CORS for frontend access with proper headers. Tested API endpoints and verified integration with EC2 backend. Integrated Amazon Cognito for authentication:\nCreated Cognito User Pool with password policies, MFA, and email verification. Created Cognito User Pool App Client for application integration. Configured Cognito Authorizer in API Gateway for authenticated API access. Tested user registration, login, and authenticated API access flows. Established secure user authentication and authorization. Configured Auto Scaling Group for scalability:\nCreated Launch Template based on base AMI for consistent instance configuration. Created Auto Scaling Group with Launch Template in private subnet. Configured Auto Scaling policies (target tracking, step scaling) for automatic scaling. Set appropriate capacity limits (minimum, desired, maximum). Tested scale-out and scale-in functionality to verify automatic scaling. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Set up CI/CD Pipeline: Connect GitLab repository to AWS CodePipeline for automated deployments. Configure AWS CodeBuild for frontend and backend builds with automatic S3 upload and CloudFront invalidation. Implement SSH-less deployment for backend using AWS Systems Manager or CodeDeploy. Set up comprehensive monitoring with CloudWatch logs, metrics, and enhanced monitoring for EC2 and RDS. Configure AWS CloudTrail for audit logging and security compliance. Set up SNS Alerts with CloudWatch alarms for critical metrics (EC2 CPU, RDS connections, API 5xx errors). Perform end-to-end testing and create final project documentation with complete architecture diagram. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - GitLab to CodePipeline Integration: + Create GitLab repository for the project (if not already created). + Set up AWS CodePipeline with source stage connected to GitLab repository. + Configure webhook or polling for automatic pipeline triggers on code commits. + Create S3 bucket for pipeline artifacts storage. + Test pipeline trigger by making a test commit to GitLab repository. + Verify CodePipeline can successfully connect to GitLab and retrieve source code. 16/11/2025 16/11/2025 CodePipeline documentation 3 - CodeBuild for Frontend: + Create CodeBuild project for frontend build process. + Configure buildspec.yml file for frontend build steps (install dependencies, build assets, optimize). + Set up CodeBuild environment with appropriate Docker image (Node.js, npm, etc.). + Configure build output to upload built files to S3 bucket (FE Bucket). + Set up automatic CloudFront invalidation after S3 upload (invalidate cache for updated files). + Test frontend build process and verify files are uploaded to S3 and CloudFront cache is invalidated. 17/11/2025 17/11/2025 CodeBuild documentation 4 - CodeBuild for Backend \u0026amp; SSH-less Deployment: + Create CodeBuild project for backend build process. + Configure buildspec.yml for backend build steps (compile, test, package artifacts). + Set up CodeBuild environment for backend (Java/Python/Node.js based on application). + Configure artifact upload to S3 or CodeDeploy. + Implement SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy: - Option 1: Use SSM Run Command to deploy to EC2 instances without SSH. - Option 2: Use CodeDeploy to deploy application to Auto Scaling Group. + Test backend build and deployment process end-to-end. 18/11/2025 18/11/2025 CodeDeploy / SSM documentation 5 - CloudWatch Logs \u0026amp; Metrics Setup: + Create CloudWatch log groups for EC2 application logs. + Configure CloudWatch agent on EC2 instances to send logs and custom metrics. + Set up CloudWatch metrics for EC2: CPU utilization, memory, disk I/O, network. + Enable RDS Enhanced Monitoring for detailed database metrics. + Configure API Gateway access logs to CloudWatch Logs. + Create CloudWatch dashboards for monitoring application health and performance. + Configure log retention policies for cost optimization. 19/11/2025 19/11/2025 CloudWatch documentation 6 - CloudTrail \u0026amp; Audit Dashboard: + Enable AWS CloudTrail for API call logging across all AWS services. + Create CloudTrail trail with S3 bucket for log storage. + Configure CloudTrail log file validation and encryption. + Set up CloudWatch Logs integration for CloudTrail events (optional). + Create CloudWatch dashboard for audit and security monitoring. + Review CloudTrail logs to verify API call logging is working correctly. + Document CloudTrail configuration and log retention policies. 20/11/2025 20/11/2025 CloudTrail documentation 7 - SNS Alerts \u0026amp; CloudWatch Alarms: + Create SNS topic for alarm notifications. + Subscribe email/SMS endpoints to SNS topic. + Create CloudWatch alarm for EC2 CPU utilization (threshold: \u0026gt;80% for 5 minutes). + Create CloudWatch alarm for RDS database connections (threshold: \u0026gt;80% of max connections). + Create CloudWatch alarm for API Gateway 5xx errors (threshold: \u0026gt;10 errors in 5 minutes). + Configure alarm actions to send notifications via SNS. + Test alarms by triggering conditions and verify email/SMS notifications are received. 21/11/2025 21/11/2025 CloudWatch Alarms \u0026amp; SNS 8 - End-to-End Testing \u0026amp; Final Documentation: + Perform comprehensive end-to-end testing: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. + Test CI/CD pipeline with code changes: verify automated frontend and backend deployments. + Test monitoring and alerting: trigger alarms and verify SNS notifications are received. + Test security: verify IAM permissions, Cognito authentication, Secrets Manager access, WAF protection. + Test scalability: verify Auto Scaling Group responds to load changes. + Create final architecture diagram with all components, data flows, and resource relationships. + Write comprehensive project documentation: deployment procedures, troubleshooting guides, runbooks, and architecture overview. + Prepare Worklog summary for all 4 weeks (Week 8-11). - Week 11 Summary: Complete AWS web application architecture deployed with CI/CD, monitoring, security, and automation. Project ready for production use. 22/11/2025 22/11/2025 Project documentation Week 11 Achievements: Successfully set up CI/CD Pipeline:\nConnected GitLab repository to AWS CodePipeline for automated deployments. Configured automatic pipeline triggers on code commits (webhook or polling). Created S3 bucket for pipeline artifacts storage. Verified end-to-end pipeline connectivity and source code retrieval. Configured CodeBuild for Frontend:\nCreated CodeBuild project with buildspec.yml for frontend build automation. Configured build environment with appropriate Docker image and dependencies. Set up automatic S3 upload of built frontend files. Implemented automatic CloudFront cache invalidation after deployments. Verified frontend build and deployment process works correctly. Implemented CodeBuild for Backend with SSH-less Deployment:\nCreated CodeBuild project for backend build automation. Configured buildspec.yml for backend compilation, testing, and packaging. Implemented SSH-less deployment using AWS Systems Manager (SSM) or CodeDeploy. Eliminated need for SSH keys and improved security posture. Verified backend build and deployment process works end-to-end. Set up comprehensive CloudWatch Monitoring:\nCreated CloudWatch log groups for EC2 application logs. Configured CloudWatch agent on EC2 instances for logs and custom metrics. Set up CloudWatch metrics for EC2 (CPU, memory, disk, network). Enabled RDS Enhanced Monitoring for detailed database insights. Configured API Gateway access logs to CloudWatch Logs. Created CloudWatch dashboards for real-time monitoring. Configured log retention policies for cost optimization. Configured CloudTrail for Audit and Compliance:\nEnabled CloudTrail for comprehensive API call logging. Created CloudTrail trail with S3 bucket for secure log storage. Configured log file validation and encryption. Set up CloudWatch dashboard for audit monitoring. Established audit trail for security and compliance requirements. Implemented SNS Alerts and CloudWatch Alarms:\nCreated SNS topic for alarm notifications with email/SMS subscriptions. Created CloudWatch alarm for EC2 CPU utilization monitoring. Created CloudWatch alarm for RDS database connection monitoring. Created CloudWatch alarm for API Gateway 5xx error detection. Configured alarm actions to send notifications via SNS. Tested alarms and verified notification delivery. Performed comprehensive end-to-end testing:\nVerified complete application flow: Users → Route 53 → CloudFront → WAF → API Gateway → EC2 → RDS. Tested CI/CD pipeline with code changes and verified automated deployments. Tested monitoring and alerting: triggered alarms and verified SNS notifications. Tested security: verified IAM permissions, Cognito authentication, Secrets Manager, WAF protection. Tested scalability: verified Auto Scaling Group responds to load changes. Created final project documentation:\nCreated comprehensive architecture diagram with all components, data flows, and resource relationships. Documented deployment procedures, troubleshooting guides, and runbooks. Prepared architecture overview and system design documentation. Completed Worklog summary for all 4 weeks (Week 8-11). After Week 11, the complete AWS web application architecture is fully deployed, monitored, secured, and automated:\nEdge Layer: Route 53, CloudFront, AWS WAF, ACM Certificate, S3 (Frontend). Networking Layer: VPC, public/private subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs. Compute \u0026amp; Database Layer: EC2 (with Auto Scaling), RDS, API Gateway, Cognito. CI/CD Pipeline: GitLab, CodePipeline, CodeBuild (Frontend \u0026amp; Backend), SSH-less deployment. Monitoring \u0026amp; Security: CloudWatch (Logs, Metrics, Dashboards, Alarms), CloudTrail, SNS Alerts, IAM, Secrets Manager. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Finalize Worklog and deploy it Preparate for Presentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Worklog related + Reviewing to check for mistake + Pushing worklog to Github + Deploying it as a Github pages 11/24/2025 08/24/2025 3 - Prepare Presentation 11/25/2025 08/25/2025 Week 12 Achievements: Completed Worklog and Deployed it\nPresentation prepared\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.2-build/",
	"title": "Build Frontend Application",
	"tags": [],
	"description": "",
	"content": "Build React Application In this section, you will build the React frontend application using Vite.\nNavigate to the frontend directory: cd FE Install dependencies (if not already installed): npm install Verify the .env file exists and contains the correct API URL: # Windows type .env # Linux/Mac cat .env Build the application for production: npm run build Expected Result: The dist directory should be created with production-ready files:\nindex.html assets/ folder with JavaScript and CSS files Verify the build output: # Windows dir dist # Linux/Mac ls -lh dist/ Note: The build process will use the VITE_API_URL from the .env file and embed it into the JavaScript bundle.\nBuild Troubleshooting If you encounter build errors:\nError: VITE_API_URL is not defined: Ensure the .env file exists and contains VITE_API_URL Error: Module not found: Run npm install to install dependencies Error: Port already in use: Stop any running development server "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/5.3.2-deploy-ec2/",
	"title": "Deploy to EC2",
	"tags": [],
	"description": "",
	"content": "Connect to EC2 with Session Manager For this workshop, you will use AWS Session Manager to access EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances through an interactive browser-based shell without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance from your Auto Scaling Group (use the instance ID you saved from the previous section). Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nDeploy Backend Application Switch to the ec2-user: sudo su - ec2-user Navigate to the application directory: cd /opt/workshop Download the JAR file from S3: aws s3 cp s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/jars/workshop-0.0.1-SNAPSHOT.jar . --region ap-southeast-1 Stop any existing application (if running): pkill -f workshop-0.0.1-SNAPSHOT.jar || true Get the RDS endpoint from CloudFormation: RDS_ENDPOINT=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;RDSEndpoint\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;RDS Endpoint: $RDS_ENDPOINT\u0026#34; Create the application.properties file: cat \u0026gt; /opt/workshop/application.properties \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; spring.application.name=workshop-aws # AWS RDS Database Configuration spring.datasource.url=jdbc:mysql://${RDS_ENDPOINT}:3306/workshop_aws?useSSL=true\u0026amp;requireSSL=false\u0026amp;allowPublicKeyRetrieval=true\u0026amp;serverTimezone=Asia/Ho_Chi_Minh spring.datasource.username=admin spring.datasource.password=phatsieuqua123 spring.datasource.driver-class-name=com.mysql.cj.jdbc.Driver # Connection Pool spring.datasource.hikari.maximum-pool-size=10 spring.datasource.hikari.minimum-idle=5 spring.datasource.hikari.connection-timeout=20000 # JPA Configuration spring.jpa.hibernate.ddl-auto=update spring.jpa.show-sql=false # Server Configuration server.port=8080 server.servlet.context-path=/dna_service # JWT Configuration jwt.signerKey=2VJ50pdhYm96e4VECp/vsZGVmkSl9xp1rSYAZKsZL7n9Ti1pZYle3k9mheQEKt6+ # CORS Configuration cors.allowed.origins=https://d3gmmg22uirq0t.cloudfront.net,https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com # Logging logging.level.root=INFO logging.level.aws_project.workshop=DEBUG logging.file.name=/opt/workshop/application.log # Actuator Configuration management.endpoints.web.exposure.include=health,info,metrics management.endpoint.health.show-details=when-authorized EOF Note: Replace ${RDS_ENDPOINT} with the actual RDS endpoint value.\nStart the application: nohup java -jar workshop-0.0.1-SNAPSHOT.jar \\ --spring.config.location=file:/opt/workshop/application.properties \\ \u0026gt;\u0026gt; /opt/workshop/application.log 2\u0026gt;\u0026amp;1 \u0026amp; Wait a few seconds and check if the application is running: sleep 10 ps aux | grep java tail -20 /opt/workshop/application.log Verify Backend Deployment Test the health endpoint: curl http://localhost:8080/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nTest through the API Gateway (get the URL from CloudFormation outputs): API_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Section Summary Congratulations! You have successfully deployed the Spring Boot backend application to EC2. The application is now running in a private subnet, accessible through the Application Load Balancer and API Gateway. The VPC endpoints allow the EC2 instance to access S3 (for downloading the JAR) and Systems Manager (for Session Manager) without traversing the public internet.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:*\u0026#34;,\r\u0026#34;iam:*\u0026#34;,\r\u0026#34;s3:*\u0026#34;,\r\u0026#34;rds:*\u0026#34;,\r\u0026#34;apigateway:*\u0026#34;,\r\u0026#34;cloudfront:*\u0026#34;,\r\u0026#34;ssm:*\u0026#34;,\r\u0026#34;logs:*\u0026#34;,\r\u0026#34;autoscaling:*\u0026#34;,\r\u0026#34;elasticloadbalancing:*\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} Provision resources using CloudFormation In this workshop, we will use Singapore region (ap-southeast-1).\nTo deploy the infrastructure, use the following command:\naws cloudformation create-stack \\ --stack-name workshop-aws-dev \\ --template-body file://aws/infrastructure.yaml \\ --parameters file://aws/parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Or update an existing stack:\naws cloudformation update-stack \\ --stack-name workshop-aws-dev \\ --template-body file://aws/infrastructure.yaml \\ --parameters file://aws/parameters.json \\ --capabilities CAPABILITY_NAMED_IAM \\ --region ap-southeast-1 Prerequisites Before deploying, ensure you have:\nAWS CLI installed and configured with appropriate credentials IAM Permissions as specified in the IAM permissions section above Parameters File: aws/parameters.json configured with your values EC2 Key Pair: Create a key pair in AWS Console (used for EC2 access, though we\u0026rsquo;ll use SSM Session Manager) CloudFormation Stack Deployment The CloudFormation deployment requires about 20-25 minutes to complete. The stack will create:\n1 VPC with public and private subnets across 2 Availability Zones 1 Auto Scaling Group with EC2 instances (t3.micro) 1 RDS MySQL database instance (db.t3.micro) 2 S3 Buckets (frontend and backend) 1 CloudFront Distribution for frontend 1 Application Load Balancer for backend 1 API Gateway REST API 5 VPC Endpoints (S3 Gateway, SSM, SSM Messages, EC2 Messages, CloudWatch Logs) IAM Roles and Policies for EC2, Lambda, and other services Security Groups for network access control Route Tables and Internet Gateway for networking Verify Deployment After the stack creation completes, verify the following resources:\nVPC: Check VPC console for workshop-aws-dev-vpc EC2 Instances: Check Auto Scaling Group for running instances RDS: Verify database endpoint in RDS console S3 Buckets: Confirm frontend and backend buckets exist CloudFront: Check distribution status API Gateway: Verify REST API is deployed "
},
{
	"uri": "http://localhost:1313/fcj-workshop/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "IMF chatbot Myelo transforms cancer support with AWS Bedrock and Max.AI Myelo is a powerful example of how generative AI + cloud infrastructure + nonprofit mission can combine to make a real difference in healthcare support — especially for complex, global diseases like multiple myeloma. By giving patients — and their caregivers — instant access to well-organized, trustworthy, multilingual information, at any time and anywhere, IMF helps lower barriers in understanding, decision-making, and navigating treatment. As Myelo and the MKP evolve, we’re seeing a shift toward more personalized, accessible, and patient-centered care — not replacing doctors, but empowering patients with knowledge and support.\nTransforming SAP Technical Documentation with GenAI: Accelerating knowledge generation for SAP Notes with Amazon Bedrock TSAP is using generative AI powered by Amazon Bedrock to automatically summarize and restructure its massive technical documentation (SAP Notes and KBAs). This makes complex support information faster to find, easier to understand, and far more usable on mobile devices. It reduces troubleshooting time, boosts productivity, and moves SAP’s documentation from static reference material toward an intelligent, AI-driven knowledge system that can provide more proactive and personalized guidance in the future.\nMOSIP on AWS: Transforming digital identity for modern governments MOSIP on AWS provides governments with a secure, scalable, and cost-efficient platform to issue and manage digital identities at national scale, replacing traditional on-premises systems. Its modular, open-source design allows customization and integration while maintaining data security and compliance. By enabling inclusive, accessible identity systems — even in remote or low-resource areas — MOSIP helps expand access to essential services like healthcare, education, and social programs, making digital identity a foundation for more efficient, equitable public services.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.3-backend-deployment/",
	"title": "Backend Deployment",
	"tags": [],
	"description": "",
	"content": "Deploy Backend Application In this section, you will build and deploy the Spring Boot backend application to EC2 instances. The backend will run in private subnets and be accessible through the Application Load Balancer and API Gateway.\nThe deployment process includes:\nBuilding the Spring Boot JAR file Uploading the JAR to S3 Deploying to EC2 instances via Session Manager Configuring application properties Starting the application service Content Build and Upload Backend Deploy to EC2 "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.3-deploy/",
	"title": "Deploy to S3 and CloudFront",
	"tags": [],
	"description": "",
	"content": "Upload Frontend to S3 Get the frontend bucket name (if you don\u0026rsquo;t have it): BUCKET_NAME=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;FrontendBucketName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Bucket: $BUCKET_NAME\u0026#34; Upload the built files to S3: # Windows aws s3 sync dist\\ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 # Linux/Mac aws s3 sync dist/ s3://$BUCKET_NAME/ --delete --region ap-southeast-1 Expected Result: Files are uploaded to S3 with output like:\nupload: dist/index.html to s3://...\rupload: dist/assets/... Verify the upload: aws s3 ls s3://$BUCKET_NAME/ --recursive --region ap-southeast-1 Invalidate CloudFront Cache Get the CloudFront Distribution ID: DIST_ID=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDistributionId\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Distribution ID: $DIST_ID\u0026#34; Create a CloudFront invalidation: aws cloudfront create-invalidation \\ --distribution-id $DIST_ID \\ --paths \u0026#34;/*\u0026#34; \\ --region ap-southeast-1 Expected Result: An invalidation ID is returned. The invalidation typically takes 1-2 minutes to complete.\nCheck invalidation status (optional): INVALIDATION_ID=$(aws cloudfront list-invalidations \\ --distribution-id $DIST_ID \\ --region ap-southeast-1 \\ --query \u0026#34;InvalidationList.Items[0].Id\u0026#34; \\ --output text) aws cloudfront get-invalidation \\ --distribution-id $DIST_ID \\ --id $INVALIDATION_ID \\ --region ap-southeast-1 Verify Frontend Deployment Get the CloudFront domain name: CLOUDFRONT_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;CloudFrontDomainName\u0026#39;].OutputValue\u0026#34; \\ --output text) echo \u0026#34;Frontend URL: https://$CLOUDFRONT_URL\u0026#34; Open the frontend URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Test the application: The frontend should load Try logging in or registering a new user Verify API calls are working (check browser console for errors) Section Summary Congratulations! You have successfully deployed the React frontend application to S3 and CloudFront. The frontend is now accessible globally via CloudFront CDN, providing low latency and high availability. The application communicates with the backend through API Gateway.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: AWS Vietnam Office\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the complete AWS AI/ML ecosystem, from Amazon SageMaker for traditional ML to Amazon Bedrock for Generative AI. I gained deep understanding of prompt engineering techniques like Chain-of-Thought reasoning and Few-shot learning. I also learned about RAG (Retrieval-Augmented Generation) architecture and how it\u0026rsquo;s crucial for building accurate GenAI applications. The importance of guardrails for AI safety and content filtering in production applications was also emphasized. New Skills: I developed skills in using Amazon SageMaker Studio for ML model development and deployment. I learned how to implement RAG architecture for knowledge base integration. I gained practical knowledge of prompt engineering and how to build Bedrock Agents for multi-step workflows. I also learned about different foundation models (Claude, Llama, Titan) and when to use each one. Contribution to Team/Project: I shared comprehensive notes about SageMaker capabilities and Bedrock features with my team. I identified opportunities to implement RAG solutions for our domain-specific applications. I proposed pilot projects using Bedrock Agents for customer service automation. I also created guidelines for prompt engineering best practices and guardrail implementation for our GenAI projects. Event 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about DevOps culture and principles, including DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate) for measuring DevOps maturity. I gained deep understanding of AWS CI/CD services (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) and different deployment strategies (Blue/Green, Canary, Rolling). I also learned about Infrastructure as Code with CloudFormation and CDK, and when to use each approach. New Skills: I developed skills in building complete CI/CD pipelines using AWS DevOps services. I learned how to implement Infrastructure as Code with both CloudFormation and CDK. I gained practical knowledge of container services (ECR, ECS, EKS, App Runner) and when to use each one. I also learned how to set up monitoring and observability using CloudWatch and X-Ray. Contribution to Team/Project: I shared comprehensive notes about AWS DevOps services and best practices with my team. I proposed implementing CI/CD pipelines using CodePipeline for automated deployments. I suggested adopting Infrastructure as Code for all our infrastructure using CloudFormation or CDK. I also created guidelines for containerization strategies and monitoring best practices. The knowledge gained helps our team implement modern DevOps practices and improve deployment frequency and reliability. Event 3 Event Name: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: AWS Vietnam Office, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about the AWS Well-Architected Framework Security Pillar and its five core pillars: Identity \u0026amp; Access Management, Detection, Infrastructure Protection, Data Protection, and Incident Response. I gained deep understanding of core security principles including Least Privilege, Zero Trust, and Defense in Depth. I also learned about the Shared Responsibility Model and top security threats in cloud environments in Vietnam. The importance of building security into architecture from the start, not as an afterthought, was emphasized. New Skills: I developed skills in modern IAM architecture using IAM Identity Center, Service Control Policies, and permission boundaries. I learned how to implement comprehensive detection and monitoring using CloudTrail, GuardDuty, and Security Hub. I gained practical knowledge of network security with VPC segmentation, Security Groups, NACLs, WAF, and Shield. I also learned about encryption at rest and in transit, KMS key management, and secrets management with Secrets Manager and Parameter Store. Contribution to Team/Project: I shared comprehensive security best practices and the five pillars framework with my team. I proposed implementing modern IAM patterns with IAM Identity Center for SSO. I suggested setting up comprehensive monitoring using CloudTrail, GuardDuty, and Security Hub. I created incident response playbooks for common scenarios like compromised IAM keys, S3 public exposure, and EC2 malware detection. I also developed guidelines for encryption strategies and secrets management. The knowledge gained helps our team build secure cloud architectures following AWS Well-Architected best practices. Event 4 Event Name: Building Agentic AI: Context Optimization with Amazon Bedrock\nDate \u0026amp; Time: 09:00, December 5, 2025\nLocation: 26th Floor, Bitexco Financial Tower, 2 Đ. Hải Triều, Bến Nghé, Quận 1, Thành phố Hồ Chí Minh\nRole: Attendee\nOutcomes or Value Gained:\nLessons Learned: I learned about Building Agentic AI and Context Optimization with Amazon Bedrock. I gained deep understanding of how to build autonomous AI agents with Amazon Bedrock through hands-on techniques. I learned about agentic orchestration patterns and advanced context optimization techniques. I also understood the importance of context optimization in reducing costs and improving performance. The workshop emphasized that agentic AI is the future of AI applications and context optimization is key to scaling effectively. New Skills: I developed skills in building Bedrock Agents from scratch with guidance from experts. I learned about context optimization techniques such as compression, summarization, and relevant information extraction. I gained practical knowledge of agentic orchestration patterns and how to coordinate multiple agents. I also learned about the CloudThinker platform and how it simplifies building agentic systems. The hands-on workshop gave me opportunities to practice with real AWS environments. Contribution to Team/Project: I shared comprehensive notes about Building Agentic AI and Context Optimization with my team. I proposed pilot projects using Bedrock Agents for automation tasks. I created guidelines for context optimization best practices to reduce costs and improve performance. I also documented CloudThinker platform capabilities and integration patterns. The knowledge gained helps our team explore agentic AI solutions and optimize costs in AI/ML projects. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/",
	"title": "Frontend Deployment",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will build and deploy the React frontend application to S3 and CloudFront. The frontend will be served via CloudFront CDN for global content delivery and low latency.\nThe deployment process includes:\nBuilding the React application with Vite Uploading static files to S3 Invalidating CloudFront cache Verifying the frontend is accessible Content Prepare Frontend Environment Build Frontend Application Deploy to S3 and CloudFront Verify Full Stack Deployment "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.4-frontend-deployment/5.4.4-verify/",
	"title": "Verify Full Stack Deployment",
	"tags": [],
	"description": "",
	"content": "Test Complete Application Flow In this section, you will verify that the entire application stack is working correctly, from frontend to backend to database.\nStep 1: Verify Backend Health Test the backend health endpoint through API Gateway:\nAPI_URL=$(aws cloudformation describe-stacks \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 \\ --query \u0026#34;Stacks[0].Outputs[?OutputKey==\u0026#39;APIGatewayURL\u0026#39;].OutputValue\u0026#34; \\ --output text) curl ${API_URL}/dna_service/actuator/health Expected Result: {\u0026quot;status\u0026quot;:\u0026quot;UP\u0026quot;}\nStep 2: Test Frontend Access Open the CloudFront URL in your browser: https://d3gmmg22uirq0t.cloudfront.net Open browser Developer Tools (F12) and check the Console tab for any errors.\nVerify the API URL is correctly configured by checking network requests.\nStep 3: Test User Registration/Login Try to register a new user or log in with existing credentials.\nVerify that:\nAPI calls are successful (check Network tab in DevTools) JWT token is stored in localStorage User is redirected to the appropriate page after login Step 4: Verify Database Connection Connect to EC2 via Session Manager and check application logs:\n# Connect to EC2 aws ssm start-session --target \u0026lt;INSTANCE_ID\u0026gt; --region ap-southeast-1 # On EC2, check logs tail -50 /opt/workshop/application.log | grep -i \u0026#34;database\\|connection\\|error\u0026#34; Expected Result: No database connection errors in logs.\nStep 5: Monitor Application Check CloudWatch Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 metrics in CloudWatch console: CPU utilization Network in/out Application health Troubleshooting Common Issues Frontend can\u0026rsquo;t connect to API:\nVerify VITE_API_URL in .env matches API Gateway URL Check CORS configuration in backend Verify API Gateway integration with ALB Backend not responding:\nCheck EC2 instance is running Verify application is running: ps aux | grep java Check application logs: tail -100 /opt/workshop/application.log Database connection errors:\nVerify RDS security group allows traffic from EC2 security group Check RDS endpoint is correct in application.properties Verify database credentials Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access Section Summary You have successfully deployed and verified the complete full-stack application. The architecture includes:\nFrontend served via CloudFront from S3 Backend running on EC2 in private subnets API Gateway routing requests to ALB RDS MySQL database for data persistence VPC endpoints for secure AWS service access "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Full-Stack Web Application Deployment on AWS Overview This workshop demonstrates how to deploy a complete full-stack web application on AWS using Infrastructure as Code (CloudFormation). You will learn to build a production-ready architecture with:\nBackend: Spring Boot REST API on EC2 with Auto Scaling Frontend: React application served via CloudFront from S3 Database: MySQL RDS for data persistence API Gateway: RESTful API with CORS support Load Balancer: Application Load Balancer for high availability VPC Endpoints: Private connectivity to AWS services (S3, SSM, CloudWatch) Architecture Highlights Infrastructure as Code: Entire infrastructure defined in CloudFormation templates High Availability: Multi-AZ deployment with Auto Scaling Groups Security: Private subnets, security groups, IAM roles, VPC endpoints Monitoring: CloudWatch logs, alarms, and metrics Cost Optimization: VPC endpoints to reduce NAT Gateway data transfer costs Scalability: Auto Scaling based on CPU metrics Workshop Content Workshop Overview - Introduction and architecture overview Prerequisites - IAM permissions and CloudFormation deployment Backend Deployment - Build and deploy Spring Boot application Frontend Deployment - Build and deploy React application Testing and Monitoring - Application testing and CloudWatch monitoring Clean up - Resource cleanup instructions Technologies Used AWS Services: VPC, EC2, RDS, S3, CloudFront, API Gateway, ALB, Auto Scaling, CloudWatch, Systems Manager Backend: Spring Boot, Java 17, MySQL Frontend: React, Vite, TypeScript Infrastructure: CloudFormation, IAM, Security Groups "
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.5-testing-monitoring/",
	"title": "Testing and Monitoring",
	"tags": [],
	"description": "",
	"content": "Application Testing and Monitoring In this section, you will learn how to test the application and monitor its performance using AWS CloudWatch and other monitoring tools.\nTesting the Application Health Check Endpoints:\nBackend: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/actuator/health Frontend: https://d3gmmg22uirq0t.cloudfront.net API Testing:\nUse Swagger UI: https://98385v3jef.execute-api.ap-southeast-1.amazonaws.com/dev/dna_service/swagger-ui.html Test endpoints using curl or Postman End-to-End Testing:\nRegister a new user Login and verify JWT token Create and manage resources Verify data persistence in database CloudWatch Monitoring View Application Logs: aws logs tail /aws/workshop-aws/dev/application --follow --region ap-southeast-1 Check EC2 Metrics:\nCPU Utilization Network In/Out Status Check Failed Monitor RDS:\nDatabase connections CPU utilization Storage space API Gateway Metrics:\nRequest count Latency Error rates Auto Scaling The Auto Scaling Group will automatically:\nScale up when CPU \u0026gt; 70% for 5 minutes Scale down when CPU \u0026lt; 30% for 5 minutes Maintain between 1-2 instances (configurable) Monitor scaling activities:\naws autoscaling describe-scaling-activities \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --region ap-southeast-1 Performance Optimization CloudFront Cache:\nStatic assets are cached at edge locations Invalidate cache when deploying updates Database Optimization:\nMonitor slow queries Optimize indexes Consider read replicas for high traffic Application Optimization:\nMonitor JVM heap usage Optimize database queries Use connection pooling effectively "
},
{
	"uri": "http://localhost:1313/fcj-workshop/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at First Cloud Journey (FCJ) from September 2025 to November 2025, I had the opportunity to learn, practice, and apply cloud computing knowledge to a real-world AWS environment.\nI participated in the AWS Cloud Journey internship program, where I completed a comprehensive 12-week learning journey and deployed a production-ready web application architecture on AWS. Through this project, I improved my skills in cloud architecture design, AWS services, Infrastructure as Code (CloudFormation), CI/CD pipelines, monitoring and observability, security best practices, and problem-solving in cloud environments.\nThe main project involved designing and implementing a complete AWS web application architecture including:\nEdge Layer: Route 53, CloudFront CDN, AWS WAF, ACM Certificate, S3 static hosting Networking: VPC, subnets, Internet Gateway, NAT Gateway, Security Groups, VPC Flow Logs Compute \u0026amp; Database: EC2 with Auto Scaling, RDS, API Gateway, Amazon Cognito CI/CD: GitLab, CodePipeline, CodeBuild with automated deployments Monitoring \u0026amp; Security: CloudWatch, CloudTrail, SNS alerts, IAM, Secrets Manager In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ☐ ✅ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ☐ ✅ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ☐ ✅ 8 Teamwork Working effectively with colleagues and participating in teams ☐ ✅ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ☐ ✅ 12 Overall General evaluation of the entire internship period ☐ ☐ ✅ Strengths Ability to learn: I find that I can learn and easily understand AWS services and concepts. When encountering something I am not sure of, I determine to search and find answer till I am satisfied discipline: I read and respectfully follow company rules and regulation. Progressive mindset: I am not be demotivated by any kind of criticism. instead, I will reflect what I do wrong, and strive to improve better and not to repeat the same mistake Professional Conduct: I respect everyone in the company, including my fellow interns and attend work seriously Needs Improvement Professional knowledge \u0026amp; skills: Due to the lack of time management, I wasn\u0026rsquo;t able to absorb knowledge fully and deeply. Time Management and Discipline: While I completed tasks, I greetly struggled with time management when dealing with balancing life and work. I need to improve my ability to manage,schedule and priortize what must be done first and urgently. Communication: I am fully aware of how passive I am when it comes to communication. I am still struggle to proactively with my teammates and people around me Cost Optimization Awareness: I focused more on functionality than cost optimization because it was so exciting to explore AWS services. I should consider cost implications Contribution to project/team: due to my Incompetence in communication. I found myself not contriubting much to the team and project. I am ashamed of myself Reflection This internship provided invaluable hands-on experience with AWS cloud services and best practices. It has strengthened my problem-solving skills and technical knowledge. Broaden my mind just how difficult work is, in which I believe it is but just a small fraction of what to come in future\nI am grateful for the opportunity to work in AWS intership program and look forward to applying skills I have learnt in future cloud architecture and DevOps roles.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations! You have successfully completed the Workshop AWS Project deployment! In this workshop, you learned:\nHow to deploy a full-stack application on AWS using CloudFormation Architecture patterns for high availability and scalability Security best practices with private subnets and VPC endpoints Monitoring and auto-scaling configurations Frontend and backend deployment processes Clean Up Resources To avoid incurring charges, delete all resources created during this workshop.\nMethod 1: Delete CloudFormation Stack (Recommended) The easiest way to clean up is to delete the CloudFormation stack:\naws cloudformation delete-stack \\ --stack-name workshop-aws-dev \\ --region ap-southeast-1 Note: Some resources may need manual deletion if they have deletion protection or dependencies.\nMethod 2: Manual Cleanup (if needed) If the stack deletion fails or leaves some resources, manually delete:\nStop EC2 Instances:\naws autoscaling update-auto-scaling-group \\ --auto-scaling-group-name \u0026lt;ASG_NAME\u0026gt; \\ --min-size 0 \\ --desired-capacity 0 \\ --region ap-southeast-1 Delete S3 Buckets:\n# Empty frontend bucket aws s3 rm s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-frontend-502310717700-ap-southeast-1 # Empty backend bucket aws s3 rm s3://workshop-aws-dev-backend-502310717700-ap-southeast-1/ --recursive aws s3 rb s3://workshop-aws-dev-backend-502310717700-ap-southeast-1 Disable and Delete CloudFront Distribution:\nFirst disable the distribution (wait for it to be disabled) Then delete it Delete RDS Database:\nTake a final snapshot if needed Delete the database instance (may take 10-15 minutes) Delete API Gateway:\nDelete the REST API Delete Load Balancer:\nDelete the Application Load Balancer Delete VPC Endpoints:\nDelete all VPC endpoints Delete CloudWatch Logs:\naws logs delete-log-group \\ --log-group-name /aws/workshop-aws/dev/application \\ --region ap-southeast-1 Verify Cleanup Check the following services to ensure all resources are deleted:\nEC2: No instances, security groups, or load balancers RDS: No database instances S3: No buckets CloudFront: No distributions API Gateway: No APIs VPC: VPC and related resources (if not managed by CloudFormation) CloudWatch: No log groups IAM: Review and delete custom roles/policies if created manually Cost Verification After cleanup, verify in AWS Cost Explorer that no charges are accruing for the deleted resources.\n"
},
{
	"uri": "http://localhost:1313/fcj-workshop/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company offers flexible working hours when needed. In addition, having the opportunity to join event to experience new trending technology is a big plus.\nAdditional Questions What did you find most satisfying during your internship? +Mentor and FCJ team members are all wonderful people as I stated in section 1 and 2 What do you think the company should improve for future interns? Due to the limitation of my knowledge and Experience, I am not able to point out If recommending to a friend, would you suggest they intern here? Why or why not? I will only recommend them if they truly treat internship as a learning opportunity, A value chance. If they only treat internship program as a subject to pass to advance to next semester, I will not. However anyone who thirsts for knowledge and love IT, I will definitely recommend them. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? It would be nice if there is an area to refill water for hydration Would you like to continue this program in the future? Yes Any other comments (free sharing): I simply can\u0026rsquo;t think of any. "
},
{
	"uri": "http://localhost:1313/fcj-workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/fcj-workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]